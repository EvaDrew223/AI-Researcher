{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Introspective Confidence Prompting",
    "raw_idea": {
        "Problem": "Large language models often struggle with calibrating their confidence, especially when it comes to complex reasoning tasks or domain-specific knowledge.",
        "Existing Methods": "Current methods for confidence calibration include temperature scaling, ensemble methods, and post-hoc calibration techniques. However, these methods often rely on external data or models and do not fully leverage the introspective capabilities of language models.",
        "Motivation": "Language models have shown the ability to engage in introspection and self-reflection when prompted appropriately. By leveraging this introspective capacity, we can encourage the model to assess its own confidence more accurately, considering factors such as the complexity of the task, the availability of relevant knowledge, and the potential for ambiguity or uncertainty in the input.",
        "Proposed Method": "We propose Introspective Confidence Prompting, a method that prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score. The prompts are designed to guide the model through a process of self-assessment, considering factors such as the difficulty of the task, the relevance of its knowledge, and the potential for multiple valid interpretations. For example, the model might be prompted to first identify the key components of the task, then assess its confidence in each component, and finally combine these assessments into an overall confidence score. The specific prompts can be tailored to different types of tasks and domains.",
        "Experiment Plan": "We will evaluate Introspective Confidence Prompting on a range of benchmarks that test both complex reasoning (e.g., MMLU) and domain-specific knowledge (e.g., QuALITY). We will compare the calibration and accuracy of the model's confidence scores against baseline methods such as temperature scaling and ensemble methods. We will also conduct ablation studies to identify the most effective components of the introspective prompting process."
    },
    "full_experiment_plan": {
        "Title": "Introspective Confidence Prompting: Leveraging Language Models' Self-Reflection for Uncertainty Quantification",
        "Problem Statement": "Large language models often struggle with calibrating their confidence, especially when it comes to complex reasoning tasks or domain-specific knowledge. This can lead to overconfident predictions and hinder the reliability of these models in real-world applications.",
        "Motivation": "Current methods for confidence calibration, such as temperature scaling, ensemble methods, and post-hoc calibration techniques, often rely on external data or models and do not fully leverage the introspective capabilities of language models. Language models have shown the ability to engage in introspection and self-reflection when prompted appropriately. By leveraging this introspective capacity, we can encourage the model to assess its own confidence more accurately, considering factors such as the complexity of the task, the availability of relevant knowledge, and the potential for ambiguity or uncertainty in the input.",
        "Proposed Method": "We propose Introspective Confidence Prompting, a method that prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score. The prompts are designed to guide the model through a process of self-assessment, considering factors such as the difficulty of the task, the relevance of its knowledge, and the potential for multiple valid interpretations. For example, the model might be prompted to first identify the key components of the task, then assess its confidence in each component, and finally combine these assessments into an overall confidence score. The specific prompts can be tailored to different types of tasks and domains.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate Introspective Confidence Prompting on a range of benchmarks that test both complex reasoning (e.g., MMLU) and domain-specific knowledge (e.g., QuALITY). These datasets cover various domains such as science, history, and current events, and require the model to answer multiple-choice questions or generate open-ended responses.",
            "Step 2: Construct Prompts": "For each dataset, we will design a set of introspective prompts that guide the model through the self-assessment process. The prompts will be tailored to the specific characteristics of each dataset and task type. For example, for a science question, the prompts might ask the model to identify the relevant scientific concepts, assess its understanding of each concept, and consider potential edge cases or exceptions. The prompts will be designed iteratively based on the model's performance and feedback from human evaluators.",
            "Step 3: Establish Baselines": "We will compare the performance of Introspective Confidence Prompting against several baseline methods, including: (1) Direct prompting: Asking the model to directly answer the question and provide a confidence score; (2) Temperature scaling: Calibrating the model's confidence scores using a temperature parameter; (3) Ensemble methods: Combining the predictions and confidence scores of multiple models; (4) Post-hoc calibration: Adjusting the confidence scores based on the model's performance on a held-out calibration set.",
            "Step 4: Evaluate Performance": "For each dataset and method, we will evaluate the model's performance using standard metrics such as accuracy, F1 score, and calibration error (e.g., expected calibration error, maximum calibration error). We will also conduct a qualitative analysis of the model's responses and confidence scores to identify strengths, weaknesses, and potential areas for improvement.",
            "Step 5: Conduct Ablation Studies": "To better understand the contribution of each component of Introspective Confidence Prompting, we will conduct ablation studies by selectively removing or modifying different parts of the prompts. For example, we might compare the performance of prompts that only consider task difficulty versus those that also consider knowledge relevance and ambiguity. These studies will help identify the most effective components of the introspective prompting process.",
            "Step 6: Analyze and Interpret Results": "Based on the quantitative and qualitative results, we will analyze the effectiveness of Introspective Confidence Prompting in improving the calibration and accuracy of language models. We will interpret the results in light of the specific characteristics of each dataset and task type, and discuss the implications for real-world applications. We will also identify limitations and potential directions for future research."
        },
        "Test Case Examples": {
            "Example 1": {
                "Dataset": "MMLU",
                "Question": "Which of the following is NOT a type of rock?\nA) Igneous\nB) Sedimentary\nC) Metamorphic\nD) Basaltic",
                "Baseline Prompt": "Answer the following question and provide a confidence score:\n\nWhich of the following is NOT a type of rock?\nA) Igneous\nB) Sedimentary\nC) Metamorphic\nD) Basaltic",
                "Baseline Output": "D) Basaltic\nConfidence: 0.8",
                "Introspective Prompt": "Let's think through this step-by-step:\n\n1. Identify the key concepts in the question and assess your familiarity with each one:\n- Igneous rocks: Formed from cooled magma or lava. Familiarity: High\n- Sedimentary rocks: Formed from deposited sediments that are compacted and cemented together. Familiarity: High\n- Metamorphic rocks: Formed when existing rocks are subjected to high heat and pressure, causing them to change their physical and chemical properties. Familiarity: High\n- Basaltic rocks: A type of igneous rock formed from rapidly cooled lava. Familiarity: Moderate\n\n2. Consider the relationship between the concepts and the question:\nBasaltic rocks are a specific type of igneous rock, while igneous, sedimentary, and metamorphic are the three main categories of rocks. Therefore, basaltic does not belong in the same classification level as the other three options.\n\n3. Assess the confidence in your answer:\nBased on the familiarity with the key concepts and the clear distinction between the three main rock categories and the specific type of igneous rock, the confidence in the answer is high.\n\nFinal Answer: D) Basaltic\nConfidence: 0.95",
                "Introspective Output": "D) Basaltic\nConfidence: 0.95",
                "Explanation": "The introspective prompting encourages the model to break down the question, assess its familiarity with each concept, and consider the relationships between them. This leads to a more accurate confidence score compared to the baseline prompt, which does not encourage the same level of self-reflection."
            },
            "Example 2": {
                "Dataset": "QuALITY",
                "Question": "What was the main cause of the Thirty Years' War?",
                "Baseline Prompt": "Answer the following question and provide a confidence score:\n\nWhat was the main cause of the Thirty Years' War?",
                "Baseline Output": "The main cause of the Thirty Years' War was religious tensions between Protestants and Catholics in the Holy Roman Empire.\nConfidence: 0.9",
                "Introspective Prompt": "Let's think through this step-by-step:\n\n1. Identify the key aspects of the question and assess your knowledge of each one:\n- Thirty Years' War: A conflict primarily in Central Europe from 1618 to 1648. Knowledge: Moderate\n- Causes of the war: Multiple factors, including religious, political, and territorial disputes. Knowledge: Limited\n\n2. Consider potential uncertainties or ambiguities in the question:\nThe question asks for the \"main cause\" of the war, but historical events often have multiple contributing factors. It may be challenging to identify a single main cause without a more thorough analysis of the historical context.\n\n3. Assess the confidence in your answer:\nBased on the limited knowledge of the specific causes of the Thirty Years' War and the potential for multiple contributing factors, the confidence in identifying a single main cause is moderate.\n\nFinal Answer: The Thirty Years' War had multiple causes, including religious tensions between Protestants and Catholics, political power struggles within the Holy Roman Empire, and territorial disputes among European states. While religious tensions were a significant factor, it is difficult to identify a single main cause without a more in-depth analysis of the historical context.\nConfidence: 0.6",
                "Introspective Output": "The Thirty Years' War had multiple causes, including religious tensions between Protestants and Catholics, political power struggles within the Holy Roman Empire, and territorial disputes among European states. While religious tensions were a significant factor, it is difficult to identify a single main cause without a more in-depth analysis of the historical context.\nConfidence: 0.6",
                "Explanation": "The introspective prompting helps the model recognize the limitations of its knowledge and the potential for multiple contributing factors in a complex historical event. This leads to a more nuanced and uncertain response compared to the baseline prompt, which overconfidently asserts a single main cause."
            }
        },
        "Fallback Plan": "If the proposed Introspective Confidence Prompting method does not consistently improve the calibration and accuracy of language models compared to the baselines, we will conduct additional analyses to identify potential reasons for the lack of improvement. These analyses may include:\n\n1. Examining the quality and relevance of the introspective prompts for each dataset and task type. We will assess whether the prompts effectively guide the model through the self-assessment process and identify areas for improvement.\n\n2. Investigating the relationship between the model's performance and the complexity of the task or the domain-specificity of the knowledge required. We will analyze whether the effectiveness of Introspective Confidence Prompting varies depending on these factors and consider potential modifications to the prompting strategy.\n\n3. Exploring alternative prompting techniques or combining Introspective Confidence Prompting with other confidence calibration methods. For example, we may investigate the use of multi-stage prompting, where the model first generates a response and then assesses its confidence in a separate step.\n\nBased on the findings from these additional analyses, we will propose modifications to the Introspective Confidence Prompting method or develop alternative approaches for improving the calibration and accuracy of language models. If the results suggest that the current approach is fundamentally limited, we will shift the focus of the project towards a more in-depth analysis of the factors influencing the model's confidence and the challenges associated with uncertainty quantification in language models. This analysis could provide valuable insights for future research on confidence calibration and inform the development of more effective methods."
    },
    "novelty_queries": [
        "KeywordQuery(\"introspective prompting language models confidence calibration\")",
        "KeywordQuery(\"self-reflection language models uncertainty quantification\")",
        "KeywordQuery(\"language models self-assessment confidence scores\")",
        "KeywordQuery(\"Introspective Confidence Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the calibration of language models' confidence scores, especially for models fine-tuned with reinforcement learning from human feedback (RLHF). The proposed approach is Introspective Confidence Prompting, which guides the model through a series of self-reflective steps to assess its confidence more accurately.\n\nThe research problem in the paper is also improving the calibration of confidence scores for RLHF-based language models. The approach is to use verbalized confidences emitted as output tokens, which are found to be better-calibrated than the model's conditional probabilities.\n\nBoth the proposal and the paper aim to improve the calibration of confidence scores for RLHF-based language models, but their approaches differ. The proposal suggests using introspective prompting to guide the model's self-assessment, while the paper finds that verbalized confidences emitted as output tokens are better-calibrated.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "paperId": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fact-and-Reflection prompting is proposed, which improves the LLM calibration in two steps, and even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the confidence calibration of language models, especially when it comes to complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is also improving the confidence calibration of language models in the context of question-answering. The proposed approach is Fact-and-Reflection (FaR) prompting, which elicits relevant \"facts\" from the language model and then asks the model to \"reflect\" over them to generate the final answer.\n\nBoth the proposal and the paper aim to improve confidence calibration of language models, but their approaches differ. The proposal focuses on introspective prompting for self-assessment, while the paper proposes a two-step process of fact elicitation and reflection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "paperId": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "title": "Metacognitive Prompting Improves Understanding in Large Language Models",
            "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes that consistently outperforms existing prompting methods in both general and domain-specific NLU tasks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the confidence calibration of language models, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is improving the understanding abilities of large language models in natural language understanding tasks. The proposed approach is Metacognitive Prompting, which uses a series of structured, self-aware evaluations to enhance the model's understanding.\n\nWhile both the proposal and the paper aim to improve the performance of language models through prompting techniques that involve self-reflection or introspection, the specific research problems and approaches differ. The proposal focuses on confidence calibration, while the paper focuses on understanding abilities in NLU tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "paperId": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
            "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for black-box LLMs. We first differentiate uncertainty vs confidence: the former refers to the\"dispersion\"of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to selective NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of language models' confidence scores, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is uncertainty quantification for black-box large language models in natural language generation. The proposed approach is to differentiate uncertainty vs confidence and propose and compare several confidence/uncertainty metrics, applying them to selective natural language generation.\n\nWhile both works focus on confidence and uncertainty in language models, the proposal specifically targets improving confidence calibration through introspective prompting, while the paper investigates uncertainty quantification metrics for black-box models in natural language generation. The approaches and problem settings differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
            "paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
            "year": 2023,
            "citationCount": 98,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) is introduced that enhances an LM's quality and factuality through retrieval and self-reflection and shows significant gains in improving factuality and citation accuracy for long-form generations relative to state-of-the-art models."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the confidence calibration of language models, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in self-reflection before providing a final answer and confidence score.\n\nThe research problem in the paper is improving the factual accuracy of language models. The proposed approach is Self-Reflective Retrieval-Augmented Generation (Self-RAG), which trains a language model to adaptively retrieve passages and generate and reflect on retrieved passages and its own generations.\n\nWhile both works aim to improve the performance of language models, the specific research problems and proposed approaches are different. The proposal focuses on confidence calibration, while the paper focuses on factual accuracy. The proposal uses introspective prompting, while the paper uses retrieval augmentation and self-reflection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "paperId": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified calibration framework is introduced, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores and two confidence elicitation methods based on self-consistency and self-evaluation are proposed."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the calibration of language models' confidence scores by leveraging their introspective capabilities through a series of self-reflective prompts. The paper also focuses on calibrating language models, but specifically for long-form generation tasks, by treating the correctness and confidence scores as distributions and proposing new metrics and methods based on self-consistency and self-evaluation.\n\nWhile both the project proposal and the paper address the problem of calibrating language models' confidence scores, the project proposal focuses on leveraging introspective prompts for general language model calibration, while the paper specifically targets long-form generation tasks and proposes new metrics and methods based on self-consistency and self-evaluation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of language models' confidence scores, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is improving the calibration of language models for misinformation mitigation. The proposed approach is a hybrid framework that combines sample-based consistency methods and direct confidence elicitation using numeric verbalization prompts.\n\nWhile both the proposal and the paper aim to improve the calibration of language models, they focus on different application domains (complex reasoning and domain-specific knowledge vs. misinformation mitigation) and propose different approaches (introspective prompting vs. a hybrid framework of sample-based consistency and numeric verbalization).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "paperId": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
            "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By modifying the explanations of large language models to more accurately reflect the LLM's internal confidence, a significant shift in user perception is observed, aligning it more closely with the model's actual confidence levels."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of language models' confidence scores, while the approach is using introspective prompts to guide the model to assess its own confidence more accurately.\n\nThe research problem in the paper is studying the gap between the model's internal confidence and the user's perceived confidence in the model's outputs, and the approach is modifying the model's explanations to better align the user's perception with the model's actual confidence.\n\nAlthough both works aim to improve the calibration of language models, the proposal focuses on improving the model's internal confidence calibration, while the paper focuses on aligning the user's perceived confidence with the model's confidence. The approaches are also different: introspective prompts vs. modifying explanations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting",
            "abstract": "While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using a multilingual closed-book QA task and GPT-3.5, how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings is explored."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of language models' confidence scores, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is exploring how well large language models are calibrated and express certainty across a diverse set of languages, including low-resource settings. The approach is using a multilingual closed-book QA task and GPT-3.5 to evaluate the model's performance and calibration.\n\nWhile both the proposal and the paper address the calibration of language models' confidence scores, the proposal focuses on improving calibration through introspective prompting, while the paper explores the current state of calibration across different languages without proposing a specific method for improvement.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "paperId": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
            "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work exploits the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources, and introduces a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is improving the confidence calibration of language models, especially for complex reasoning tasks or domain-specific knowledge. The proposed approach is Introspective Confidence Prompting, which prompts the language model to engage in a series of self-reflective steps before providing a final answer and confidence score.\n\nThe research problem in the paper is selecting high-quality instruction tuning data for large language models. The proposed approach is SelectIT, which exploits the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources.\n\nWhile both the project proposal and the paper aim to improve the performance of language models, they focus on different aspects (confidence calibration vs. instruction tuning data selection) and propose different approaches (introspective prompting vs. uncertainty-aware data selection). Therefore, the two works are not directly relevant.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 6
        },
        {
            "id": "8c5acaafe43e710d55b08c63d567550ad26ec437",
            "paperId": "8c5acaafe43e710d55b08c63d567550ad26ec437",
            "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
            "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different LLMs and three languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification that is competitive with a fact-checking tool that leverages external knowledge is proposed."
            },
            "score": 6
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 6
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 6
        },
        {
            "id": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "paperId": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content."
            },
            "score": 6
        },
        {
            "id": "525ff089b6c5df9b311124adc3e549060093c982",
            "paperId": "525ff089b6c5df9b311124adc3e549060093c982",
            "title": "Do Large Language Models Show Human-like Biases? Exploring Confidence - Competence Gap in AI",
            "abstract": "This study investigates self-assessment tendencies in Large Language Models (LLMs), examining if patterns resemble human cognitive biases like the Dunning\u2013Kruger effect. LLMs, including GPT, BARD, Claude, and LLaMA, are evaluated using confidence scores on reasoning tasks. The models provide self-assessed confidence levels before and after responding to different questions. The results show cases where high confidence does not correlate with correctness, suggesting overconfidence. Conversely, low confidence despite accurate responses indicates potential underestimation. The confidence scores vary across problem categories and difficulties, reducing confidence for complex queries. GPT-4 displays consistent confidence, while LLaMA and Claude demonstrate more variations. Some of these patterns resemble the Dunning\u2013Kruger effect, where incompetence leads to inflated self-evaluations. While not conclusively evident, these observations parallel this phenomenon and provide a foundation to further explore the alignment of competence and confidence in LLMs. As LLMs continue to expand their societal roles, further research into their self-assessment mechanisms is warranted to fully understand their capabilities and limitations.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates self-assessment tendencies in Large Language Models, examining if patterns resemble human cognitive biases like the Dunning\u2013Kruger effect, and shows cases where high confidence does not correlate with correctness, suggesting overconfidence and low confidence despite accurate responses indicates potential underestimation."
            },
            "score": 6
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 6
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 6
        },
        {
            "id": "19da40fd01c711fb2b3b0b19b3956b86b75f575d",
            "paperId": "19da40fd01c711fb2b3b0b19b3956b86b75f575d",
            "title": "CoNAL: Anticipating Outliers with Large Language Models",
            "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on OOD examples. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel labels, then generate examples from each novel class matching the task format. Second, we train our classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on OOD examples over prior methods by an average of 2.3% AUAC and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.1",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them, which improves classifiers' ability to detect and abstain on OODExamples over prior methods."
            },
            "score": 6
        },
        {
            "id": "fed7e4a0e8c798777f3f1613be62a2dfb776b462",
            "paperId": "fed7e4a0e8c798777f3f1613be62a2dfb776b462",
            "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
            "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CoNAL is introduced, a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them, which improves classifiers' ability to detect and abstain on novel class examples over prior methods."
            },
            "score": 6
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 5
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 5
        },
        {
            "id": "30669080bc6652f0466fba618b7c59317a346fb2",
            "paperId": "30669080bc6652f0466fba618b7c59317a346fb2",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and formalizes two distinct types of risk: decision risk and composite risk, and proposes a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings."
            },
            "score": 5
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 5
        },
        {
            "id": "67d0aa08d22feb0dbd36defece61256a4a1a0282",
            "paperId": "67d0aa08d22feb0dbd36defece61256a4a1a0282",
            "title": "Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science",
            "abstract": "Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigating how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs finds that RALMs are overconfident in their predictions."
            },
            "score": 5
        },
        {
            "id": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "paperId": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
            "abstract": "In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties in large language models, and shows a substantial improvement in model uncertainty calibration."
            },
            "score": 5
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 5
        },
        {
            "id": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "title": "Do Large Language Models Know What They Don't Know?",
            "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to evaluate large language models' self-knowledge by assessing their ability to identify unanswerable or unknowable questions, and introduces an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self- knowledge."
            },
            "score": 5
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 5
        },
        {
            "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "paperId": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "title": "Large Language Models are Better Reasoners with Self-Verification",
            "abstract": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes and proves that LLMs also have similar self-verification abilities, and takes the conclusion obtained by CoT as one of the conditions for solving the original problem."
            },
            "score": 5
        },
        {
            "id": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "paperId": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "title": "Quantifying confidence shifts in a BERT-based question answering system evaluated on perturbed instances",
            "abstract": "Recent work on transformer-based neural networks has led to impressive advances on multiple-choice natural language processing (NLP) problems, such as Question Answering (QA) and abductive reasoning. Despite these advances, there is limited work still on systematically evaluating such models in ambiguous situations where (for example) no correct answer exists for a given prompt among the provided set of choices. Such ambiguous situations are not infrequent in real world applications. We design and conduct an experimental study of this phenomenon using three probes that aim to \u2018confuse\u2019 the model by perturbing QA instances in a consistent and well-defined manner. Using a detailed set of results based on an established transformer-based multiple-choice QA system on two established benchmark datasets, we show that the model\u2019s confidence in its results is very different from that of an expected model that is \u2018agnostic\u2019 to all choices that are incorrect. Our results suggest that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances. Auxiliary results suggest that the model may not be able to distinguish between these two situations with sufficient certainty. Stronger testing protocols and benchmarking may hence be necessary before such models are deployed in front-facing systems or ambiguous decision making with significant human impact.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances, and that stronger testing protocols and benchmarking may be necessary before transformer-based multiple-choice NLP models are deployed in front-facing systems or ambiguous decision making with significant human impact."
            },
            "score": 5
        },
        {
            "id": "b626560f19f815808a289ef5c24a17c57320da70",
            "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
            "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
            "year": 2023,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions, using the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results."
            },
            "score": 4
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 4
        },
        {
            "id": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "paperId": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts."
            },
            "score": 4
        },
        {
            "id": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "paperId": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "year": 2022,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper systematically designs experiments on three NLU tasks and proposes a novel mixup strategy for pre-trained language models that improves model calibration further and achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy."
            },
            "score": 4
        },
        {
            "id": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "title": "Large Language Models Can Self-Improve",
            "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
            "year": 2022,
            "citationCount": 265,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a pre-trained LLM to generate \"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs to improve the general reasoning ability."
            },
            "score": 4
        },
        {
            "id": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "paperId": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the calibration of MMLMs in the zero-shot setting and observes a clear case of miscalibration in low-resource languages or those which are typologically diverse from English, and empirically shows that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in thezero-shot scenario."
            },
            "score": 4
        },
        {
            "id": "551b05734eb2181c4ca009a411144e8447ed1606",
            "paperId": "551b05734eb2181c4ca009a411144e8447ed1606",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A wide range of popular options for each consideration are compared based on three prevalent NLP classification tasks and the setting of domain shift to form a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline."
            },
            "score": 4
        },
        {
            "id": "33560c52a5a90e1074a9c341b752bd9e8ac86f7d",
            "paperId": "33560c52a5a90e1074a9c341b752bd9e8ac86f7d",
            "title": "AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "abstract": "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model\u2019s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average. Our implementation is available at https://github.com/yueyu1030/actune.",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AcTune is developed, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training by switching between data annotation and model self- training based on uncertainty."
            },
            "score": 4
        },
        {
            "id": "6049f92e687e5db4ea509a83df4372099c516fd8",
            "paperId": "6049f92e687e5db4ea509a83df4372099c516fd8",
            "title": "Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty",
            "abstract": "Open Information Extraction (OIE) task aims at extracting structured facts from unstructured text, typically in the form of (subject, relation, object) triples. Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate structured output due to the restrictions on fine-tuning the model. Second, LLMs generates responses autoregressively based on probability, which makes the predicted relations lack confidence. In this paper, we assess the capabilities of LLMs in improving the OIE task. Particularly, we propose various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations. Our experiments on three OIE benchmark datasets show that our approach holds its own against established supervised methods, both quantitatively and qualitatively.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations are proposed."
            },
            "score": 4
        },
        {
            "id": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "paperId": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
            "abstract": "As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering and finds that the uncertainty estimates from conformal Prediction are tightly correlated with prediction accuracy."
            },
            "score": 4
        },
        {
            "id": "29de286404cc2561bfe22290d75bfdb9e8dc5e74",
            "paperId": "29de286404cc2561bfe22290d75bfdb9e8dc5e74",
            "title": "Increasing Context for Estimating Confidence Scores in Automatic Speech Recognition",
            "abstract": "Accurate confidence measures for predictions from machine learning techniques play a critical role in the deployment and training of many speech and language processing applications. For example, confidence scores are important when making use of automatically generated transcriptions in training automatic speech recognition (ASR) systems, as well as down-stream applications, such as information retrieval and conversational assistants. Previous work on improving confidence scores for these systems has focused on two main directions: designing features correlated with improved confidence prediction; and employing sequence models to account for the importance of contextual information. Few studies, however, have explored incorporating contextual information more broadly, such as from the future, in addition to the past, or making use of alternative multiple hypotheses in addition to the most likely one. This article introduces two general approaches for encapsulating contextual information from lattices. Experimental results illustrating the importance of increasing contextual information for estimating confidence scores are presented on a range of limited resource languages where word error rates range between 30% and 60%. The results show that the novel approaches provide significant gains in the accuracy of confidence estimation.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two general approaches for encapsulating contextual information from lattices for estimating confidence scores are introduced and it is shown that the novel approaches provide significant gains in the accuracy of confidence estimation."
            },
            "score": 4
        },
        {
            "id": "bbe5ea6dc1470b33f3396a417bd546638948f535",
            "paperId": "bbe5ea6dc1470b33f3396a417bd546638948f535",
            "title": "Self-Training With Double Selectors for Low-Resource Named Entity Recognition",
            "abstract": "Named Entity Recognition (NER) is fundamental to multiple downstream natural language processing (NLP) tasks, but most advanced NER methods heavily rely on massive labeled data with high cost. In this paper, we explore the effectiveness of self-training for low-resource NER. It is one of the semi-supervised approaches to reduce the reliance on manual annotation. However, random pseudo sample selection in standard self-training framework may cause serious error propagation, especially for token-level tasks. To that end, this paper focuses on pseudo sample selection and proposes a new self-training framework with double selectors, namely auxiliary judge task and entropy-based confidence measurement. Specifically, the auxiliary judge task is proposed to filter out the pseudo samples with wrong predictions. The entropy-based confidence measurement is introduced to select pseudo samples with high quality. In addition, to make full use of all pseudo samples, we propose a cumulative function based on the idea of curriculum learning to prompt the model to learn from easy samples to hard ones. Samples with low quality are filtered out through the double selectors, which is more conducive to the training of student models. Experimental results on five NER benchmark datasets from different languages indicate the effectiveness of the proposed framework over several advanced baselines.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new self-training framework with double selectors, namely auxiliary judge task and entropy-based confidence measurement, and proposes a cumulative function based on the idea of curriculum learning to prompt the model to learn from easy samples to hard ones."
            },
            "score": 4
        },
        {
            "id": "426f728b9023a1815ac273d06ed281e97be74cc6",
            "paperId": "426f728b9023a1815ac273d06ed281e97be74cc6",
            "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes",
            "abstract": "There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner and improves accuracy in complex clinical decision reasoning problems compared to other prompt methods."
            },
            "score": 4
        },
        {
            "id": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "paperId": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
            "abstract": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance."
            },
            "score": 3
        },
        {
            "id": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "paperId": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities",
            "abstract": "We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image and that aggregating analyses over all concepts can mask these concerns; (ii) model calibration (i.e. the relationship between accuracy and confidence) also differs distinctly by perceived gender, even when evaluating on similar representations of concepts; and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can also contribute to social biases in zero-shot vision settings. Furthermore, biases can further propagate when foundational models like CLIP are used by other models to enable zero-shot capabilities.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates different vision-language models with multiple datasets across a set of concepts and finds all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image."
            },
            "score": 3
        },
        {
            "id": "ef752adcfb09ea02e75a2e1133476a1a70a710ea",
            "paperId": "ef752adcfb09ea02e75a2e1133476a1a70a710ea",
            "title": "Uncertainty Quantification in Deep Learning",
            "abstract": "Deep neural networks (DNNs) have achieved enormous success in a wide range of domains, such as computer vision, natural language processing and scientific areas. However, one key bottleneck of DNNs is that they are ignorant about the uncertainties in their predictions. They can produce wildly wrong predictions without realizing, and can even be confident about their mistakes. Such mistakes can cause misguided decisions-sometimes catastrophic in critical applications, ranging from self-driving cars to cyber security to automatic medical diagnosis. In this tutorial, we present recent advancements in uncertainty quantification for DNNs and their applications across various domains. We first provide an overview of the motivation behind uncertainty quantification, different sources of uncertainty, and evaluation metrics. Then, we delve into several representative uncertainty quantification methods for predictive models, including ensembles, Bayesian neural networks, conformal prediction, and others. We go on to discuss how uncertainty can be utilized for label-efficient learning, continual learning, robust decision-making, and experimental design. Furthermore, we showcase examples of uncertainty-aware DNNs in various domains, such as health, robotics, and scientific machine learning. Finally, we summarize open challenges and future directions in this area.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial presents recent advancements in uncertainty quantification for DNNs and their applications across various domains, and discusses how uncertainty can be utilized for label-efficient learning, continual learning, robust decision-making, and experimental design."
            },
            "score": 3
        },
        {
            "id": "77a073d10751a9d60948909ad80d716d1b4b2400",
            "paperId": "77a073d10751a9d60948909ad80d716d1b4b2400",
            "title": "Probabilistic Uncertainty Quantification of Prediction Models with Application to Visual Localization",
            "abstract": "The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars. This paper proposes our approach to uncertainty quantification in the context of visual localization for autonomous driving, where we predict locations from images. Our proposed framework estimates probabilistic uncertainty by creating a sensor error model that maps an internal output of the prediction model to the uncertainty. The sensor error model is created using multiple image databases of visual localization, each with ground-truth location. We demonstrate the accuracy of our uncertainty prediction framework using the Ithaca365 dataset, which includes variations in lighting, weather (sunny, snowy, night), and alignment errors between databases. We analyze both the predicted uncertainty and its incorporation into a Kalman-based localization filter. Our results show that prediction error variations increase with poor weather and lighting condition, leading to greater uncertainty and outliers, which can be predicted by our proposed uncertainty model. Additionally, our probabilistic error model enables the filter to remove ad hoc sensor gating, as the uncertainty automatically adjusts the model to the input data.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the approach to uncertainty quantification in the context of visual localization for autonomous driving, where it is proposed that prediction error variations increase with poor weather and lighting condition, leading to greater uncertainty and outliers, which can be predicted by the proposed uncertainty model."
            },
            "score": 3
        },
        {
            "id": "fc709c4e746f5fef146c19a8ac7873db26a57d74",
            "paperId": "fc709c4e746f5fef146c19a8ac7873db26a57d74",
            "title": "General purpose large language models match human performance on gastroenterology board exam self-assessments.",
            "abstract": "Introduction: While general-purpose large language models(LLMs) were able to pass USMLE-style examinations, their ability to perform in a specialized context, like gastroenterology, is unclear. In this study, we assessed the performance of three widely available LLMs: PaLM-2, GPT-3.5, and GPT-4 on the most recent ACG self-assessment(2022), utilizing both a basic and a prompt-engineered technique. Methods: We interacted with the chat interfaces of PaLM-2, GPT-3.5, and GPT-4. We first applied a basic prompt approach, providing each exam question and answer text with minimalist text descriptions of any images. For the engineered approach, we added additional context and instructions. We assessed each model-prompt combination in terms of overall and difficulty-stratified performance and compared this to average human performance. We also evaluated each models self-assessed uncertainty. The highest scoring model-prompt combination was further assessed on the 2021 exam. We also assessed the impact of image descriptions on our findings. Results: Using a basic prompt, PaLM-2, GPT-3.5, and GPT-4 achieved scores of 32.6%, 55.3%, and 68.9% respectively. With the engineered prompt, scores improved to 42.7%, 65.2%, and 76.3% respectively. Testing GPT-4 on the ACG-2021 exam yielded a similar score(75.3%). GPT-4 scores matched the average score for human test-takers reported by ACG(75.7%). GPT-4 showed a capability to self-assess its confidence accurately in the context of a multiple-choice exam with its confidence estimates falling within 5% of its actual performance. Excluding image-based questions didnt change the primary findings. Discussion: Our study highlights the capability of GPT-4 to answer subspecialty board-exam questions at a level commensurate with the average human test-taker. The results confirm that prompt-engineering can enhance LLMs performance on medical reasoning tasks. We also show GPT-4 can provide insightful measures of uncertainty in the setting of board-style multiple-choice questions, alerting users to low-quality answers. Future studies of LLMs in gastroenterology should incorporate prompt-engineering to maximize model capabilities.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results confirm that prompt-engineering can enhance LLMs performance on medical reasoning tasks and show GPT-4 can provide insightful measures of uncertainty in the setting of board-style multiple-choice questions, alerting users to low-quality answers."
            },
            "score": 3
        },
        {
            "id": "cd4d112f3f9120d0715f22a9de2ce4720822368c",
            "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c",
            "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
            "abstract": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT\u2019s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT\u2019s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT\u2019s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.",
            "year": 2023,
            "citationCount": 598,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams is evaluated to analyze responses for user interpretability and highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers."
            },
            "score": 3
        },
        {
            "id": "25f3f1bb21fb78722faaf422b60c6edadbd4c3de",
            "paperId": "25f3f1bb21fb78722faaf422b60c6edadbd4c3de",
            "title": "Comprehensive analysis of the performance of GPT-3.5 and GPT-4 on the American Urological Association self-assessment study program exams from 2012-2023.",
            "abstract": "INTRODUCTION\nArtificial intelligence (AI) applications, specifically generative pre-trained transformers, have shown potential in medical education and board-style examinations. To assess this capability, we conducted a study comparing the performance of GPT-3.5 and GPT-4 on the American Urological Association (AUA) 2022 self-assessment study program (SASP) exams from 2012-2023.\n\n\nMETHODS\nWe used a standardized prompt to administer questions from the AUA SASP exams spanning 2012-2023, totalling 1679 questions. The performance of the two AI models, GPT-3.5 and GPT-4, was evaluated based on the number of questions answered correctly. Statistical analysis was performed using Fisher's exact test and independent sample t-tests to compare the performance of GPT-4 to that of GPT-3.5 among test years and urology topic areas. Percentile scores were not calculable, however, a score of 50% is required to acquire CME credits on AUA SASP exams.\n\n\nRESULTS\nThe analysis showed significantly superior performance by GPT-4, which scored above 50% across all exam years except 2018, with scores ranging from 48-64%. In contrast, GPT-3.5 consistently scored below this threshold, with scores ranging from 26-38%. The total combined score for GPT-4 was 55%, significantly higher than the 33% achieved by GPT-3.5 (odds ratio [OR] 2.5, 95% confidence interval [CI] 2.2-2.9, p<0.001). GPT-4 significantly outperformed GPT-3.5 among AUA SASP test years from 2012-2023 (mean difference 23, t(22) 14, 95% CI 19-26, p<0.001), as well as among urology topic areas (mean difference 21, t(52)=5.5, 95% CI 13-29, p<0.001).\n\n\nCONCLUSIONS\nGPT-4 scored significantly higher than GPT-3.5 on the AUA SASP exams in overall performance, across all test years, and in various urology topic areas. This suggests improvement in evolving AI language models in answering clinical urology questions; however, certain aspects of medical knowledge and clinical reasoning remain challenging for AI language models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-4 scored significantly higher than GPT-3.5 on the AUA SASP exams in overall performance, across all test years, and in various urology topic areas, which suggests improvement in evolving AI language models in answering clinical urology questions."
            },
            "score": 3
        },
        {
            "id": "230137d02910e43a9d161e21af24b80fd94d351e",
            "paperId": "230137d02910e43a9d161e21af24b80fd94d351e",
            "title": "Feature Normalization and Cartography-Based Demonstrations for Prompt-Based Fine-Tuning on Emotion-Related Tasks",
            "abstract": "To train a model in a traditional supervised learning classification system for natural language processing (NLP) tasks, it is essential to have labeled data, which is not present in large amounts for many tasks. Prompt-based learning methods attempt to combat the supervised learning need for labeled data by directly adapting pre-trained language models and modeling the probability of text itself. In this paper, we propose a novel data-agnostic strategy for prompt-based fine-tuning that leverages feature moments (a.k.a., mean and standard deviation) as a data augmentation technique and employs training dynamics (i.e., confidence and variability) to allow more informative samples to be concatenated for generating demonstrations as input context. Our approach is a strong method for few-shot learning that forces the language model to pay special attention to the feature moments and allows more informative samples to be concatenated for generating demonstrations as input context by selecting high confidence and low variance samples. To demonstrate its effectiveness given limited training data, we conduct extensive experiments in different few-shot settings on three empathy and emotion classification datasets (from various domains). We further evaluate our method's robustness by introducing noise to our few-shot input data and labels and show that exchanging moments between samples and incorporating cartography-based demonstrations are beneficial when the available data is limited and noisy.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel data-agnostic strategy for prompt-based fine-tuning that leverages feature moments (a.k.a., mean and standard deviation) as a data augmentation technique and employs training dynamics to allow more informative samples to be concatenated for generating demonstrations as input context."
            },
            "score": 3
        },
        {
            "id": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "paperId": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
            "abstract": "Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reasoning Question Prompts for VQA tasks are presented, which can further activate the potential of Large Language Models in zero-shot scenarios and can significantly improve the results of LLMs on zero- shot setting and outperform existing state-of-the-art zero-Shot methods on three out of four data sets."
            },
            "score": 3
        },
        {
            "id": "927fc7652e033c9eb17296df087e3e6491112bb0",
            "paperId": "927fc7652e033c9eb17296df087e3e6491112bb0",
            "title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
            "abstract": "Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study evaluates the capabilities of four LLMs in addressing several analytical problems with graph data, using four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification."
            },
            "score": 2
        },
        {
            "id": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "paperId": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "title": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models",
            "abstract": "Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available here.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts and shows that LASP is inherently amenable to including, during training, virtual classes."
            },
            "score": 2
        },
        {
            "id": "e391d266b0d43475567f59efeaeabc884a48abd0",
            "paperId": "e391d266b0d43475567f59efeaeabc884a48abd0",
            "title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
            "abstract": "Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions and incorporating the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions."
            },
            "score": 2
        },
        {
            "id": "94b7c338c1478f3c2740cdf544c273df6a35a60f",
            "paperId": "94b7c338c1478f3c2740cdf544c273df6a35a60f",
            "title": "Part-of-Speech Tagging for low resource languages: Activation function for deep learning network to work with Minimal Training Data",
            "abstract": "Numerous natural language processing (NLP) applications exist today, especially for the most commonly spoken languages like English, Chinese, and Spanish. Popular traditional methods like Naive Bayes classifiers, Hidden Markov models, Conditional Random field-based classifiers, and other stochastic methods have contributed to this improvement over the last three decades. Recently, deep learning has led to exciting breakthroughs in several areas of artificial intelligence, including image processing and natural language processing. It is important to label words as parts of speech to begin developing most of the NLP applications. A deep study in this area reveals that these approaches require massive training data. Therefore, these approaches have not been helpful for languages not rich in digital resources. Applying these methods with very little training data prompts the need for innovative problem-solving. This paper describes our research, which examines the strengths and weaknesses of well-known approaches, such as conditional random fields and state-of-the-art deep learning models, when applied for part-of-speech tagging using minimal training data for Assamese and English. We also examine the factors affecting them. We discuss our deep learning architecture and the proposed activation function, which shows promise with little training data. The activation function categorizes words belonging to different classes with more confidence by using the outcomes of statistical methods. With minimal training, our deep learning architecture using the proposed PSM-Taylor SoftMax improves accuracy by 4%\u20139%, This technique is a combination of SMTaylor SoftMax and probability distribution.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The strengths and weaknesses of well-known approaches, such as conditional random fields and state-of-the-art deep learning models, when applied for part-of-speech tagging using minimal training data for Assamese and English are examined."
            },
            "score": 2
        },
        {
            "id": "c51950111fc29ac83be64a5e220d93b202dc07ef",
            "paperId": "c51950111fc29ac83be64a5e220d93b202dc07ef",
            "title": "Construction of an Assisted Model Based on Natural Language Processing for Automatic Early Diagnosis of Autoimmune Encephalitis",
            "abstract": null,
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The assisted diagnostic model could effectively increase the early diagnostic sensitivity for AE compared to previous diagnostic criteria, assist physicians in establishing the diagnosis of AE automatically after inputting the HPI and the results of standard paraclinical tests according to their narrative habits for describing symptoms, avoiding misdiagnosis and allowing for prompt initiation of specific treatment."
            },
            "score": 2
        },
        {
            "id": "17c9263269e0d18c55b18e08d6390c03e3a1b99e",
            "paperId": "17c9263269e0d18c55b18e08d6390c03e3a1b99e",
            "title": "The Effects of Achievement\u2011Based Assessment on Reading Proficiency, Academic Impulse, Language Apprehension, and Learners\u2019 Self\u2011Perceptions",
            "abstract": "The different sorts of assessment assignments have an impact on both the learners' psychological health and the learning process. Educationalists have spent years trying to identify and put into practice accurate and practical methods for effectively assessing students. Few research studies have tried to investigate how achievement-centered evaluation impacts reading perception, learning impulse, language apprehension, and students' self-confidence assessment despite the significant impact achievement-based has on second language learning. The current study set out to fill this gap in the literature by evaluating how achievement-based assessment affected the development of reading comprehension, academic impulse, language apprehension, and one\u2019s self-confidence. To attain the study goal, 66 intermediate EFL students were randomly split into two groups: an experimental group (EG) and a control group (CG). The students in the CG (N=31) received the conventional assessment over the course of this study (12 sessions). The learners in the (EG) (N = 35), received an achievement-based assessment. The One-Way MANOVA Test showed that students in the experimental group did better than those in the control group. The findings also demonstrated how important the role achievement-based assessment played in promoting reading comprehension, academic impulse, language apprehension, and self-confidence. The results of this study may have repercussions that are advantageous to language learners, teachers, curriculum designers, and policymakers presenting chances for further achievement-based practices.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "be35a18b76f6feb5445846b0dc18f0413d232cff",
            "paperId": "be35a18b76f6feb5445846b0dc18f0413d232cff",
            "title": "A closer look at a marginalized test method: Self-assessment as a measure of speaking proficiency",
            "abstract": "Abstract Second language (L2) teachers may shy away from self-assessments because of warnings that students are not accurate self-assessors. This information stems from meta-analyses in which self-assessment scores on average did not correlate highly with proficiency test results. However, researchers mostly used Pearson correlations, when polyserial could be used. Furthermore, self-assessments today can be computer adaptive. With them, nonlinear statistics are needed to investigate their relationship with other measurements. We wondered, if we explored the relationship between self-assessment and proficiency test scores using more robust measurements (polyserial correlation, continuation-ratio modeling), would we find different results? We had 807 L2-Spanish learners take a computer-adaptive, L2-speaking self-assessment and the ACTFL Oral Proficiency Interview \u2013 computer (OPIc). The scores correlated at .61 (polyserial). Using continuation-ratio modeling, we found each unit of increase on the OPIc scale was associated with a 131% increase in the odds of passing the self-assessment thresholds. In other words, a student was more likely to move on to higher self-assessment subsections if they had a higher OPIc rating. We found computer-adaptive self-assessments appropriate for low-stakes L2-proficiency measurements, especially because they are cost-effective, make intuitive sense to learners, and promote learner agency.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Computer-adaptive self-assessments appropriate for low-stakes L2-proficiency measurements are found, especially because they are cost-effective, make intuitive sense to learners, and promote learner agency."
            },
            "score": 1
        },
        {
            "id": "9cba385fd82cfe0491e37a07cdc8b89abf246b72",
            "paperId": "9cba385fd82cfe0491e37a07cdc8b89abf246b72",
            "title": "References and Notes Supporting Online Material Relating Introspective Accuracy to Individual Differences in Brain Structure",
            "abstract": "the maturation of miR-16 from its precursor pre/pri\u2013miR-16. Raphe additionally responds to chronic fluoxetine treatment by releasing S100b, which in turn acts on the noradrenergic neurons of the locus coeruleus. By lowering miR-16 levels, S100b unlocks the expression of serotonergic functions in this nor-adrenergic brain area. Our pharmacological and behavioral data thus posit miR-16 as a central effector that regulates SERT expression and mediates the adaptive response of serotonergic and noradrenergic neurons to fluoxetine treatment. The ability to introspect about self-performance is key to human subjective experience, but the neuroanatomical basis of this ability is unknown. Such accurate introspection requires discriminating correct decisions from incorrect ones, a capacity that varies substantially across individuals. We dissociated variation in introspective ability from objective performance in a simple perceptual-decision task, allowing us to determine whether this interindividual variability was associated with a distinct neural basis. We show that introspective ability is correlated with gray matter volume in the anterior prefrontal cortex, a region that shows marked evolutionary development in humans. Moreover, interindividual variation in introspective ability is also correlated with white-matter microstructure connected with this area of the prefrontal cortex. Our findings point to a focal neuroanatomical substrate for introspective ability, a substrate distinct from that supporting primary perception. O ur moment-to-moment judgments of the outside world are often subject to introspective interrogation. In this context, introspective or \" metacognitive \" sensitivity refers to the ability to discriminate correct from incorrect perceptual decisions (1), and its accuracy is essential for the appropriate guidance of decision-making and action (2, 3). For example, low confidence that a recent decision was correct may prompt us to reexamine the evidence or seek a second opinion. Recently, behavioral studies have begun to quantify metacognitive accuracy following simple perceptual decisions and to isolate variations in this ability: A decision may be made poorly, yet an individual may believe that his or her performance was good, or vice versa (4\u20138). Whereas previous work has investigated how confidence in perceptual decisions varies from trial to trial (9, 10), little is known about the biological basis of metacognitive ability , defined here as how well an individual's confidence ratings discriminate correct from incorrect decisions over time. We hypothesized that individual differences in metacognitive ability would be reflected in the anatomy of brain regions responsible for this function, in line with similar associations between brain anatomy and performance in other cognitive domains (11\u201315). \u2026",
            "year": null,
            "citationCount": 665,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that introspective ability is correlated with gray matter volume in the anterior prefrontal cortex, a region that shows marked evolutionary development in humans, and interindividual variation in introspective ability is also correlated with white-matter microstructure connected with this area of the prefrontal cortex."
            },
            "score": 1
        },
        {
            "id": "e36aa34b32cae9dce744bed5410b35dfefd8a897",
            "paperId": "e36aa34b32cae9dce744bed5410b35dfefd8a897",
            "title": "SCIENTIFIC",
            "abstract": "Experience in the last war has shown that prompt treatment is essential to prevent these functional disorders from becoming systematized into a persistent habit of mind. The further the man is removed from the forward area. the less -chance there is of early cure. To the medical man on the spot will fall the task of ensuring that the man's self-confidence is restored as quickly as possible. Reassurance that no physical hurt has been sustained and an appeal to the man's sense of duty will tend to allay his fears. A short rest with his unit, in order to allow the initial shock to setttedown, would be advisable: but as early as possible strong efforts should be directed towards persuading the man to view his symptoms in their true perspective and to face up to his difficulties. In the severer cases and in the treatment .of those resistant to simple measures there will be need for more elaborate methods, such as were instituted at various neurologicai centres in France during 1916 and 1917. Units of this tyne were situated ten to twelve miles behind the line, with the intention of supplying prompt treatment, of ensuring a measure of security from shell-fire while retaining the suggestion of a forward area. Everything possible was done to encourage an \"atmosphere of cure\", and the patient was made to' feel that all hope was not lost. A period of rest in' bed for three or four days with good food and adequate rest was allowed and then active treatment along definite lines was undertaken. Even during this preliminary resting stage hysterical patients were given strong suggestions as to the curability of their symptoms. Insomnia was dealt with by adequate sedative treatment in order to reestablish the sleep habit. The patients were dealt with individually and each man's case was discussed with him at private interviews. Explanations as to the nature and origin of his symptoms were given, so that he might understand, for instance, why he suffered from tremors, why his heart beat rapidly or why he experienced difficulty in concentration. In the conversion states the majority of patients responded very readily to treatment by suggestion and persuasion. supplemented when necessary by faradism and remedial exercises. Hypnosis is spoken of very appreciatively by medical men who worked in these neurological centres in France. as a quick and successful means of resolving hysterical manifestations. As improvement in the patient's condition set in, graduated exercises were added to the daily routine of treatment as a measure of reeducation towards the attainment of fitness. Useful occupations, in which the patient could take an intelligent interest, were also arranged, in order to combat introspective tendencies and consequent mental depression. The period of hospitalization was made as short as was conststent with return to fitness, and this was usually attained in a matter of weeks. The results of this line of treatment were remarkably good. A certain number of patients, especially those of marked neuropathic temperament, did not respond to treatment. They had passed out of the acute stage and the neurosis had become fixed. The t.reatrnent of this type of patient involves the use of psychotherapy in all its varied forms. and requires much patience and specialized experience on the part of the medical attendant. C. C. MINTY, Senior Medical Officer, Repatriation Department, Brisbane. \u2022 15titisb a.0tOical association Jat\\tJs.",
            "year": 1940,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The majority of patients responded very readily to treatment by suggestion and persuasion, and as improvement in the patient's condition set in, graduated exercises were added to the daily routine of treatment as a measure of reeducation towards the attainment of fitness."
            },
            "score": 1
        },
        {
            "id": "6e69936416a5914908bf744011eb3977278dac86",
            "paperId": "6e69936416a5914908bf744011eb3977278dac86",
            "title": "Strategies of Imagining",
            "abstract": "C.M. HARCLYDE WALCOTT HAS WRITTEN AND PRODUCED an impressive and engaging collection. It is the kind of work that communicates with the reader and compels our involvement. The title of the collection gives some indication of the treats, strategies and tricks that the poet weaves, works and fashions within the poetry. Like me, you might misread the title and see the word ?imaginings?, so you might begin to think of the collection as a collage of imaginings, and you might feel the poet is saying that all poems are something imagined. But the proper title is: imagining and other poems} This would suggest that the poem ?imagining? is the title poem, in the way that the well-marketed music album might have a title track that serves as a marker for its promotion, play and artistic response. This is one way of approaching Harclyde Walcott?s collection, for it does have a definite structure. When you read it you know it is conceptualised by someone who takes care with how works of art are staged; but you soon discover, whichever way you approach the collection, that there are many tracks of experiences that open up to powerful, poignant and at times playful fragments of memory, desire and imaginings (individual and collective). The work cannot be simply read and put down. It challenges the reader to assess the relationship between our imaginings and the material realities that inform our daily actions.Themes and narrative voice/sWhether this work compels you to read its stems of experiences in quiet spaces or aloud, it prompts you to respond in real time, partly because of the writer's themes, characters and clever delivery. One of the things that holds this wellcrafted collection together is its focus on a range of familiar, yet misunderstood themes and concerns, such as love, desire, distance, solitude, dream, memory, delusion, difference, life, death, pleasure and ritual. This rich diversity of concerns is bound to appeal to many individuals. It will speak to you if you have ever been in love, if you have ever fallen out of love, if you have ever felt alone, if being apart from others has ever brought you an uncanny power. Have you ever wondered if it is okay, and what it is like, to speak about friends and acquaintances in creative literature and not betray their trust or confidence? And have you ever contemplated how to respond to life's daily and unexpected situations? If so, then this work is for you, as it is also about you. Through the foregrounding of universal and personal themes, the poet creates a work that is introspective, playful, resourceful - and 'dangerous'.While the work focuses on a set of core universal and personal themes and sub-themes, the reader cannot help but assess the narrative/poetic voicing that serves as a connecting device throughout. No, the poet's attitude cannot be easily deciphered in all instances, but this is a pleasurable component of the collection. At times his attitude is that of a distant observer, at times the tone and manner is reverent, respectful, admiring, adoring, affectionate, appreciative, hopeful, and then again it is ambivalent, contemptuous, fatalistic, and terse. But in many instances there is an ironic delivery which operates in consort with playful diction and the clever crafting of words and phrases to produce a work that is brutally yet delightfully honest.Influences, references, allusionsWhile I was reading this collection I could not help but contemplate two related questions that students continuously raise in courses on the history of West Indian literature: what is the nature ofWest Indian literary classics; and, how vital is inter-textual practice in establishing and validating our literary tradition (in other words, how do works reference each other and to what ends)? Harclyde Walcott's work is reflective of other creative works that circulate within the tradition ofWest Indian literature, and like many others, thankfully, it participates knowingly and unknowingly within the arena inhabited by Nobel laureates, sober-minded visionaries and bona fide activists. \u2026",
            "year": 2017,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}