{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "ideas": {
        "Semantic-Aware Prompting": {
            "Problem": "Large language models often generate responses that are semantically inconsistent with the input prompt, leading to factual errors and hallucinations.",
            "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation and incorporating external knowledge bases.",
            "Motivation": "We hypothesize that by explicitly modeling the semantic relationships between the input prompt and the generated response, we can guide the model to generate more factually consistent responses.",
            "Proposed Method": "We propose Semantic-Aware Prompting (SAP), a novel prompting method that incorporates semantic information into the prompt to guide the model's generation process. SAP consists of three main steps: 1) Semantic Parsing: We first parse the input prompt into a structured semantic representation using a pre-trained semantic parser. 2) Semantic Augmentation: We then augment the original prompt with the extracted semantic information, such as named entities, relations, and events. 3) Semantic Consistency Checking: During the generation process, we periodically check the semantic consistency between the generated response and the augmented prompt, and guide the model to generate responses that are semantically aligned with the input.",
            "Experiment Plan": "We plan to evaluate SAP on a range of factual question answering and knowledge-intensive generation tasks, such as TruthfulQA, FEVER, and WikiBio. We will compare SAP with state-of-the-art baselines, including retrieval-augmented generation and knowledge-grounded generation methods. We will also conduct ablation studies to investigate the effectiveness of each component in SAP."
        },
        "Counterfactual Consistency Prompting": {
            "Problem": "Large language models can generate inconsistent or contradictory statements across different parts of the generated output, leading to reduced coherence and factual accuracy.",
            "Existing Methods": "Existing methods for improving consistency include using self-consistency decoding or bootstrapping a coherence classifier to guide generation.",
            "Motivation": "We observe that large language models have the ability to reason about counterfactual scenarios and identify inconsistencies in generated text. By explicitly prompting the model to consider counterfactual scenarios and verify the consistency of its generated output, we can encourage the model to generate more coherent and factually consistent text.",
            "Proposed Method": "We propose Counterfactual Consistency Prompting, a novel prompting approach to improve the consistency and factual accuracy of generated text. First, we prompt the model to generate an initial output. Second, we prompt the model to generate counterfactual scenarios that are similar to but slightly modified from the initial output. Third, we prompt the model to verify the consistency between the initial output and the counterfactual scenarios, and to identify any contradictions or inconsistencies. Fourth, we prompt the model to revise the initial output to resolve the identified inconsistencies and generate a more coherent and factually consistent final output.",
            "Experiment Plan": "We will evaluate our proposed method on consistency benchmarks such as the Contradiction Detection task and the Abductive Natural Language Inference task. We will compare our method with baseline prompting approaches without counterfactual consistency verification, as well as state-of-the-art methods for improving consistency in generated text."
        },
        "Meta-Cognitive Prompting": {
            "Problem": "Large language models often generate overconfident responses that lack self-awareness and meta-cognitive reasoning, leading to hallucinations and factual inconsistencies.",
            "Existing Methods": "Existing methods for improving the meta-cognitive abilities of language models mainly focus on incorporating human feedback and using reinforcement learning.",
            "Motivation": "We observe that humans are able to monitor their own thought processes and regulate their behavior based on meta-cognitive reasoning. We hypothesize that by explicitly modeling the meta-cognitive processes in the prompt, we can guide the model to generate more self-aware and calibrated responses.",
            "Proposed Method": "We propose Meta-Cognitive Prompting (MCP), a novel prompting method that incorporates meta-cognitive reasoning into the generation process. MCP consists of three main steps: 1) Meta-Cognitive Query Generation: Given an input prompt, we first generate a set of meta-cognitive queries that probe the model's understanding of its own knowledge and reasoning process, such as \"What do you know about this topic?\" and \"How confident are you in your response?\" 2) Meta-Cognitive Prompt Augmentation: We then augment the original prompt with the generated meta-cognitive queries, and ask the model to generate responses that are self-aware and calibrated. 3) Meta-Cognitive Reflection: After generating the initial response, we ask the model to reflect on its own response and generate a meta-cognitive assessment, such as \"I'm not very confident in this response because I don't have enough information about the topic.\" We then use this assessment to guide the model to revise its response if needed.",
            "Experiment Plan": "We plan to evaluate MCP on a range of tasks that require meta-cognitive reasoning and self-awareness, such as open-domain question answering, language model calibration, and conversational agents. We will compare MCP with state-of-the-art baselines, including methods that incorporate human feedback and reinforcement learning. We will also conduct human evaluation to assess the self-awareness and calibration of the generated responses."
        },
        "Iterative Refinement Prompting": {
            "Problem": "Large language models often generate fluent but factually incorrect responses in one shot, without the ability to iteratively refine and correct their responses.",
            "Existing Methods": "Existing methods for iterative refinement mainly focus on using retrieval-augmented generation and incorporating human feedback.",
            "Motivation": "We observe that humans often generate initial drafts of responses and then iteratively refine them based on self-reflection and external feedback. We hypothesize that by explicitly modeling the iterative refinement process in the prompt, we can guide the model to generate more factually accurate and coherent responses.",
            "Proposed Method": "We propose Iterative Refinement Prompting (IRP), a novel prompting method that incorporates iterative refinement into the generation process. IRP consists of three main steps: 1) Initial Response Generation: Given an input prompt, we first ask the model to generate an initial response. 2) Refinement Prompt Generation: We then generate a set of refinement prompts based on the initial response, such as \"What are the potential factual errors in this response?\" and \"How can we make this response more coherent and logical?\" 3) Iterative Refinement: We iteratively refine the response by asking the model to revise its response based on the refinement prompts. We repeat this process for a fixed number of iterations or until the model generates a satisfactory response.",
            "Experiment Plan": "We plan to evaluate IRP on a range of tasks that require factual accuracy and coherence, such as question answering, summarization, and dialogue generation. We will compare IRP with state-of-the-art baselines, including retrieval-augmented generation and methods that incorporate human feedback. We will also conduct human evaluation to assess the factual accuracy and coherence of the generated responses, as well as the effectiveness of the iterative refinement process."
        },
        "Socratic Prompting": {
            "Problem": "Large language models often generate shallow and superficial responses that lack deep understanding and reasoning, leading to factual inconsistencies and logical fallacies.",
            "Existing Methods": "Existing methods for improving the reasoning abilities of language models mainly focus on using chain-of-thought prompting and incorporating external knowledge.",
            "Motivation": "We draw inspiration from the Socratic method, a form of cooperative argumentative dialogue based on asking and answering questions to stimulate critical thinking and draw out ideas and underlying presuppositions. We hypothesize that by incorporating Socratic questioning into the prompting process, we can guide the model to generate more thoughtful and well-reasoned responses.",
            "Proposed Method": "We propose Socratic Prompting (SP), a novel prompting method that incorporates Socratic questioning into the generation process. SP consists of three main steps: 1) Socratic Question Generation: Given an input prompt, we first generate a set of Socratic questions that probe the model's understanding and reasoning process, such as \"What are the assumptions behind this statement?\" and \"Can you provide evidence to support your claim?\" 2) Socratic Dialogue Prompting: We then engage the model in a Socratic dialogue by asking it to answer the generated questions and provide justifications for its responses. We alternate between question generation and answering until the model reaches a satisfactory level of understanding and reasoning. 3) Final Response Generation: Based on the Socratic dialogue, we ask the model to generate a final response that incorporates the insights and reasoning from the dialogue.",
            "Experiment Plan": "We plan to evaluate SP on a range of tasks that require deep understanding and reasoning, such as argument generation, debating, and persuasive writing. We will compare SP with state-of-the-art baselines, including chain-of-thought prompting and methods that incorporate external knowledge. We will also conduct human evaluation to assess the depth and quality of the reasoning in the generated responses, as well as the effectiveness of the Socratic questioning in eliciting critical thinking."
        },
        "Recursive Decomposition Prompting": {
            "Problem": "Large Language Models (LLMs) struggle with complex multi-hop reasoning tasks that require decomposing the problem into simpler sub-problems and recursively solving them. Existing methods like chain-of-thought prompting only perform single-level decomposition.",
            "Existing Methods": "Datasets like DROP and MathQA contain complex multi-hop questions. Baselines include standard prompting and chain-of-thought reasoning.",
            "Motivation": "Many real-world reasoning tasks require recursive problem decomposition. We can prompt LLMs to mimic this by generating a decomposition tree, solving sub-problems recursively, and combining partial solutions.",
            "Proposed Method": "We propose recursive decomposition prompting with three steps: 1) Decomposition: Prompt the LLM to decompose the complex problem into simpler sub-problems, forming a decomposition tree. 2) Recursive Solving: Recursively prompt the LLM to solve each sub-problem. If a sub-problem is still complex, apply decomposition prompting to it recursively. 3) Combination: Prompt the LLM to combine the partial solutions according to the decomposition tree to derive the final answer. Prompts are designed to generate the decomposition tree and sub-problem solutions in structured formats to facilitate parsing and combination.",
            "Experiment Plan": "Evaluate on multi-hop reasoning datasets like DROP and MathQA. Compare with baselines including standard prompting, chain-of-thought, and generated knowledge prompting. Metrics include accuracy and number of reasoning steps."
        },
        "Commonsense Grounding Prompting": {
            "Problem": "LLMs can generate fluent but factually incorrect statements by hallucinating nonexistent or invalid concepts, due to lack of grounding in commonsense knowledge during generation.",
            "Existing Methods": "Datasets like CommonGen and aNLI evaluate generative commonsense reasoning. Baselines include standard language modeling and knowledge-augmented generation.",
            "Motivation": "LLMs should be guided to ground each generated sentence in commonsense facts to avoid hallucination. We can utilize LLMs' inherent commonsense knowledge by prompting them to state relevant commonsense facts before each sentence generation step.",
            "Proposed Method": "We propose commonsense grounding prompting, where before generating each sentence, we prompt the LLM to state a relevant commonsense fact that grounds the sentence to be generated. The fact can be about everyday concepts, relations between concepts, or possible events. The LLM then generates the next sentence conditioned on both the previous context and the commonsense fact. This encourages the LLM to generate sentences that are consistent with commonsense. The commonsense facts can be parsed from the LLM outputs and used to construct supporting evidence to make the generation process more transparent.",
            "Experiment Plan": "Evaluate on generative commonsense reasoning datasets like CommonGen and aNLI. Compare with baselines like standard autoregressive generation and knowledge-augmented generation. Metrics include automatic scoring of commonsense consistency and human evaluation of factual correctness."
        },
        "Entailment Tree Prompting": {
            "Problem": "LLMs can generate false statements that are not logically entailed by the given context or not consistent with each other. Existing methods do not explicitly model the logical structure of the generated text.",
            "Existing Methods": "Datasets like EntailmentBank and CLUTRR evaluate logical entailment in generated text. Baselines include standard language modeling and inconsistency detection methods.",
            "Motivation": "We can prompt LLMs to generate texts that form a valid entailment tree, where each sentence is entailed by the conjunction of its parent sentences and the initial context. This ensures the logical consistency of the generated text.",
            "Proposed Method": "We propose entailment tree prompting, where starting from the initial context as the root, we prompt the LLM to generate child sentences that are entailed by the parent sentences. For each generated sentence, we prompt the LLM to score its entailment likelihood given the parent sentences. If the score is low, the sentence is discarded. We recursively prompt the LLM to generate entailed sentences until reaching a maximum depth or no more sentences can be generated. The final generated text is the conjunction of all sentences in the entailment tree. The entailment scores can be used to quantify the logical validity of the generated text.",
            "Experiment Plan": "Evaluate on logical entailment datasets like EntailmentBank and CLUTRR. Compare with baselines like standard language modeling and inconsistency detection methods. Metrics include entailment accuracy and human evaluation of logical coherence."
        },
        "Debate Prompting": {
            "Problem": "Current language models can fall prey to the phenomenon of 'group think' when asked to verify their own generated outputs, as the model may share the same biases or knowledge gaps across multiple personas that are based on the same model.",
            "Existing Methods": "Existing methods that use a single language model to self-verify its outputs (via techniques such as generating self-consistency scores) are prone to model biases and blindspots.",
            "Motivation": "Debate is a powerful technique used by humans to expose blindspots in their reasoning. By prompting multiple language models to engage in a debate and take different stances on a topic, we can use the disagreements that surface to detect potential inconsistencies and hallucinations in model-generated text.",
            "Proposed Method": "We propose a debate prompting method involving at least two language models. Given an initial prompt, Model A generates a candidate output. Model B is then prompted to find flaws and counterarguments in Model A's output. Following this, Model A is prompted to refute Model B's arguments, and the debate continues over multiple rounds until a stopping criterion is met. Finally, the arguments for and against the initial output are aggregated and presented to a human judge or a scoring model to determine the validity of the candidate output. The candidate is accepted only if it withstands scrutiny from the debate.",
            "Experiment Plan": "We propose evaluating the debate prompting technique on fact verification datasets such as FEVER and VitaminC, as well as on long-form question answering tasks. Baselines include standard prompting, self-consistency, and techniques from the RAWLSNET (Reinforced Adaptive Writing Loss) framework. Metrics include the accuracy of fact verification and the faithfulness of generated answers (measured via both automatic scoring and human evaluation)."
        },
        "World Representation Prompting": {
            "Problem": "Large language models can generate statements that are inconsistent with how the real world works, as they do not have an explicit representation of the world state during generation.",
            "Existing Methods": "Datasets like bAbI and ProPara evaluate world state tracking in generated text. Baselines include standard language modeling and knowledge-augmented generation.",
            "Motivation": "We can prompt LLMs to generate an explicit world state representation after each sentence generation step, and use it to guide future generation to avoid inconsistencies with the world state.",
            "Proposed Method": "We propose world representation prompting, where after generating each sentence, we prompt the LLM to generate a world state representation in a structured format (e.g., a set of logical predicates or a scene graph). The world state is then used as an additional input to the LLM when generating the next sentence, by constructing a prompt like 'Given the current world state [...], what happens next?'. This encourages the LLM to generate sentences that are consistent with the current world state. To handle multi-step reasoning, we can also prompt the LLM to update the world state representation based on each generated sentence. The generated world states can be used to visualize the reasoning process.",
            "Experiment Plan": "Evaluate on world state tracking datasets like bAbI and ProPara. Compare with baselines like standard language modeling and knowledge-augmented generation. Metrics include accuracy of the generated world states and consistency of the generated text."
        },
        "Semantic Scaffolding Prompting": {
            "Problem": "Large language models often generate outputs that are grammatically correct but lack semantic coherence or logical structure. This can lead to the generation of texts that are difficult to follow or that fail to effectively communicate the intended message.",
            "Existing Methods": "Existing methods for improving the semantic coherence of generated text, such as using discourse-aware models or post-processing the outputs to improve coherence, often require additional training or computational overhead.",
            "Motivation": "We propose that providing the model with a semantic scaffold - a high-level outline of the key ideas and their logical relationships - can help to guide the generation process and ensure that the output has a clear and coherent structure. By decomposing the writing task into smaller, more manageable steps and providing explicit guidance on the desired structure of the output, we can improve the semantic coherence and effectiveness of the generated text.",
            "Proposed Method": "We introduce Semantic Scaffolding Prompting (SSP), a prompting method that provides the model with a high-level semantic scaffold to guide the generation process. Given a writing prompt, we first prompt the model to generate a high-level outline of the key ideas and their logical relationships, using prompts such as \"What are the main points you want to make in this essay?\" and \"How do these points relate to each other?\". We then prompt the model to generate the full text, using the semantic scaffold as a guide. For each section of the scaffold, we provide focused prompts that encourage the model to elaborate on the key ideas and their relationships, while ensuring that the overall structure remains coherent.",
            "Experiment Plan": "We will evaluate SSP on a range of writing tasks, such as essay writing, article generation, and report writing. We will compare the semantic coherence and logical structure of outputs generated by SSP to those generated by standard prompting methods, as well as to human-written texts. We will use established metrics for evaluating text coherence, such as the entity grid model and the lexical chain model, as well as human evaluations of the clarity and effectiveness of the generated texts. We will also explore the use of different types of semantic scaffolds, such as outlines, concept maps, and argumentative schemas, to determine which are most effective for different types of writing tasks."
        },
        "Epistemic State Tracking Prompting": {
            "Problem": "Large language models can easily lose track of what information is factual vs. hypothetical or uncertain when generating long sequences, leading to subtle inconsistencies and hallucinations. This is especially challenging for tasks like story generation, open-ended QA, and dialogues.",
            "Existing Methods": "Most existing methods focus on grounding the generation with retrieved evidence. However, even with grounding, models can still hallucinate by confusing the epistemic status of information over a long context.",
            "Motivation": "We take inspiration from the cognitive science concept of epistemic state tracking, which posits that humans maintain a mental state of the epistemic status of information (e.g., factual, uncertain, hypothetical, false) and use it to guide reasoning and communication. For example, when telling a story, humans keep track of what events definitely happened vs. might have happened. Explicitly tracking epistemic state helps maintain consistency.",
            "Proposed Method": "We propose augmenting prompts with annotations of epistemic state, and having the model generate its own epistemic state annotations during the generation process. For example, the prompt could include tags like <fact>, <uncertain>, <hypothesis>, <counterfactual> to mark the epistemic state of information, which the model must generate when producing its own response. This forces the model to more explicitly consider the epistemic state of the information it's generating and use that to maintain consistency. To guide the model to track epistemic state, we include a few-shot chain-of-thought prompt that steps through the reasoning of determining what information to mark with each epistemic state.",
            "Experiment Plan": "We will evaluate Epistemic State Tracking Prompting on story generation (e.g. WritingPrompts, WorldFlaws), open-ended QA (e.g. NarrativeQA, OpenBookQA), and dialogue (e.g. SocialIQA, DialogueCOPA) tasks. Baselines will include standard prompting, Chain-of-Thought prompting, and knowledge-augmented generation. We will measure automatic metrics like perplexity, GED, and FactCC, and also do human evaluation studies of coherence, consistency, factual accuracy, and level of hallucination."
        },
        "Cognitive Dissonance Prompting": {
            "Problem": "Large language models can often generate confident but factually incorrect responses. Furthermore, when prompted to double check their response, models often fail to recognize their mistakes and instead invent rationalizations for the incorrect information.",
            "Existing Methods": "Existing methods for reducing hallucination include using external knowledge retrieval, prompting the model to cross-check its response, or training the model to be more calibrated. However, models still often struggle to recognize their own mistakes.",
            "Motivation": "Humans have a well-known cognitive bias called cognitive dissonance, where they experience discomfort when holding contradictory beliefs, and are motivated to resolve the contradiction, often by changing one of the beliefs. We can leverage this phenomenon to help models recognize their own mistakes, by prompting them to generate evidence that contradicts their original response, inducing a kind of artificial cognitive dissonance that the model is then motivated to resolve.",
            "Proposed Method": "Given an initial prompt and a model's response, we generate a \"cognitive dissonance\" prompt that asks the model to generate evidence contradicting its original response (e.g., \"Play devil's advocate and generate evidence that contradicts the previous response\"). We then prompt the model to resolve the dissonance by re-evaluating its original response in light of the contradictory evidence (e.g. \"In light of this contradictory evidence, do you still believe your original response? If not, what would you change your answer to?\"). This process can be repeated for multiple rounds of dissonance and resolution until the model arrives at a factually consistent response.",
            "Experiment Plan": "We will evaluate Cognitive Dissonance Prompting on closed-book QA datasets known to induce hallucination in models, such as TruthfulQA, FEVER, and SQuALITY. We will compare to baselines like standard prompting, Chain-of-Thought, and self-consistency. Metrics will include accuracy and FactCC for factual correctness, as well as human ratings of how well the model recognizes its own mistakes and changes its beliefs. We will also analyze how many rounds of dissonance and resolution are typically needed for models to converge to factual responses."
        },
        "Discourse Coherence Prompting": {
            "Problem": "Large language models can lose coherence when generating long sequences, leading to issues like self-contradiction, irrelevant tangents, or abrupt topic shifts. This is especially problematic for tasks that require generating coherent long-form text, like story generation, dialogues, or explanations.",
            "Existing Methods": "Most existing methods for improving coherence focus on better planning and content selection to guide generation, such as by generating intermediates sketches, keyword transition sequences, or learned sentence representations. However, these approaches do not directly optimize for the linguistic markers of discourse coherence.",
            "Motivation": "Linguistic theories of discourse coherence, such as Centering Theory and Rhetorical Structure Theory, describe how humans maintain coherence in text and dialogue through specific discourse patterns, such as chains of mention, coreference, discourse relations, and coherent topic progression. We can prompt models to generate these discourse coherence markers as part of the generation process, to more directly guide them towards coherent output.",
            "Proposed Method": "We develop discourse coherence prompts that include explicit markers for the key components of discourse coherence theories, such as entities and coreference chains, discourse relations, and topic flow. For example, a prompt could include tags like <entity1>, <entity2>, <coref>, <elaboration>, <contrast>, <topic_shift> that the model must generate to make the discourse structure explicit. We also include a few-shot prompt that demonstrates how to map the discourse coherence markers to natural text. At generation time, we first prompt the model to generate the discourse coherence structure of the response, then condition the response generation on that structure.",
            "Experiment Plan": "We will evaluate Discourse Coherence Prompting on tasks that require generating coherent long-form text, including story generation (e.g. WritingPrompts), long-form QA (e.g. ELI5, NarrativeQA), and dialogue (e.g. TopicalChat, PersonaChat). We will measure coherence with automatic metrics like DiscEval, GED, and sentence topic flow (STF), as well as human ratings of coherence, consistency, and single-topic flow. We will also qualitatively analyze the generated discourse structures to understand what patterns lead to more or less coherent output."
        },
        "Embodied Prompting": {
            "Problem": "Large language models struggle to reason about physical and spatial properties of objects and environments. This leads to non-sensical or inconsistent generations for tasks like visual storytelling, embodied QA, or robotic instructions that require an understanding of the physical world.",
            "Existing Methods": "Most prior work focuses on using visual grounding like images or videos to improve models' physical understanding. However, this requires aligned vision-language training data. Other approaches use knowledge graphs or simulations, but these have limited coverage or are hard to interface with language.",
            "Motivation": "Studies of child development show that humans learn about the physical world through interactive, embodied experience - moving around, manipulating objects, testing out physics, etc. This embodied learning builds rich intuitive knowledge that is critical for language understanding and generation. We propose prompting models to mimic embodied learning by imagining themselves as agents exploring and interacting with a physical environment.",
            "Proposed Method": "We develop a prompt chain that walks the model through an imagined embodied experience relevant to the task. For example, for visual storytelling, we may prompt the model to imagine itself as a character moving through the scene of the image, describing what it sees and does at each step. For a physical reasoning task, we may prompt the model to imagine performing a series of actions and physical tests to determine an object's properties. Critically, the prompt encourages the model to ground its imagination in physical laws, spatial relationships, and temporal dynamics. We also include a few-shot prompt demonstrating this kind of embodied imagination and physical grounding.",
            "Experiment Plan": "We will evaluate Embodied Prompting on a range of tasks that require physical and spatial reasoning, including visual storytelling (e.g. VIST), embodied QA (e.g. EQA), physical commonsense reasoning (e.g. PIQA), and robotic instruction following (e.g. R2R). We will compare to baselines like direct prompting, Chain-of-Thought prompting, and vision-language models. Evaluation will use both automatic metrics like perplexity, BLEU, and execution success rate, and human judgment of criteria like physical plausibility, temporal coherence, and consistency with task constraints. We will also collect human ratings of how well the generated imaginations capture relevant physical and spatial details."
        },
        "Counterfactual Reasoning Prompting": {
            "Problem": "Large language models often struggle to reason about counterfactual scenarios, leading to hallucinations and inconsistencies when generating responses that require considering alternative possibilities or outcomes.",
            "Existing Methods": "Current methods for counterfactual reasoning in LLMs include fine-tuning on counterfactual datasets or using prompts that encourage considering alternative scenarios. However, these approaches often lack a systematic way to guide the model's reasoning process.",
            "Motivation": "Counterfactual reasoning is a crucial aspect of human cognition, allowing us to consider alternative possibilities and make better decisions. By explicitly prompting LLMs to engage in counterfactual reasoning, we can improve their ability to generate more factual and consistent responses.",
            "Proposed Method": "We propose Counterfactual Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs through a structured counterfactual reasoning process. The steps include: 1) Identifying key factors in the given scenario; 2) Generating counterfactual scenarios by systematically varying these factors; 3) Reasoning about the consequences of each counterfactual scenario; 4) Comparing the outcomes of the counterfactual scenarios with the original scenario; 5) Generating a final response that incorporates insights from the counterfactual reasoning process.",
            "Experiment Plan": "Evaluate CRP on counterfactual reasoning benchmarks such as WIQA and COSMOS. Compare performance with baselines such as zero-shot prompting and fine-tuning on counterfactual datasets. Assess the factuality and consistency of generated responses using metrics like FactCC and human evaluation."
        },
        "Temporal Coherence Prompting": {
            "Problem": "LLMs often generate responses that lack temporal coherence, leading to inconsistencies and hallucinations when dealing with events or stories that unfold over time.",
            "Existing Methods": "Current approaches to improve temporal coherence in LLMs include fine-tuning on temporally annotated datasets or using prompts that encourage maintaining a consistent timeline. However, these methods often fail to capture the nuances of temporal relationships between events.",
            "Motivation": "Temporal coherence is essential for understanding and generating coherent narratives, as well as reasoning about cause-and-effect relationships. By explicitly prompting LLMs to consider the temporal aspects of a given scenario, we can improve their ability to generate more factual and consistent responses.",
            "Proposed Method": "We propose Temporal Coherence Prompting (TCP), a multi-step prompting approach that guides LLMs to maintain temporal consistency in their responses. The steps include: 1) Identifying key events in the given scenario; 2) Constructing a temporal graph that captures the relationships between these events; 3) Generating a response that adheres to the temporal constraints imposed by the graph; 4) Verifying the temporal coherence of the generated response by comparing it with the temporal graph; 5) Iteratively refining the response to resolve any temporal inconsistencies.",
            "Experiment Plan": "Evaluate TCP on datasets that require temporal reasoning, such as TimeTravel and MCTaco. Compare performance with baselines such as zero-shot prompting and fine-tuning on temporally annotated datasets. Assess the temporal coherence of generated responses using metrics like TimeML and human evaluation."
        },
        "Analogical Transfer Prompting": {
            "Problem": "LLMs often struggle to transfer knowledge from one domain to another, leading to hallucinations and inconsistencies when encountering novel situations or tasks.",
            "Existing Methods": "Current approaches to improve knowledge transfer in LLMs include fine-tuning on diverse datasets or using prompts that encourage drawing analogies between different domains. However, these methods often fail to capture the deep structural similarities between seemingly disparate domains.",
            "Motivation": "Analogical reasoning is a powerful tool for transferring knowledge from well-understood domains to novel situations. By explicitly prompting LLMs to draw analogies between different domains, we can improve their ability to generate more factual and consistent responses in unfamiliar contexts.",
            "Proposed Method": "We propose Analogical Transfer Prompting (ATP), a multi-step prompting approach that guides LLMs to transfer knowledge across domains using analogical reasoning. The steps include: 1) Identifying the target domain and a source domain that shares structural similarities; 2) Mapping the key elements and relationships between the source and target domains; 3) Generating a response in the target domain by adapting the solution from the source domain based on the analogical mapping; 4) Verifying the factuality and consistency of the generated response by comparing it with the target domain knowledge; 5) Iteratively refining the response to incorporate domain-specific nuances.",
            "Experiment Plan": "Evaluate ATP on cross-domain transfer learning benchmarks such as XNLI and XQA. Compare performance with baselines such as zero-shot prompting and fine-tuning on multi-domain datasets. Assess the factuality and consistency of generated responses using metrics like F1 score and human evaluation."
        },
        "Causal Reasoning Prompting": {
            "Problem": "LLMs often struggle to reason about causal relationships between events or entities, leading to hallucinations and inconsistencies when generating responses that require understanding cause-and-effect dynamics.",
            "Existing Methods": "Current approaches to improve causal reasoning in LLMs include fine-tuning on datasets with causal annotations or using prompts that encourage identifying causal relationships. However, these methods often fail to capture the complexity of real-world causal networks.",
            "Motivation": "Causal reasoning is a fundamental aspect of human cognition, enabling us to understand and predict the consequences of actions or events. By explicitly prompting LLMs to engage in causal reasoning, we can improve their ability to generate more factual and consistent responses in scenarios that involve complex causal relationships.",
            "Proposed Method": "We propose Causal Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs to reason about causal relationships in a given scenario. The steps include: 1) Identifying the key entities and events in the scenario; 2) Constructing a causal graph that captures the relationships between these entities and events; 3) Generating a response that takes into account the causal dependencies encoded in the graph; 4) Verifying the causal consistency of the generated response by comparing it with the causal graph; 5) Iteratively refining the response to resolve any causal inconsistencies.",
            "Experiment Plan": "Evaluate CRP on datasets that require causal reasoning, such as COPA and CausalQA. Compare performance with baselines such as zero-shot prompting and fine-tuning on causally annotated datasets. Assess the causal consistency of generated responses using metrics like precision and recall of causal relationships, as well as human evaluation."
        },
        "Adaptive Prompting": {
            "Problem": "Existing prompting methods often rely on fixed prompts that do not adapt to the specific characteristics or difficulty of the input, leading to suboptimal performance and increased risk of hallucinations.",
            "Existing Methods": "Current prompting approaches typically use a fixed set of prompts for all inputs, regardless of their complexity or domain. Some methods attempt to customize prompts based on input features, but they often require extensive manual engineering or domain-specific knowledge.",
            "Motivation": "Adapting prompts to the specific characteristics of each input can help LLMs generate more accurate and consistent responses. By dynamically selecting or generating prompts based on the input's complexity, domain, or other relevant features, we can guide LLMs to focus on the most important aspects of the problem and reduce the risk of hallucinations.",
            "Proposed Method": "We propose Adaptive Prompting (AP), a dynamic prompting approach that customizes prompts based on the characteristics of each input. The key steps include: 1) Extracting relevant features from the input, such as its complexity, domain, or topic; 2) Selecting or generating a suitable prompt based on these features, using a meta-learning model trained on a diverse set of prompts and their performance on various inputs; 3) Applying the selected prompt to guide the LLM in generating a response; 4) Evaluating the quality of the generated response using metrics that capture factuality, consistency, and relevance; 5) Updating the meta-learning model based on the performance of the selected prompt to improve future prompt selection.",
            "Experiment Plan": "Evaluate AP on diverse benchmarks that cover multiple domains and difficulty levels, such as MultiNLI, DecaNLP, and MMLU. Compare performance with baselines such as fixed prompting and manually engineered prompt selection methods. Assess the factuality, consistency, and relevance of generated responses using metrics like accuracy, F1 score, and human evaluation. Analyze the effectiveness of the meta-learning model in selecting suitable prompts for different types of inputs."
        },
        "Feedback-Driven Prompting": {
            "Problem": "Large language models often generate factually incorrect information, especially when the input query is complex or ambiguous.",
            "Existing Methods": "Current methods for improving factuality include using external knowledge bases, generating self-consistency checks, or using reinforcement learning with human feedback.",
            "Motivation": "Instead of relying on external resources or expensive human feedback, we can leverage the model's own ability to self-reflect and identify potential errors in its generated output. By prompting the model to provide feedback on its own response and then incorporating that feedback into a revised response, we can iteratively improve the factuality of the generated text.",
            "Proposed Method": "We propose a multi-step prompting approach called Feedback-Driven Prompting (FDP). Given an input query, FDP first prompts the model to generate an initial response. Then, it prompts the model to provide specific feedback on the initial response, focusing on identifying any factual errors, inconsistencies, or unsupported claims. Finally, FDP prompts the model to generate a revised response that incorporates the feedback and addresses the identified issues. This process can be repeated for multiple iterations until a satisfactory response is generated.",
            "Experiment Plan": "We will evaluate FDP on a range of factual question-answering datasets, such as Natural Questions, TriviaQA, and WebQuestions. We will compare FDP to baseline methods such as direct prompting and self-consistency prompting, as well as state-of-the-art methods that use external knowledge bases. We will measure factuality using both automatic metrics (e.g., ROUGE, BLEU) and human evaluation."
        },
        "Counterfactual Probing": {
            "Problem": "Large language models can generate plausible but incorrect information, especially when the input query contains counterfactual or hypothetical scenarios.",
            "Existing Methods": "Existing methods for improving factuality often struggle with counterfactual reasoning, as they rely on direct comparisons with factual knowledge bases or generate self-consistency checks that may not capture the nuances of counterfactual scenarios.",
            "Motivation": "We can improve the factuality of generated responses to counterfactual queries by explicitly probing the model's understanding of the counterfactual scenario and its implications. By prompting the model to reason about the differences between the counterfactual and factual scenarios, we can guide it towards generating more accurate and consistent responses.",
            "Proposed Method": "We propose Counterfactual Probing (CP), a prompting method designed to improve factuality in counterfactual scenarios. Given a counterfactual query, CP first prompts the model to identify the key differences between the counterfactual and factual scenarios. Then, it prompts the model to reason about the implications of those differences on the query at hand. Finally, CP prompts the model to generate a response that is consistent with the counterfactual scenario and its implications. CP can be applied iteratively to refine the response based on further probing of the model's understanding.",
            "Experiment Plan": "We will evaluate CP on a range of counterfactual reasoning datasets, such as TimeTravel, NLVR2, and CounterQA. We will compare CP to baseline methods such as direct prompting and self-consistency prompting, as well as state-of-the-art methods that use external knowledge bases. We will measure factuality using both automatic metrics (e.g., accuracy, consistency) and human evaluation of the generated responses' plausibility and coherence."
        },
        "Socratic Dialogue Prompting": {
            "Problem": "Large language models often generate outputs that lack depth and nuance, failing to consider multiple perspectives or engage in critical reasoning. This can lead to the generation of shallow or biased responses that do not fully address the complexity of the given prompt.",
            "Existing Methods": "Existing methods for improving the depth and quality of generated outputs, such as using adversarial training or reinforcement learning, often require significant computational resources and may not generalize well to new domains or tasks.",
            "Motivation": "We propose that engaging the model in a Socratic dialogue, where it is prompted to consider multiple perspectives, question its own assumptions, and engage in critical reasoning, can help to improve the depth and quality of the generated outputs. By encouraging the model to think more deeply and critically about the given prompt, we can generate more nuanced and well-reasoned responses.",
            "Proposed Method": "We introduce Socratic Dialogue Prompting (SDP), a prompting method that engages the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning. Given an initial prompt, we first ask the model to generate an initial response. We then prompt the model to consider alternative perspectives by asking questions such as \"What are some potential counterarguments to this view?\" or \"How might someone with a different background or experience view this issue?\". We then prompt the model to respond to these alternative perspectives, and to question its own assumptions by asking questions such as \"What are the underlying assumptions behind this argument?\" or \"What evidence supports or contradicts this view?\". This process is repeated iteratively, with the model generating increasingly nuanced and well-reasoned responses at each step.",
            "Experiment Plan": "We will evaluate SDP on a range of language generation tasks that require critical reasoning and nuanced understanding, such as argumentative essay writing, policy analysis, and philosophical dialogue. We will compare the depth, nuance, and logical coherence of outputs generated by SDP to those generated by standard prompting methods, as well as to human-generated outputs. We will also conduct a human evaluation to assess the perceived quality and persuasiveness of the generated outputs."
        },
        "Fact-Aware Prompting": {
            "Problem": "Large language models tend to generate factually incorrect information when answering questions or generating text. Existing methods often rely on fact-checking against knowledge bases or human feedback, which can be expensive and time-consuming.",
            "Existing Methods": "Current methods for reducing hallucination include using retrieved evidence to guide generation, training models to be more calibrated in their outputs, or using reinforcement learning with factual consistency rewards.",
            "Motivation": "Instead of relying on post-hoc fact-checking or additional training, we propose to make the model more aware of factual information during the generation process itself. By conditioning the model on relevant facts and training it to attend to factual evidence, we can guide it to generate more factually consistent outputs.",
            "Proposed Method": "We propose Fact-Aware Prompting (FAP), a method that incorporates factual information into the prompting process to guide the model's generation. Given an input query, FAP first retrieves relevant facts from a trusted knowledge source (e.g., Wikipedia, Wikidata). It then constructs a prompt that includes both the original query and the retrieved facts, using special tokens to distinguish the facts from the query. During generation, the model is trained to attend to the factual evidence provided in the prompt, using techniques such as attention masking or fact-aware decoding. The model is also trained to generate outputs that are consistent with the provided facts, using techniques such as factual consistency loss or fact-aware beam search.",
            "Experiment Plan": "We will evaluate FAP on a range of factual generation tasks, such as open-domain question answering, fact-grounded dialog, and fact-guided summarization. We will compare FAP to baseline methods such as direct prompting and retrieved-augmented generation, as well as state-of-the-art methods that use post-hoc fact-checking or reinforcement learning. We will measure factuality using both automatic metrics (e.g., factual F1, consistency score) and human evaluation of the generated outputs' correctness and consistency with the provided facts."
        },
        "Counterfactual Data Augmentation": {
            "Problem": "Large language models are prone to generating factually incorrect information, especially when the input query is ambiguous, complex, or outside the model's training distribution.",
            "Existing Methods": "Existing methods for reducing hallucination often rely on curated datasets or human-written prompts, which can be expensive to obtain and may not cover a wide range of factual errors.",
            "Motivation": "Instead of relying solely on human-curated data, we propose to automatically generate counterfactual examples that challenge the model's factual knowledge and reasoning abilities. By training the model on these counterfactual examples, we can improve its robustness to factual errors and enhance its ability to generate factually consistent outputs.",
            "Proposed Method": "We propose Counterfactual Data Augmentation (CDA), a method that automatically generates counterfactual examples to augment the model's training data. Given a factual statement or query, CDA applies a set of predefined perturbations to generate counterfactual versions of the input. These perturbations can include swapping entities, negating facts, or introducing logical inconsistencies. The model is then trained to distinguish between the original factual input and the generated counterfactual examples, using techniques such as contrastive learning or consistency regularization. During inference, the model is prompted to generate outputs that are consistent with the original factual input and inconsistent with the counterfactual examples.",
            "Experiment Plan": "We will evaluate CDA on a range of factual generation tasks, such as fact verification, fact-grounded dialog, and fact-guided question answering. We will compare CDA to baseline methods such as direct prompting and consistency training, as well as state-of-the-art methods that use human-curated datasets or adversarial training. We will measure factuality using both automatic metrics (e.g., accuracy, consistency score) and human evaluation of the generated outputs' correctness and consistency with the original factual input. We will also analyze the quality and diversity of the generated counterfactual examples, and their impact on the model's robustness to different types of factual errors."
        },
        "Knowledge Retrieval Prompting": {
            "Problem": "Large language models often generate hallucinated content that is not grounded in factual knowledge, leading to inaccurate and unreliable outputs.",
            "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation or fine-tuning the model on factual datasets. However, these approaches require additional training or external knowledge bases.",
            "Motivation": "We hypothesize that large language models have already encoded a vast amount of factual knowledge during pre-training, and that this knowledge can be leveraged to improve the factuality of generated outputs. By prompting the model to retrieve relevant facts from its own knowledge before generating an answer, we can encourage it to rely on factual information rather than hallucinating.",
            "Proposed Method": "We propose Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer. Given a question, we first prompt the model with \"Retrieve relevant facts from your knowledge to answer the following question: [question]\". The model generates a list of facts that it believes are relevant to the question. We then prompt the model with \"Based on the retrieved facts, answer the following question: [question]\nRetrieved facts: [generated facts]\", encouraging it to generate an answer that is grounded in the retrieved facts.",
            "Experiment Plan": "We will evaluate KRP on a range of factual question-answering datasets, such as Natural Questions and TriviaQA. We will compare the factuality and accuracy of answers generated by KRP to those generated by zero-shot prompting, few-shot prompting, and retrieval-augmented generation baselines. We will also conduct a human evaluation to assess the quality and factuality of the generated answers."
        },
        "Inquisitive Prompting": {
            "Problem": "Large language models often generate overconfident and assertive responses, even when they are uncertain or lack sufficient information to answer a question accurately. This can lead to the generation of hallucinated or incorrect information.",
            "Existing Methods": "Existing methods for reducing hallucination, such as using calibrated confidence scores or adversarial filtering, focus on post-processing the generated outputs to identify and remove potentially hallucinated content.",
            "Motivation": "We propose that encouraging the model to ask clarifying questions when it is uncertain can help to reduce hallucination and improve the accuracy of generated responses. By prompting the model to adopt an inquisitive stance and seek additional information when needed, we can mitigate the tendency to generate overconfident and assertive responses.",
            "Proposed Method": "We introduce Inquisitive Prompting (InqP), a prompting method that encourages the model to ask clarifying questions when it is uncertain about how to answer a given prompt. Given a question or prompt, we first ask the model to assess its confidence in being able to answer the question accurately. If the model's confidence is below a certain threshold, we prompt it to generate a clarifying question that would help it to answer the original question more accurately. We then provide the model with the answer to its clarifying question (either manually or by querying an external knowledge base), and prompt it to generate a final answer based on the original question and the additional information provided.",
            "Experiment Plan": "We will evaluate InqP on a range of question-answering and open-ended generation tasks, such as answering questions from the Natural Questions dataset and generating summaries of news articles. We will compare the factuality and accuracy of responses generated by InqP to those generated by standard prompting methods, as well as to human-generated responses. We will also conduct a human evaluation to assess the effectiveness of the clarifying questions generated by the model in eliciting additional information and improving the accuracy of the final responses."
        },
        "Context-Aware Verification Prompting": {
            "Problem": "Large language models often generate hallucinated content that is inconsistent with the input context or world knowledge, leading to factual inaccuracies in the generated output.",
            "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
            "Motivation": "We hypothesize that large language models have the ability to verify the factual consistency of their own generated content, if prompted to do so with the relevant context. By explicitly prompting the model to cross-reference its output with the input context and world knowledge, we can encourage the model to generate more factually consistent output.",
            "Proposed Method": "We propose Context-Aware Verification Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to verify the factual consistency of its generated output by cross-referencing with the input context and world knowledge, and to identify any inconsistencies or unsupported claims. Third, we prompt the model to revise its initial output to address the identified inconsistencies and generate a more factually consistent final output.",
            "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without verification, as well as state-of-the-art methods for reducing hallucination."
        },
        "Iterative Knowledge Distillation Prompting": {
            "Problem": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output.",
            "Existing Methods": "Existing methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
            "Motivation": "We hypothesize that large language models can iteratively distill factual knowledge from their own generated output, if prompted to do so with appropriate instructions. By explicitly prompting the model to extract factual knowledge from its generated output and conditioning future generation on the extracted knowledge, we can encourage the model to generate more factually consistent output over multiple iterations.",
            "Proposed Method": "We propose Iterative Knowledge Distillation Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to extract factual knowledge from its generated output, in the form of concise factual statements or knowledge triples. Third, we prompt the model to generate a revised output conditioned on both the original input context and the extracted factual knowledge. We repeat steps 2-3 for multiple iterations, with the model distilling factual knowledge from its own generated output and conditioning future generation on the accumulated knowledge.",
            "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without knowledge distillation, as well as state-of-the-art methods for reducing hallucination. We will also analyze the quality and accuracy of the distilled factual knowledge across multiple iterations."
        },
        "Uncertainty-Aware Prompting": {
            "Problem": "Large language models can generate overconfident or misleading outputs when prompted with ambiguous or uncertain inputs, leading to hallucination and factual inaccuracies.",
            "Existing Methods": "Existing methods for reducing hallucination in the presence of uncertainty include using calibrated confidence scores, incorporating uncertainty estimates into the decoding process, or using uncertainty-aware training objectives.",
            "Motivation": "We hypothesize that large language models can be prompted to express uncertainty and generate more calibrated outputs, if given appropriate instructions and uncertainty-aware prompts. By explicitly prompting the model to reason about its own uncertainty and express it in the generated output, we can encourage the model to generate more factually accurate and calibrated responses.",
            "Proposed Method": "We propose Uncertainty-Aware Prompting, a novel prompting approach to reduce hallucination and improve calibration in the presence of uncertain inputs. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to reason about its own uncertainty and express it in the form of calibrated confidence scores or natural language expressions of uncertainty (e.g., \"I'm not sure, but...\", \"This is just a guess, but...\"). Third, we prompt the model to revise its initial output to incorporate the expressed uncertainty and generate a more calibrated final output.",
            "Experiment Plan": "We will evaluate our proposed method on benchmarks that test for calibration and uncertainty awareness, such as the Uncertain Natural Language Inference task and the Calibration of Pre-trained Transformers benchmark. We will compare our method with baseline prompting approaches without uncertainty awareness, as well as state-of-the-art methods for improving calibration and reducing overconfidence. We will also analyze the quality and accuracy of the expressed uncertainty in the generated outputs."
        },
        "Adversarial Fact-Checking Prompting": {
            "Problem": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output.",
            "Existing Methods": "Existing methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
            "Motivation": "We hypothesize that large language models can be prompted to fact-check their own generated output, if framed as an adversarial game between a generator and a fact-checker. By explicitly prompting the model to generate both the initial output and a critique of its own output from an adversarial fact-checking perspective, we can encourage the model to generate more factually consistent output and catch its own mistakes.",
            "Proposed Method": "We propose Adversarial Fact-Checking Prompting, a novel prompting approach to reduce hallucination and improve factual accuracy. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to critique its own generated output from an adversarial fact-checking perspective, identifying any factual inaccuracies, inconsistencies, or unsupported claims. Third, we prompt the model to revise its initial output to address the critiques and generate a more factually consistent final output. We can repeat steps 2-3 for multiple rounds of adversarial fact-checking and revision.",
            "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without adversarial fact-checking, as well as state-of-the-art methods for reducing hallucination. We will also analyze the quality and accuracy of the generated fact-checking critiques across multiple rounds of interaction."
        },
        "Counterfactual Hallucination Prompting": {
            "Problem": "Large language models tend to hallucinate facts that are not grounded in reality, leading to generation of false information.",
            "Existing Methods": "Current methods for reducing hallucination include retrieval augmentation, knowledge grounding, and consistency modeling.",
            "Motivation": "Humans often imagine counterfactual scenarios to reason about the validity of facts. We can prompt LLMs to engage in similar counterfactual reasoning to assess the factuality of generated statements.",
            "Proposed Method": "We propose a three-step prompting procedure: 1) Given an input query, prompt the LLM to generate a response. 2) Prompt the LLM to generate a counterfactual version of the response that is similar but contains a false fact. 3) Prompt the LLM to compare the original and counterfactual responses, and explain why the counterfactual is false. This encourages the model to ground its reasoning in factual evidence. The final output is the original response, refined based on the insights from the counterfactual analysis.",
            "Experiment Plan": "Evaluate the proposed method on fact verification datasets like FEVER and VitaminC. Compare against baselines like direct prompting and retrieval augmentation. Metrics include accuracy, hallucination rate, and factual consistency."
        },
        "Socratic Probing Prompting": {
            "Problem": "Large language models can generate fluent text that lacks factual grounding, especially when generating long-form responses.",
            "Existing Methods": "Existing methods include incorporating retrieval and using learned fact-checking models.",
            "Motivation": "Socratic questioning is a technique used to probe assumptions, expose contradictions, and encourage deeper reasoning. We can use Socratic probing to guide LLMs towards more factual and coherent generation.",
            "Proposed Method": "The procedure involves: 1) Generating an initial response to a query. 2) Prompting the model with a series of targeted Socratic questions about the generated text, e.g., 'What is the source of this claim?', 'Is there any evidence that contradicts this?', 'Are there any implicit assumptions being made?'. 3) Using the model's answers to these probes to score the factuality of each sentence. 4) Prompting the model to refine the low-scoring sentences using retrieved evidence.",
            "Experiment Plan": "Evaluate on long-form QA datasets like ELI5 and Wikihow. Compare to baselines like vanilla prompting and retrieval-augmented generation. Metrics include factuality scores from a learned fact-checker, and human evaluation of factual consistency and coherence."
        },
        "Epistemic Uncertainty Prompting": {
            "Problem": "Language models can confidently generate false or nonsensical information, without any awareness of their uncertainty.",
            "Existing Methods": "Some methods use learned classifiers to filter model outputs based on factuality or uncertainty.",
            "Motivation": "Language models can express uncertainty using phrases like 'I'm not sure' or 'I don't know'. We can prompt LLMs to generate explicit uncertainty markers when they are unsure about a claim, and use this to guide generation.",
            "Proposed Method": "The prompting procedure is: 1) Augment the input prompt with instructions to express uncertainty about any claims the model is not fully confident about. 2) Generate a response to the augmented prompt. 3) Parse the generated text to identify any uncertainty markers. 4) For each uncertain span, prompt the model to either find supporting evidence, or regenerate the span. 5) Repeat steps 3-4 until there are no more uncertainty markers.",
            "Experiment Plan": "Evaluate on closed-book QA datasets like NaturalQuestions and TriviaQA. Compare to baselines like direct prompting and confidence-based filtering. Metrics include accuracy and precision/recall of uncertainty detection."
        },
        "Multimodal Grounding Prompting": {
            "Problem": "Language models can generate statements that are inconsistent with visual or multimodal evidence, e.g. image captioning models can hallucinate objects not actually present in an image.",
            "Existing Methods": "Some methods use visual retrieval or visual QA models to ground text generation in visual evidence.",
            "Motivation": "Language models can be prompted to reason about visual inputs, and to cross-reference textual claims against visual evidence. This can help ground generation and reduce hallucination.",
            "Proposed Method": "The procedure is: 1) Given a query and an image, prompt the model to generate a claim relevant to the image. 2) Prompt the model to assess whether the generated claim is consistent with the visual evidence, and to identify any inconsistencies. 3) If inconsistencies are found, prompt the model to revise the claim to be more consistent with the image. 4) Repeat steps 2-3 until the claim is fully consistent.",
            "Experiment Plan": "Evaluate on visual storytelling and visual QA datasets. Compare to baselines like direct prompting and retrieve-and-generate. Metrics include consistency scores between images and generated text, and human evaluation of factual alignment."
        },
        "Embodied Reasoning Prompting": {
            "Problem": "Language models can generate statements that are physically implausible or inconsistent with the constraints of the real world.",
            "Existing Methods": "Some methods incorporate physical knowledge bases or simulation engines to constrain generation.",
            "Motivation": "Language models have been shown to capture some physical and temporal commonsense knowledge. We can prompt them to reason step-by-step about the physical implications of their generations, grounding the process in embodied reasoning.",
            "Proposed Method": "The procedure is: 1) Given a prompt, generate an initial continuation. 2) Prompt the model to break down the generated continuation into a temporal sequence of physical events. 3) For each event, prompt the model to assess its physical plausibility, and revise if needed. 4) Prompt the model to re-compose the revised events into a coherent continuation. 5) Repeat steps 2-4 until the continuation is physically coherent.",
            "Experiment Plan": "Evaluate on physical reasoning datasets like PIQA and Physical IQA. Compare to baselines like direct prompting and KG-augmented generation. Metrics include physical plausibility scores and human evaluation of physical coherence."
        }
    }
}