{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "ideas": {
        "Dynamic Subgoal Prompting": {
            "Problem": "Large language models struggle with complex multi-step mathematical reasoning problems that require identifying and solving multiple subgoals in a dynamic manner.",
            "Existing Methods": "Chain-of-thought prompting has shown promise in guiding step-by-step reasoning, but it relies on a fixed set of reasoning steps and can struggle with problems requiring a variable number of steps.",
            "Motivation": "Human problem-solving often involves dynamically identifying subgoals and recursively applying problem-solving strategies to each subgoal. We hypothesize that prompting language models to mimic this dynamic subgoal decomposition process could enable them to tackle more complex problems.",
            "Proposed Method": "We propose Dynamic Subgoal Prompting, where the model is prompted to: 1) Analyze the problem to identify relevant subgoals that need to be solved. 2) For each subgoal, recursively apply the same subgoal identification and problem-solving process. 3) Integrate the solutions to the subgoals to arrive at the final solution. The prompt includes examples demonstrating this recursive subgoal decomposition and encourages the model to flexibly adapt its reasoning steps based on the problem complexity.",
            "Experiment Plan": "Evaluate Dynamic Subgoal Prompting on the MATH benchmark which contains multistep problems. Compare to chain-of-thought prompting and baseline prompting methods. Assess both final answer accuracy as well as the correctness and coherence of the reasoning steps."
        },
        "Commonsense Grounding Prompting": {
            "Problem": "Math word problems often require understanding commonsense relations between quantities and entities, but large language models may lack such understanding and generate nonsensical solutions.",
            "Existing Methods": "Current methods focus on generating solutions with in-context exemplars or chain-of-thought prompting, but do not explicitly incorporate commonsense knowledge.",
            "Motivation": "Humans draw upon commonsense knowledge to sanity-check the plausibility of solutions to math word problems. For example, the height of a person should be greater than that of a cat. Language models can be augmented with such commonsense comparisons to improve the quality of generated solutions.",
            "Proposed Method": "We propose Commonsense Grounding Prompting (CGP), where the model is prompted to: 1) Extract all entities and quantities from the problem. 2) Generate commonsense comparisons between pairs of quantities based on general knowledge (e.g., \"a person is typically taller than a cat\"). 3) Generate a solution to the problem and extract the computed quantities. 4) Verify the commonsense consistency of the computed quantities and refine if needed (e.g., rejecting a solution where a cat's height is greater than a person's). The prompts will instruct the model to follow these commonsense reasoning steps.",
            "Experiment Plan": "Evaluate CGP on the GSM-IC benchmark with irrelevant/inconsistent context and the ASDiv benchmark requiring commonsense. Compare to chain-of-thought prompting. Analyze the generated commonsense comparisons and consistency refinements."
        },
        "Multilingual Solution Prompting": {
            "Problem": "Most existing work on mathematical problem-solving with language models has focused on English, limiting the accessibility and generalizability of the techniques.",
            "Existing Methods": "Some multilingual language models exist but have not been extensively evaluated on mathematical reasoning tasks across languages.",
            "Motivation": "Mathematical problem-solving skills should be language-agnostic. We hypothesize that prompting models to generate solutions in multiple languages could improve the robustness and transferability of the reasoning skills.",
            "Proposed Method": "We propose Multilingual Solution Prompting, where the model is instructed to: 1) Generate the solution to a mathematical problem in a source language. 2) Translate the problem and solution into one or more target languages. 3) Cross-verify the consistency of the solutions across languages and revise if inconsistencies are found. The prompts include examples in multiple languages and encourage the model to use the multilingual reasoning process for error detection and correction.",
            "Experiment Plan": "Evaluate Multilingual Solution Prompting on mathematical problems in multiple languages, such as the Multilingual ArithmeticWord Problem Solving dataset. Compare to monolingual baselines and assess the cross-lingual transferability of the reasoning skills. Analyze the effectiveness of using multilingual consistency for error correction."
        },
        "Metacognitive Verification Prompting": {
            "Problem": "Large language models can generate plausible-looking but incorrect solutions to mathematical problems, lacking the metacognitive skills to verify and correct their own reasoning.",
            "Existing Methods": "Some methods use external tools or knowledge bases for answer verification, but they require additional resources and infrastructure.",
            "Motivation": "Humans engage in metacognitive processes to monitor and regulate their problem-solving, such as checking for logical consistency, verifying calculations, and backtracking when stuck. Prompting language models to perform similar self-verification could improve the accuracy and reliability of their solutions.",
            "Proposed Method": "We propose Metacognitive Verification Prompting, where the model is guided to: 1) Generate an initial solution to the problem. 2) Verify the logical consistency and mathematical validity of each step in the solution. 3) Identify any errors or uncertainties and backtrack to revise the reasoning accordingly. 4) Iterate steps 2-3 until a verified solution is reached. The prompts include examples of the metacognitive verification process and encourage the model to think critically about its own reasoning.",
            "Experiment Plan": "Evaluate Metacognitive Verification Prompting on challenging mathematical reasoning benchmarks known to cause models to generate plausible but incorrect solutions, such as the GSM8K dataset. Compare to baseline methods without explicit verification. Assess the accuracy of the final solutions and the effectiveness of the self-verification process in catching and correcting errors."
        },
        "Socratic Dialogue Prompting": {
            "Problem": "Current prompting methods for mathematical problem-solving are often one-directional, lacking the interactive feedback and guidance that humans provide during teaching and learning.",
            "Existing Methods": "Some methods use iterative refinement or feedback loops, but they do not engage in structured, pedagogically-informed dialogues.",
            "Motivation": "The Socratic method is a pedagogical approach that uses structured questioning to stimulate critical thinking, guide learners to recognize gaps in their reasoning, and collaboratively construct knowledge. We hypothesize that prompting language models to engage in Socratic-style dialogues could facilitate more effective learning and problem-solving.",
            "Proposed Method": "We propose Socratic Dialogue Prompting, where the model is instructed to engage in a multi-turn dialogue to: 1) Pose clarifying questions about the problem statement to identify relevant information and constraints. 2) Attempt an initial solution and expose its reasoning for external feedback. 3) Respond to Socratic questions that probe its understanding, challenge its assumptions, and guide it towards a more robust solution. 4) Revise its solution based on the insights gained from the dialogue. The prompts include examples of Socratic dialogues for mathematical problem-solving and encourage the model to actively seek feedback and refine its reasoning.",
            "Experiment Plan": "Evaluate Socratic Dialogue Prompting on mathematical reasoning datasets that provide detailed problem explanations, such as the AQuA dataset. Compare to baseline methods without dialogue interaction. Assess the accuracy and robustness of the final solutions, as well as the quality and coherence of the generated dialogues. Conduct human evaluation to gauge the pedagogical effectiveness of the Socratic dialogues in facilitating understanding and problem-solving."
        },
        "Iterative Refinement Prompting": {
            "Problem": "Large language models often struggle with complex multi-step mathematical problem solving, especially when the problem requires a long chain of reasoning steps. Existing methods like chain-of-thought prompting can help, but still fall short on more difficult problems.",
            "Existing Methods": "Benchmarks like MATH and GSM8K test mathematical reasoning abilities. Baselines include chain-of-thought prompting and self-consistency.",
            "Motivation": "Human mathematicians often solve problems through an iterative process, where an initial solution attempt is refined and improved over multiple passes. We hypothesize that prompting a language model to engage in a similar iterative refinement process, where it critiques and improves its own solution over multiple steps, can lead to more robust and accurate mathematical reasoning.",
            "Proposed Method": "We propose Iterative Refinement Prompting (IRP), where the model is prompted to generate an initial solution, then critique that solution and identify areas for improvement, then generate an updated solution based on its own feedback. This is repeated for multiple iterations. The prompt at each step encourages the model to focus on different aspects, like checking calculations, validating the overall logic, considering alternative approaches, etc. The final output is the solution from the last iteration.",
            "Experiment Plan": "We will evaluate IRP on the MATH and GSM8K benchmarks, comparing to chain-of-thought prompting and self-consistency baselines. The key metric is solution accuracy. We will also report the distribution of how many refinement iterations were needed. Qualitative analysis will examine how the solutions evolved over iterations on representative examples."
        },
        "Divide-and-Conquer Prompting": {
            "Problem": "Current language models can solve simple math problems, but struggle with problems that require complex problem decomposition and combining multiple reasoning steps. Flat chain-of-thought prompting works for simpler problems but cannot handle more complex problems.",
            "Existing Methods": "Datasets like MATH and MathQA test compositional mathematical reasoning, often with multiple problem-solving steps. Chain-of-thought prompting with self-consistency is a strong baseline.",
            "Motivation": "Many complex problems can be solved by recursively breaking them down into simpler subproblems, solving each subproblem, and composing the results. We draw inspiration from the divide-and-conquer paradigm in algorithm design, and hypothesize that we can elicit similar problem-solving behavior in language models through careful prompt design.",
            "Proposed Method": "We propose Divide-and-Conquer Prompting (DCP), where we prompt the model to decompose a complex problem into subproblems, then recursively apply the same decomposition to each subproblem until the subproblems are simple enough to be solved directly. The model is then prompted to compose the subproblem solutions into a final overall solution. The prompts encourage the model to explicitly state each decomposition and composition step in a hierarchical, recursive structure.",
            "Experiment Plan": "We will evaluate DCP on the MATH and MathQA benchmarks, comparing to chain-of-thought and self-consistency baselines. The key metric is problem-solving accuracy. We will also measure the complexity of the problem decompositions (average depth and branching factor) and correlate with accuracy. Qualitative analysis will examine the problem decomposition structure on representative examples."
        },
        "Proof-Guided Solution Prompting": {
            "Problem": "Language models can generate plausible-looking solutions to mathematical problems, but often make subtle logical mistakes and arrive at incorrect conclusions. Existing methods focus on the final answer but neglect the underlying proof and reasoning.",
            "Existing Methods": "Benchmarks like IsarStep test mathematical proof understanding and generation. Baselines include direct prompting to generate proofs.",
            "Motivation": "In mathematical problem-solving, a rigorous proof is just as important as the final answer. A correct proof provides strong evidence that the answer is correct and trustworthy. We hypothesize that prompting a language model to generate a complete mathematical proof in a recognized format, in addition to the final answer, will lead to more reliable and accurate problem-solving.",
            "Proposed Method": "We propose Proof-Guided Solution Prompting (PGSP), where we prompt the model to generate a formal mathematical proof for each problem-solving step, in addition to the final numerical answer. The proofs are prompted to follow a standard format like natural deduction or sequent calculus. Key intermediate results in the proof are referenced in the calculation steps. The final answer must be derived as the conclusion of the proof.",
            "Experiment Plan": "We will evaluate PGSP on the IsarStep benchmark, as well as proof-oriented versions of MATH and GSM8K where the model must output a complete proof in addition to the final answer. We will compare to direct prompting and chain-of-thought baselines. The key metrics are proof validity and completeness, as well as overall problem-solving accuracy. We will perform qualitative analysis of the generated proofs on representative problems."
        },
        "Unsupervised Data Generation Prompting": {
            "Problem": "Solving complex mathematical problems requires extensive domain knowledge and problem-solving strategies. While large language models have been trained on broad mathematical content, their knowledge is often incomplete and unreliable for niche topics and novel problem types.",
            "Existing Methods": "Prior work has used hand-crafted problem/solution prompts to elicit mathematical knowledge from language models. However, these methods are limited by the diversity and quality of the manually created data.",
            "Motivation": "We hypothesize that language models can be prompted to generate their own high-quality mathematical problems and solutions, which can then be used to improve their problem-solving abilities in a self-supervised manner. By generating a large volume of diverse problems and solutions, the model can learn more robust and generalizable problem-solving strategies.",
            "Proposed Method": "We propose Unsupervised Data Generation Prompting (UDGP), where we prompt the model to generate novel mathematical problems and corresponding step-by-step solutions. The prompts include instructions to generate problems with specific characteristics (e.g., multiple calculation steps, requiring a particular problem-solving technique, based on a certain mathematical concept). The model is prompted to ensure that each generated problem is well-posed and has a unique correct solution. The generated problem/solution pairs are then used as additional training data to improve the model's mathematical problem-solving abilities.",
            "Experiment Plan": "We will evaluate UDGP by using it to generate a large dataset of mathematical problems and solutions, then fine-tuning the language model on this generated data. We will measure the model's problem-solving accuracy on the MATH and GSM8K benchmarks before and after fine-tuning. We will compare to baselines that use manually created training data or no additional fine-tuning. We will also evaluate the quality and diversity of the generated problems and solutions using human ratings and automated metrics."
        },
        "Multilingual Cross-Verification Prompting": {
            "Problem": "Mathematical problem solving often involves complex chains of logic and calculation. Errors can easily propagate and lead to incorrect final answers. Existing methods for verifying solutions, like self-consistency, can help catch errors but have limited effectiveness.",
            "Existing Methods": "Self-consistency methods generate multiple solution chains and compare them for agreement. However, they are limited by the diversity of the solution chains and can miss subtle errors.",
            "Motivation": "Different languages and cultures often teach and express mathematical concepts in subtly different ways. We hypothesize that by prompting a multilingual language model to solve the same problem in multiple languages and comparing the solutions, we can catch errors and inconsistencies that may be missed when working in a single language.",
            "Proposed Method": "We propose Multilingual Cross-Verification Prompting (MCVP), where we prompt a multilingual language model to solve a mathematical problem step-by-step in multiple languages (e.g., English, Spanish, Chinese, Arabic). The prompts encourage the model to use language-specific problem-solving techniques and to show all work. The solutions in each language are then automatically translated to a common language and compared step-by-step. Any inconsistencies are flagged for further investigation. The model is then prompted to reconcile the inconsistencies and generate a final verified solution.",
            "Experiment Plan": "We will evaluate MCVP on multilingual versions of the MATH and GSM8K benchmarks, comparing to monolingual chain-of-thought and self-consistency baselines. The key metric is problem-solving accuracy. We will measure the number and type of inconsistencies detected between the multilingual solutions. We will also analyze the effectiveness of the reconciliation step in resolving inconsistencies and improving the final solution. Qualitative analysis will examine representative examples where MCVP catches errors that the baselines miss."
        },
        "Contextual Abstraction Prompting": {
            "Problem": "Large language models often struggle with mathematical problem solving when the problems involve complex real-world contexts and require identifying relevant information from irrelevant details.",
            "Existing Methods": "Existing benchmarks like GSM8K and MATH contain mathematical word problems with varying levels of contextual complexity. Baseline methods include chain-of-thought prompting and program-of-thoughts prompting.",
            "Motivation": "Humans often solve complex contextualized math problems by first abstracting away irrelevant details to form a simplified problem statement, and then solving the simplified problem. We hypothesize that prompting LLMs to mimic this two-stage abstraction and solving process can improve their mathematical problem solving capabilities.",
            "Proposed Method": "We propose Contextual Abstraction Prompting (CAP) which consists of two stages: 1) Abstraction: Given a contextualized math problem, prompt the LLM to generate a simplified version of the problem that preserves only the relevant mathematical quantities and relationships, discarding irrelevant contextual details. 2) Solving: Prompt the LLM to solve the abstracted problem using chain-of-thought reasoning or program synthesis. The final answer is then mapped back to the original context.",
            "Experiment Plan": "Evaluate CAP on contextualized math problem benchmarks like GSM8K and MATH. Compare with baselines like chain-of-thought prompting and program-of-thoughts prompting in terms of answer accuracy. Perform ablations to study the impact of the abstraction stage."
        },
        "Symbolic Scaffolding Prompting": {
            "Problem": "Large language models can generate the final answers to math problems, but often fail to generate complete and rigorous solution steps, especially for problems that require complex symbolic manipulation.",
            "Existing Methods": "Benchmarks like MathQA and MATH contain problems that require symbolic reasoning. Existing methods generate solution steps in natural language (e.g., chain-of-thought prompting) or programming languages (e.g., program-of-thoughts prompting), but lack explicit symbolic scaffolding.",
            "Motivation": "Symbolic mathematics involves manipulating mathematical expressions according to well-defined rules. Generating the complete symbolic solution steps requires know-how of both the high-level problem solving strategies and low-level rewrite rules. We hypothesize that explicitly prompting LLMs to generate symbolic steps with scaffolding from problem-solving strategies can lead to more complete and rigorous solution steps.",
            "Proposed Method": "We propose Symbolic Scaffolding Prompting (SSP) which first prompts LLMs to generate a high-level description of the solution approach (e.g., 'solve the equation for x by first isolating the variable on the LHS'). This is followed by prompting the LLM to fill in the actual symbolic steps guided by the high-level description. The generated steps are then executed by a symbolic math engine to get the final answer.",
            "Experiment Plan": "Evaluate SSP on math problem benchmarks that focus on symbolic manipulation, like MathQA and subsets of MATH. Compare with chain-of-thought and program-of-thoughts prompting in terms of the correctness and completeness of the generated solution steps. Perform qualitative analysis on how explicit scaffolding impacts the generated steps."
        },
        "Dimensional Analysis Prompting": {
            "Problem": "Large language models can solve mathematical and scientific problems when the numbers are provided, but often fail to work out the relations between physical quantities and derive new quantities from known ones.",
            "Existing Methods": "Benchmarks like ScienceQA and Fermi Questions contain problems that require reasoning about physical quantities and dimensions. Most existing methods do not explicitly perform dimensional analysis when solving such problems.",
            "Motivation": "Dimensional analysis is a powerful problem solving strategy in math and science that focuses on the units of quantities and uses their relations to guide equation formulation and solution. We hypothesize that prompting LLMs to perform dimensional analysis can expand their capabilities in scientific problem solving.",
            "Proposed Method": "We propose Dimensional Analysis Prompting (DAP) which first prompts LLMs to identify the relevant physical quantities in the problem and specify their units. It then prompts LLMs to reason about unit conversions and derive the dimensionally consistent equations that relate the quantities. Finally, it prompts LLMs to solve the equations to obtain the numerical answer.",
            "Experiment Plan": "Evaluate DAP on scientific problem solving benchmarks like ScienceQA and Fermi Questions. Compare with vanilla chain-of-thought prompting to study the impact of explicit dimensional analysis on solution quality. Perform case studies to understand the types of problems where dimensional analysis is most effective."
        },
        "Parallel Solution Path Prompting": {
            "Problem": "For multistep math problems, large language models often fixate on a single solution path which may turn out to be suboptimal or incorrect. Generating multiple diverse solution paths can potentially improve robustness and accuracy.",
            "Existing Methods": "Multistep math benchmarks like SVAMP and GSM8K are used to test stepwise problem solving capabilities of LLMs. Existing methods generate a single solution path via chain-of-thought prompting or program synthesis.",
            "Motivation": "Many multistep math problems admit multiple valid solution paths. Exploring diverse paths in parallel hedges against getting stuck on suboptimal paths and allows for cross-verification of the final answer. We hypothesize that prompting LLMs to generate multiple diverse solution paths and aggregating the results can boost accuracy and robustness.",
            "Proposed Method": "We propose Parallel Solution Path Prompting (PSPP) which prompts LLMs to generate multiple solution paths (e.g., 3-5 paths) for a given problem. Diversity among the paths is encouraged via high-level descriptions (e.g., 'let's solve this using a different approach'). The generated paths are executed in parallel to obtain multiple final answers. The most frequent answer is returned, or the paths are aggregated using majority voting or learned weighting schemes.",
            "Experiment Plan": "Evaluate PSPP on multistep math benchmarks like SVAMP and GSM8K. Compare with vanilla chain-of-thought prompting in terms of accuracy and robustness. Ablate the number of parallel paths and the path aggregation schemes. Analyze how path diversity correlates with answer correctness."
        },
        "Visual Hint Prompting": {
            "Problem": "Large language models struggle with mathematical and scientific problems that are most naturally solved with visual aids like diagrams, plots and tables. Solving such problems purely textually fails to leverage human insights from visual understanding.",
            "Existing Methods": "Benchmarks like GeoQA and PlotQA involve problems that are accompanied by visual information like diagrams and plots. Most existing methods rely on OCR to extract text from the visuals and then use the text to solve the problems, without deeply understanding the visual semantics.",
            "Motivation": "Many mathematical and scientific concepts are inherently visual (e.g., geometry, graphs and charts). Using visual representations makes the problems more intuitive to understand and solve for humans. We hypothesize that reasoning with visual hints can also improve the problem solving capabilities of language models.",
            "Proposed Method": "We propose Visual Hint Prompting (VHP) which associates key visual representations (diagrams, plots, tables) with math/science problems. For each problem, VHP first prompts LLMs to generate high-level descriptions of the insights that can be drawn from the associated visuals (e.g., 'the diagram shows that angle ABC is a right angle'). These visual hints are then appended to the problem text, and the LLM is prompted to generate the solution while taking into account the visual hints.",
            "Experiment Plan": "Evaluate VHP on visual math/science benchmarks like GeoQA and PlotQA. Compare with baselines that use plain text extracted from the visuals. Perform qualitative analysis on the generated visual hints and how they impact the solution quality. Collect expert annotations on the visual hints to enable supervised training and evaluation."
        },
        "Analogical Exemplar Prompting": {
            "Problem": "Large language models often struggle with solving complex math word problems that require multi-step reasoning and the application of relevant mathematical principles.",
            "Existing Methods": "Current methods for math problem solving include chain-of-thought prompting, which generates intermediate reasoning steps, and few-shot prompting with manually annotated exemplars.",
            "Motivation": "Humans often solve novel problems by drawing analogies to relevant example problems they have encountered before. We can leverage the knowledge already stored in large language models to automatically retrieve and adapt relevant exemplar problems to guide the solution process for a new problem.",
            "Proposed Method": "We propose Analogical Exemplar Prompting (AEP), which prompts the language model to: 1) Retrieve a set of relevant exemplar problems and solutions from its knowledge base that share similar mathematical principles or problem-solving steps with the given problem. 2) Adapt the exemplars to the specific context of the given problem by aligning variables, modifying values, and adjusting solution steps as needed. 3) Use the adapted exemplars to guide the step-by-step solution process for the original problem. The prompts will be designed to elicit each of these steps from the model.",
            "Experiment Plan": "Evaluate AEP on the MATH and MathQA benchmarks for math word problem solving. Compare to chain-of-thought prompting and few-shot prompting baselines. Conduct ablation studies on the retrieval and adaptation steps."
        },
        "Iterative Solution Refinement Prompting": {
            "Problem": "Large language models can generate complete solution steps for math problems in one shot, but the generated solutions often contain logical flaws or calculation errors that lead to incorrect final answers.",
            "Existing Methods": "Current methods focus on one-shot solution generation via chain-of-thought prompting or self-consistency decoding to marginalize over multiple solution paths.",
            "Motivation": "When humans solve complex problems, they often refine their solution through multiple iterations, by critically analyzing the solution steps to identify and correct mistakes. Language models can be prompted to similarly critique and iteratively refine their own generated solutions.",
            "Proposed Method": "We propose Iterative Solution Refinement Prompting (ISRP), which prompts the model to: 1) Generate an initial solution via chain-of-thought prompting. 2) Critically analyze each step of the solution to identify potential logical flaws or calculation errors. 3) Revise the erroneous parts of the solution while keeping the correct parts intact. 4) Repeat steps 2-3 for multiple iterations until no further errors are spotted or changes are made. The refined solution from the last iteration is returned as the final output.",
            "Experiment Plan": "Evaluate ISRP on GSM8K and SVAMP benchmarks. Compare to one-shot CoT prompting and self-consistency decoding. Vary the number of refinement iterations and analyze the refinement process."
        },
        "Hierarchical Reasoning Prompting": {
            "Problem": "Solving complex mathematical reasoning problems with multiple subproblems using a flat chain-of-thought often leads to convoluted and error-prone solution steps.",
            "Existing Methods": "Current approaches use a linear chain-of-thought prompting that generates a single sequence of reasoning steps to arrive at the final answer.",
            "Motivation": "Many complex problems can be naturally divided into a hierarchy of smaller subproblems. Humans often approach such problems by recursively breaking them down and solving the subproblems, and then composing the subsolutions to obtain the final solution. Large language models can be prompted to follow a similar hierarchical problem-solving strategy.",
            "Proposed Method": "We propose Hierarchical Reasoning Prompting (HRP), where the model is prompted to: 1) Analyze the problem to identify the main question and any prerequisite subproblems that need to be solved. 2) Recursively apply step 1 to break down each subproblem into smaller subproblems until each subproblem can be directly solved. 3) Generate a solution for each leaf subproblem. 4) Recursively compose the subsolutions to solve the higher-level subproblems and finally the main problem. The prompts will guide the model to follow this hierarchical reasoning process.",
            "Experiment Plan": "Evaluate HRP on the MultiArith and GSM-HARD benchmarks that involve compositional subproblems. Compare to flat chain-of-thought prompting and least-to-most prompting. Analyze the generated reasoning hierarchies."
        },
        "Parameterized Solution Prompting": {
            "Problem": "Large language models can adapt a solution strategy to problems with different values of the same parameters, but often fail to generalize to problems with additional parameters not seen in the prompt.",
            "Existing Methods": "Current methods use few-shot prompts with a fixed set of problems as exemplars, which have limited variation in problem parameters.",
            "Motivation": "To robustly solve a class of math problems with varying parameters, models need to learn a parameterized solution strategy that explicitly takes into account all relevant parameters and their dependencies. We can use prompting to demonstrate such a strategy and guide the model to internalize it.",
            "Proposed Method": "We propose Parameterized Solution Prompting (PSP), which prompts the model with a few-shot set of problems that vary all relevant parameters. For each problem, the prompt includes: 1) A specification of all parameters and their values. 2) A complete solution that explicitly states the dependence on each parameter. 3) A question that changes the value of one or more parameters. 4) A modified solution that updates the parameter-dependent steps based on the new values. After the few-shot examples, the original test problem is prompted as a question that specifies the parameters. The model is expected to generate a solution using the parameterized strategy.",
            "Experiment Plan": "Evaluate PSP on the MATH benchmark including the interpolation and extrapolation sets with varied parameters. Compare to standard few-shot CoT prompting. Analyze the parameter-dependence in generated solutions and robustness to parameter variations."
        },
        "Adversarial Reasoning Prompting": {
            "Problem": "Large language models struggle with mathematical reasoning tasks that involve complex problem-solving strategies and are prone to errors.",
            "Existing Methods": "Current approaches include chain-of-thought prompting, program-aided language models, and self-consistency techniques.",
            "Motivation": "Adversarial training has been successful in improving the robustness and generalization of machine learning models. We hypothesize that introducing an adversarial component during the reasoning process can help large language models identify and correct their mistakes, leading to improved problem-solving performance.",
            "Proposed Method": "We propose Adversarial Reasoning Prompting (ARP), a novel prompting method that incorporates an adversarial critic to challenge the model's reasoning steps. The method consists of three main stages: 1) Problem Solving: The model generates a solution to the mathematical problem using a standard prompting technique. 2) Adversarial Critique: An adversarial prompt is used to challenge the generated solution by identifying potential flaws, inconsistencies, or alternative approaches. 3) Refinement: The model incorporates the adversarial feedback to refine its solution, addressing the identified issues and generating an improved answer. This process can be repeated iteratively to further enhance the model's reasoning capabilities.",
            "Experiment Plan": "We will evaluate ARP on challenging mathematical reasoning benchmarks such as MATH, GSM8K, and AQuA. The proposed method will be compared against state-of-the-art baselines, including chain-of-thought prompting and program-aided language models. We will measure performance using accuracy and conduct a qualitative analysis of the generated solutions to assess the effectiveness of the adversarial reasoning process."
        },
        "Meta-Cognitive Prompting": {
            "Problem": "Large language models often struggle to solve complex mathematical problems that require multi-step reasoning and self-monitoring.",
            "Existing Methods": "Current approaches include chain-of-thought prompting, self-consistency, and program-aided language models.",
            "Motivation": "Metacognition, the ability to monitor and regulate one's own cognitive processes, is crucial for effective problem-solving in humans. By incorporating metacognitive prompts, we aim to enable large language models to better monitor and control their reasoning process, leading to improved performance on mathematical reasoning tasks.",
            "Proposed Method": "We introduce Meta-Cognitive Prompting (MCP), a novel prompting method that guides the model's reasoning process using metacognitive cues. The method consists of three main components: 1) Problem Understanding: The model is prompted to summarize the problem, identify key information, and outline a high-level solution strategy. 2) Step-wise Reasoning: The model generates a series of reasoning steps, with each step accompanied by a metacognitive prompt that encourages the model to reflect on its progress, assess the validity of its reasoning, and identify potential errors or alternative approaches. 3) Solution Verification: After generating the final answer, the model is prompted to review its solution, check for consistency, and provide a confidence score. If the confidence score is low, the model is prompted to revisit and refine its reasoning.",
            "Experiment Plan": "We will evaluate MCP on a range of mathematical reasoning benchmarks, including MATH, GSM8K, and AQuA. The proposed method will be compared against state-of-the-art baselines, such as chain-of-thought prompting and program-aided language models. We will measure performance using accuracy and conduct a qualitative analysis of the generated solutions to assess the effectiveness of the metacognitive prompts in guiding the model's reasoning process."
        },
        "Embodied Reasoning Prompting": {
            "Problem": "Large language models often struggle with mathematical reasoning tasks that require spatial reasoning, such as geometry and graph problems.",
            "Existing Methods": "Current approaches primarily focus on symbolic reasoning and do not explicitly incorporate spatial or embodied representations.",
            "Motivation": "Embodied cognition suggests that human reasoning is grounded in perceptual and motor experiences. By incorporating embodied representations and reasoning into large language models, we aim to improve their performance on mathematical tasks that involve spatial and graphical reasoning.",
            "Proposed Method": "We propose Embodied Reasoning Prompting (ERP), a novel prompting method that incorporates visual and spatial representations to support mathematical reasoning. The method consists of three main components: 1) Visual Encoding: The model is prompted to generate a visual representation of the problem, such as a diagram or graph, based on the problem description. 2) Embodied Reasoning: The model is guided to reason about the problem using embodied cues, such as imagining manipulating the visual representation, following paths, or transforming shapes. 3) Symbolic Mapping: The model maps the embodied reasoning steps back to symbolic mathematical expressions to generate the final answer. The prompts are designed to encourage the model to seamlessly transition between visual, embodied, and symbolic reasoning.",
            "Experiment Plan": "We will evaluate ERP on mathematical reasoning tasks that involve spatial and graphical reasoning, such as geometry problems, graph theory, and visual puzzle solving. The proposed method will be compared against state-of-the-art baselines, including chain-of-thought prompting and program-aided language models. We will measure performance using accuracy and conduct a qualitative analysis of the generated visual representations and embodied reasoning steps to assess the effectiveness of the embodied reasoning prompts."
        },
        "Collaborative Reasoning Prompting": {
            "Problem": "Large language models often produce inconsistent or incorrect solutions when solving complex mathematical problems that require multiple reasoning steps.",
            "Existing Methods": "Current approaches, such as chain-of-thought prompting and self-consistency, rely on a single model to generate the entire reasoning process.",
            "Motivation": "Collaborative problem-solving has been shown to be effective in human learning and problem-solving. By leveraging the diverse knowledge and reasoning capabilities of multiple large language models, we aim to improve the accuracy and robustness of mathematical problem-solving.",
            "Proposed Method": "We introduce Collaborative Reasoning Prompting (CRP), a novel prompting method that engages multiple large language models in a collaborative problem-solving process. The method consists of three main stages: 1) Problem Decomposition: The first model is prompted to break down the complex problem into smaller, more manageable sub-problems. 2) Parallel Reasoning: Each sub-problem is assigned to a different language model, which generates a solution using its own reasoning process. 3) Solution Integration: The final model is prompted to integrate the sub-problem solutions, resolve any inconsistencies, and generate the final answer. The prompts are designed to encourage the models to communicate their reasoning steps, identify and resolve conflicts, and reach a consensus on the final solution.",
            "Experiment Plan": "We will evaluate CRP on challenging mathematical reasoning benchmarks, such as MATH, GSM8K, and AQuA. The proposed method will be compared against state-of-the-art baselines, including chain-of-thought prompting and self-consistency techniques. We will measure performance using accuracy and conduct a qualitative analysis of the collaborative reasoning process to assess the effectiveness of the multi-model approach in improving problem-solving performance."
        },
        "Counterfactual Reasoning Prompting": {
            "Problem": "Large language models often struggle to generalize their mathematical reasoning skills to novel problem types and fail to consider alternative solution paths.",
            "Existing Methods": "Current approaches focus on generating a single solution path and do not explicitly explore alternative scenarios or counterfactual reasoning.",
            "Motivation": "Counterfactual reasoning, the ability to consider alternative scenarios and their consequences, is a crucial component of human problem-solving. By incorporating counterfactual reasoning into the prompting process, we aim to improve the generalization and robustness of large language models in mathematical reasoning tasks.",
            "Proposed Method": "We propose Counterfactual Reasoning Prompting (CoRP), a novel prompting method that encourages large language models to explore alternative solution paths and consider counterfactual scenarios. The method consists of three main stages: 1) Initial Solution: The model generates an initial solution to the mathematical problem using a standard prompting technique. 2) Counterfactual Exploration: The model is prompted to generate counterfactual scenarios by modifying the problem conditions, considering alternative assumptions, or exploring different solution strategies. 3) Solution Comparison: The model compares the initial solution with the counterfactual solutions, identifies the most robust and generalizable approach, and generates the final answer. The prompts are designed to encourage the model to think creatively, consider multiple perspectives, and reason about the consequences of different solution paths.",
            "Experiment Plan": "We will evaluate CoRP on mathematical reasoning benchmarks that require generalization to novel problem types, such as MathQA and SVAMP. The proposed method will be compared against state-of-the-art baselines, including chain-of-thought prompting and program-aided language models. We will measure performance using accuracy and conduct a qualitative analysis of the counterfactual scenarios and solution comparisons to assess the effectiveness of the counterfactual reasoning prompts in improving generalization and robustness."
        },
        "Contrastive Solution Prompting": {
            "Problem": "Large language models often struggle to generate precise and accurate solutions to complex mathematical problems, even when prompted with relevant examples or reasoning steps.",
            "Existing Methods": "Current approaches include chain-of-thought prompting, which provides step-by-step reasoning examples, and self-consistency, which samples multiple reasoning paths and selects the most consistent answer.",
            "Motivation": "Humans often learn by contrasting correct and incorrect solutions to a problem. By analyzing the differences between them, they can identify key principles and avoid common pitfalls. We hypothesize that prompting LLMs to generate both correct and incorrect solutions, and then analyze their differences, can improve their mathematical problem-solving abilities.",
            "Proposed Method": "We propose Contrastive Solution Prompting (CSP), a novel prompting method that guides LLMs to generate paired correct and incorrect solutions for each problem. The prompt includes instructions like: '1) Generate a correct solution to the problem, showing your reasoning step-by-step. 2) Generate an incorrect solution to the problem, also showing your reasoning step-by-step. 3) Analyze the key differences between the correct and incorrect solutions, highlighting where the incorrect solution went wrong.' By generating and contrasting solution pairs, the LLM can learn to identify and avoid common mistakes.",
            "Experiment Plan": "We will evaluate CSP on standard mathematical reasoning benchmarks like GSM8K and MATH. We will compare its performance to baselines such as chain-of-thought prompting and self-consistency. We will also conduct ablation studies to assess the importance of generating both correct and incorrect solutions, as well as the impact of the contrastive analysis step."
        },
        "Adaptive Granularity Prompting": {
            "Problem": "Current prompting methods for mathematical problem solving often use a fixed level of granularity for the reasoning steps, which may be suboptimal for problems of varying complexity.",
            "Existing Methods": "Chain-of-thought prompting and its variants typically use a fixed template for eliciting reasoning steps, such as 'Let's solve this step-by-step'.",
            "Motivation": "Different mathematical problems require different levels of detail in their solution steps. Simple problems may be solved with high-level steps, while complex problems may need fine-grained, low-level steps. Dynamically adapting the granularity of the reasoning steps to the problem complexity could lead to more efficient and accurate solutions.",
            "Proposed Method": "We propose Adaptive Granularity Prompting (AGP), a method that dynamically adjusts the level of detail in the reasoning steps based on the problem complexity. The prompt includes instructions like: '1) Assess the complexity of the problem on a scale from 1 (very simple) to 5 (very complex). 2) If the complexity is 1-2, generate high-level reasoning steps. If the complexity is 3-5, generate detailed, low-level reasoning steps. 3) Solve the problem using the appropriate level of granularity in your reasoning steps.' By adaptively selecting the granularity of the reasoning steps, AGP can generate more efficient and accurate solutions.",
            "Experiment Plan": "We will evaluate AGP on mathematical reasoning benchmarks like GSM8K and MATH, comparing its performance to fixed-granularity baselines like chain-of-thought prompting. We will also analyze the relationship between problem complexity and the granularity of the generated reasoning steps, and conduct user studies to assess the clarity and helpfulness of the adaptive explanations."
        },
        "Exemplar-Guided Prompting": {
            "Problem": "Large language models can struggle to generate accurate solutions to mathematical problems that are significantly different from the examples they were trained on or prompted with.",
            "Existing Methods": "Few-shot prompting methods, such as chain-of-thought prompting, provide a small set of examples to guide the model's reasoning process. However, these examples are often manually selected and may not be optimally relevant to the target problem.",
            "Motivation": "Retrieving and prompting with the most relevant examples from a large corpus could help guide the model to generate more accurate solutions, especially for problems that are dissimilar to the training data. By dynamically selecting examples based on their similarity to the target problem, we can provide more tailored and effective guidance.",
            "Proposed Method": "We propose Exemplar-Guided Prompting (EGP), a method that retrieves the most relevant examples from a large corpus and uses them to guide the model's reasoning process. The prompt includes instructions like: '1) Retrieve the top-k examples from the corpus that are most similar to the target problem, based on a semantic similarity metric. 2) Analyze the key features and solution steps of the retrieved examples. 3) Adapt the solution steps from the examples to solve the target problem.' By dynamically selecting and leveraging relevant examples, EGP can guide the model to generate more accurate solutions, even for problems that are dissimilar to the training data.",
            "Experiment Plan": "We will evaluate EGP on mathematical reasoning benchmarks like GSM8K and MATH, comparing its performance to few-shot prompting baselines that use manually selected examples. We will also experiment with different semantic similarity metrics and values of k for example retrieval. Additionally, we will analyze the relationship between the similarity of the retrieved examples and the accuracy of the generated solutions."
        },
        "Iterative Error Correction Prompting": {
            "Problem": "Large language models can make errors in their mathematical problem-solving process, especially for complex, multi-step problems. These errors can compound and lead to incorrect final answers.",
            "Existing Methods": "Existing methods like chain-of-thought prompting focus on generating a single, coherent solution path. However, they do not have explicit mechanisms for detecting and correcting errors in the reasoning process.",
            "Motivation": "Humans often solve complex problems by iteratively refining their solution, checking for errors, and making corrections along the way. By prompting LLMs to engage in a similar iterative error correction process, we can improve the accuracy and robustness of their mathematical problem-solving abilities.",
            "Proposed Method": "We propose Iterative Error Correction Prompting (IECP), a method that guides LLMs to iteratively refine their solution by detecting and correcting errors. The prompt includes instructions like: '1) Generate an initial solution to the problem, showing your reasoning step-by-step. 2) Check each step of your solution for errors, such as mathematical mistakes, logical inconsistencies, or violated constraints. 3) If any errors are found, correct them and update the solution accordingly. 4) Repeat steps 2-3 until no more errors are found.' By iteratively refining the solution and correcting errors, IECP can improve the accuracy and robustness of the LLM's mathematical problem-solving performance.",
            "Experiment Plan": "We will evaluate IECP on challenging mathematical reasoning benchmarks like GSM8K and MATH, comparing its performance to baselines like chain-of-thought prompting. We will also analyze the types of errors detected and corrected by IECP, and measure how many iterations are typically needed to arrive at an error-free solution. Additionally, we will conduct ablation studies to assess the importance of the error detection and correction steps."
        },
        "Uncertainty-Aware Prompting": {
            "Problem": "Large language models can often generate overly confident solutions to mathematical problems, even when they are uncertain or lack sufficient information. This can lead to incorrect answers and a lack of transparency about the model's limitations.",
            "Existing Methods": "Existing prompting methods typically focus on generating a single, deterministic solution, without explicitly representing the model's uncertainty or confidence in its reasoning steps.",
            "Motivation": "Humans often reason about complex problems by considering multiple possibilities, assessing their confidence in each step, and acknowledging when they are uncertain or need more information. By prompting LLMs to engage in similar uncertainty-aware reasoning, we can improve the transparency and reliability of their mathematical problem-solving abilities.",
            "Proposed Method": "We propose Uncertainty-Aware Prompting (UAP), a method that guides LLMs to reason about mathematical problems while explicitly representing and communicating their uncertainty. The prompt includes instructions like: '1) For each step of your reasoning process, generate multiple candidate solutions and assess your confidence in each one. 2) If your confidence in a step is below a threshold, indicate that you are uncertain and may need additional information. 3) Propagate uncertainty throughout the reasoning process, and provide a final answer along with a confidence score. 4) If the overall confidence is low, suggest what additional information might be needed to solve the problem more reliably.' By explicitly representing and communicating uncertainty, UAP can improve the transparency and reliability of the LLM's mathematical problem-solving performance.",
            "Experiment Plan": "We will evaluate UAP on mathematical reasoning benchmarks like GSM8K and MATH, comparing its performance and calibration to deterministic prompting baselines. We will measure metrics like accuracy, confidence calibration, and the frequency of 'uncertain' responses. We will also conduct user studies to assess the perceived transparency and trustworthiness of UAP's uncertainty-aware reasoning. Additionally, we will analyze the relationship between the model's confidence scores and the accuracy of its generated solutions."
        },
        "Semantic Invariance Prompting": {
            "Problem": "Large language models often struggle with solving math word problems that are semantically equivalent but differ in surface-level descriptions, indicating a lack of deeper understanding of the underlying mathematical concepts.",
            "Existing Methods": "Current methods like Chain-of-Thought prompting improve performance on math word problems, but still fall short in handling semantic variations of the same problem.",
            "Motivation": "Humans can easily recognize when two math problems are essentially the same despite differences in wording or context. We hypothesize that prompting LLMs to explicitly identify semantic equivalence between problems can help them generalize better and solve problems more reliably.",
            "Proposed Method": "We propose Semantic Invariance Prompting (SIP), which consists of three steps: 1) Given a math word problem, prompt the LLM to generate multiple semantic variations of the problem by changing the context, wording, or numerical values while preserving the underlying mathematical structure. 2) Prompt the LLM to identify the semantic equivalence between the original problem and its variations, and explain why they are mathematically the same. 3) Prompt the LLM to solve the original problem, using the understanding gained from recognizing the semantic invariance across variations.",
            "Experiment Plan": "Evaluate SIP on existing math word problem benchmarks like MATH and GSM8K, and compare with baselines like Chain-of-Thought prompting. Also create new test sets with semantic variations of problems to specifically measure the effectiveness of SIP in improving generalization."
        },
        "Solution Concept Prompting": {
            "Problem": "Large language models can solve many math problems step-by-step, but often fail to provide high-level explanations of the key solution concepts involved, which is crucial for demonstrating a deeper understanding of the problem.",
            "Existing Methods": "Methods like Chain-of-Thought prompting generate step-by-step solutions, but do not explicitly elicit higher-level conceptual explanations.",
            "Motivation": "When humans solve complex math problems, they often rely on identifying the key solution concepts needed, such as applying a specific theorem, formula or technique. Prompting LLMs to first explain the high-level solution concepts before diving into step-by-step details could lead to more reliable and interpretable problem-solving.",
            "Proposed Method": "We introduce Solution Concept Prompting (SCP), a two-stage prompting approach: 1) Given a math problem, prompt the LLM to identify and explain the key mathematical concepts, theorems, or techniques needed to solve the problem, without providing the actual solution. 2) Prompt the LLM to provide a step-by-step solution to the problem, guided by the high-level solution concepts generated in the first stage.",
            "Experiment Plan": "Evaluate SCP on math problem benchmarks that require complex reasoning, such as MATH and MathQA. Compare with baselines like Chain-of-Thought prompting. Conduct human evaluations to assess the quality and interpretability of the generated solution concepts."
        },
        "Mathematical Consistency Prompting": {
            "Problem": "Large language models can sometimes generate mathematically inconsistent solutions to problems, such as using contradictory equations or violating basic mathematical rules.",
            "Existing Methods": "Existing methods focus on generating step-by-step solutions, but do not explicitly check for mathematical consistency across the steps.",
            "Motivation": "Maintaining mathematical consistency is crucial for solving problems correctly. We can leverage the language understanding capabilities of LLMs to prompt them to check the consistency of their own generated solution steps and catch potential errors or contradictions.",
            "Proposed Method": "We propose Mathematical Consistency Prompting (MCP), which works as follows: 1) Given a math problem, prompt the LLM to generate a step-by-step solution. 2) Prompt the LLM to check each step of the generated solution for mathematical consistency, looking for any contradictions or violations of basic mathematical rules. 3) If any inconsistencies are found, prompt the LLM to revise the solution and re-check for consistency until a fully consistent solution is generated.",
            "Experiment Plan": "Evaluate MCP on math problem benchmarks like MATH and GSM8K, and compare with baselines like Chain-of-Thought prompting. Measure both the overall accuracy and the mathematical consistency of the generated solutions. Also create test sets with problems designed to trap models into making mathematically inconsistent errors."
        },
        "Intuition-First Prompting": {
            "Problem": "Large language models can generate step-by-step solutions to math problems, but often lack the ability to provide intuitive explanations of the high-level approach before diving into the details.",
            "Existing Methods": "Existing methods like Chain-of-Thought prompting focus on generating detailed solution steps, but do not explicitly prompt for intuitive explanations of the overall approach.",
            "Motivation": "When humans approach complex math problems, they often start by developing a high-level intuition of how to solve the problem before working out the details. Prompting LLMs to first provide an intuitive explanation of their approach could lead to more strategic and reliable problem-solving.",
            "Proposed Method": "We introduce Intuition-First Prompting (IFP), a two-stage prompting method: 1) Given a math problem, prompt the LLM to generate an intuitive, high-level explanation of how to approach solving the problem, without going into step-by-step details. 2) Prompt the LLM to generate a detailed step-by-step solution, guided by the intuitive approach explained in the first stage.",
            "Experiment Plan": "Evaluate IFP on challenging math problem benchmarks like MATH and MathQA, and compare with baselines like Chain-of-Thought prompting. Conduct human evaluations to assess the quality and usefulness of the generated intuitive explanations in guiding the problem-solving process."
        },
        "Assumption Elicitation Prompting": {
            "Problem": "Math word problems often contain implicit assumptions that need to be identified and made explicit to solve the problem correctly. Large language models sometimes fail to recognize these assumptions, leading to incorrect solutions.",
            "Existing Methods": "Existing methods do not explicitly prompt LLMs to identify and state the assumptions needed to solve a given math word problem.",
            "Motivation": "Recognizing and stating the assumptions is a key skill in human mathematical problem-solving. By prompting LLMs to perform this step explicitly, we can help them align their problem-solving process with how humans approach these problems, potentially leading to more reliable and interpretable solutions.",
            "Proposed Method": "We propose Assumption Elicitation Prompting (AEP), a two-stage prompting approach: 1) Given a math word problem, prompt the LLM to identify and explicitly state all the assumptions needed to solve the problem, based on the problem description and real-world knowledge. 2) Prompt the LLM to provide a step-by-step solution to the problem, using the explicitly stated assumptions from the first stage.",
            "Experiment Plan": "Evaluate AEP on math word problem benchmarks that require identifying implicit assumptions, such as the more challenging problems in MATH and GSM8K. Compare with baselines like Chain-of-Thought prompting. Conduct human evaluations to assess the relevance and completeness of the generated assumptions."
        },
        "Commonsense-Grounded Mathematical Reasoning Prompting": {
            "Problem": "Large language models often struggle to solve mathematical problems that require commonsense knowledge and reasoning, leading to solutions that are mathematically correct but nonsensical in the real world.",
            "Existing Methods": "Current methods for mathematical problem solving with LLMs primarily focus on the mathematical steps and calculations, without considering the commonsense aspects of the problem context.",
            "Motivation": "Incorporating commonsense knowledge and reasoning into the problem-solving process can help LLMs generate more realistic and contextually appropriate solutions. By prompting the model to consider the real-world implications and constraints of the problem, we can guide it towards more sensible and applicable solutions.",
            "Proposed Method": "We propose a multi-step prompting approach that integrates commonsense reasoning into the mathematical problem-solving process. First, the model is prompted to identify and extract relevant commonsense knowledge from the problem statement. Next, it is prompted to reason about the implications and constraints imposed by this commonsense knowledge on the mathematical solution. Finally, the model is prompted to generate a solution that satisfies both the mathematical requirements and the commonsense constraints. Throughout the process, the model is encouraged to provide explanations and justifications for its reasoning steps.",
            "Experiment Plan": "Evaluate the proposed method on datasets that require commonsense reasoning for mathematical problem solving, such as the Commonsense Math Problems (CMP) dataset. Compare the performance with baseline methods that do not incorporate commonsense reasoning. Assess the generated solutions for both mathematical correctness and commonsense validity."
        },
        "Generative Adversarial Prompting for Mathematical Reasoning": {
            "Problem": "Large language models can generate mathematically correct solutions that are not always the most efficient or elegant. They may also struggle to identify and correct errors in their own reasoning process.",
            "Existing Methods": "Current methods for mathematical problem solving with LLMs focus on generating a single solution, without considering alternative approaches or self-correction.",
            "Motivation": "By employing a generative adversarial approach, we can prompt the model to generate multiple competing solutions and then critically evaluate and refine them. This process can lead to more efficient, elegant, and robust solutions, as well as improve the model's ability to detect and correct its own errors.",
            "Proposed Method": "We propose a generative adversarial prompting approach for mathematical reasoning. First, the model is prompted to generate multiple candidate solutions for a given problem. Then, it is prompted to act as a critic and evaluate each solution based on criteria such as efficiency, elegance, and correctness. The model is encouraged to identify strengths, weaknesses, and potential errors in each solution. Finally, the model is prompted to refine and improve the most promising solution(s) based on the critic's feedback. This process can be iterated multiple times to further refine the solution.",
            "Experiment Plan": "Evaluate the proposed method on challenging mathematical problem-solving datasets, such as the MATH dataset. Compare the performance with baseline methods that generate a single solution without adversarial refinement. Assess the generated solutions for mathematical correctness, efficiency, and elegance. Analyze the model's ability to identify and correct errors in its own reasoning process."
        },
        "Metacognitive Uncertainty Calibration Prompting": {
            "Problem": "Large language models can generate overconfident solutions to mathematical problems, even when they are uncertain or lack sufficient understanding. This can lead to incorrect answers being presented as definitive.",
            "Existing Methods": "Current methods for mathematical problem solving with LLMs do not explicitly model or communicate the model's uncertainty about its solutions.",
            "Motivation": "By prompting the model to assess and calibrate its own uncertainty during the problem-solving process, we can encourage more responsible and transparent reasoning. This can help the model to identify areas where it lacks confidence or understanding, and to communicate this uncertainty to the user.",
            "Proposed Method": "We propose a metacognitive uncertainty calibration prompting approach. Throughout the problem-solving process, the model is prompted to assess its own uncertainty and confidence at each step. It is encouraged to identify areas where it lacks knowledge or understanding, and to express this uncertainty in its output. When generating the final solution, the model is prompted to provide a calibrated confidence score alongside the answer. This score reflects the model's overall certainty in the correctness of its solution, based on the accumulated uncertainties throughout the reasoning process.",
            "Experiment Plan": "Evaluate the proposed method on mathematical problem-solving datasets that include problems of varying complexity and difficulty, such as the GSM8K dataset. Compare the performance with baseline methods that do not explicitly model uncertainty. Assess the calibration of the model's confidence scores against its actual accuracy. Analyze the model's ability to identify and communicate its own uncertainties and knowledge gaps."
        },
        "Embodied Mathematical Reasoning Prompting": {
            "Problem": "Large language models can struggle with mathematical problems that require reasoning about physical systems and embodied concepts, such as spatial relationships, motion, and force interactions.",
            "Existing Methods": "Current methods for mathematical problem solving with LLMs primarily operate in an abstract, disembodied space, without considering the physical grounding of the problem.",
            "Motivation": "By prompting the model to reason about mathematical problems in an embodied context, we can help it to better understand and solve problems that involve physical systems. This can be achieved by encouraging the model to simulate or imagine the physical scenario described in the problem, and to reason about the mathematical relationships within this embodied context.",
            "Proposed Method": "We propose an embodied mathematical reasoning prompting approach. First, the model is prompted to construct a mental simulation or imagined scenario based on the physical details provided in the problem statement. It is encouraged to reason about the spatial relationships, motions, and interactions of the entities involved. Then, the model is prompted to identify the relevant mathematical relationships and constraints within this embodied context. Finally, it is prompted to solve the problem by applying mathematical reasoning within the simulated physical scenario.",
            "Experiment Plan": "Evaluate the proposed method on mathematical problem-solving datasets that involve physical systems and embodied reasoning, such as the Physics Questions Dataset (PQD). Compare the performance with baseline methods that do not explicitly incorporate embodied reasoning. Assess the model's ability to correctly identify and reason about the relevant physical relationships and constraints in the problems."
        },
        "Collaborative Mathematical Reasoning Prompting": {
            "Problem": "Large language models can generate step-by-step solutions to mathematical problems, but these solutions may not always be clear, concise, or easy for humans to follow and understand.",
            "Existing Methods": "Current methods for mathematical problem solving with LLMs focus on generating complete, autonomous solutions, without considering the needs and understanding of human collaborators.",
            "Motivation": "By prompting the model to engage in a collaborative problem-solving process with a human, we can encourage it to generate solutions that are more explainable, intuitive, and aligned with human understanding. This can be achieved by prompting the model to break down the solution into clear, manageable steps, and to engage in a back-and-forth dialogue with the human to clarify any confusions or misunderstandings.",
            "Proposed Method": "We propose a collaborative mathematical reasoning prompting approach. The model is prompted to engage in a dialogue with a human collaborator to solve a mathematical problem together. At each step, the model is prompted to explain its reasoning in clear, concise terms, and to check for the human's understanding. If the human expresses confusion or asks for clarification, the model is prompted to provide additional explanations or alternative perspectives. Throughout the process, the model is encouraged to break down the problem into manageable sub-steps, and to guide the human through the solution in an intuitive, easy-to-follow manner.",
            "Experiment Plan": "Evaluate the proposed method on mathematical problem-solving datasets, such as the MATH dataset, in a human-in-the-loop setting. Have human participants collaborate with the model to solve problems, and assess the clarity, conciseness, and understandability of the generated solutions. Compare the performance and user satisfaction with baseline methods that generate autonomous solutions without human collaboration."
        }
    }
}