{
    "topic_description": "novel prompting methods to reduce social biases and stereotypes of large language models",
    "all_queries": [
        "KeywordQuery(\"language model prompting reduce social bias stereotypes\")",
        "PaperQuery(\"4a2a1a107964c19a8b4a523a7fcd78e166e85f21\")",
        "GetReferences(\"2fd6ed9fa560547260005c3775f670007c4bfc09\")",
        "KeywordQuery(\"debiasing sentence representations\")",
        "PaperQuery(\"babeda48b10a4d638252118f2238d05a06f4ec55\")",
        "KeywordQuery(\"zero-shot bias mitigation techniques language models\")",
        "PaperQuery(\"88549b4f48b9709acdfb8b9e41656b6d133c5390\")",
        "GetReferences(\"4a693d96e65c54efd003fcd40424e0707220a7c2\")"
    ],
    "paper_bank": [
        {
            "id": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "paperId": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias."
            },
            "score": 9
        },
        {
            "id": "2488a75031fcd4d0f0ffe4fd0a5246325a71f241",
            "paperId": "2488a75031fcd4d0f0ffe4fd0a5246325a71f241",
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "abstract": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs\u2019 parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.",
            "year": 2023,
            "citationCount": 16,
            "score": 9
        },
        {
            "id": "2fd6ed9fa560547260005c3775f670007c4bfc09",
            "paperId": "2fd6ed9fa560547260005c3775f670007c4bfc09",
            "title": "Prompting Fairness: Learning Prompts for Debiasing Large Language Models",
            "abstract": "Large language models are prone to internalize social biases due to the characteristics of the data used for their self-supervised training scheme. Considering their recent emergence and wide availability to the general public, it is mandatory to identify and alleviate these biases to avoid perpetuating stereotypes towards underrepresented groups. We present a novel prompt-tuning method for reducing biases in encoder models such as BERT or RoBERTa. Unlike other methods, we only train a small set of additional reusable token embeddings that can be concatenated to any input sequence to reduce bias in the outputs. We particularize this method to gender bias by providing a set of templates used for training the prompts. Evaluations on two benchmarks show that our method is on par with the state of the art while having a limited impact on language modeling ability.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-tuning method for reducing biases in encoder models such as BERT or RoBERTa by training a small set of additional reusable token embeddings that can be concatenated to any input sequence to reduce bias in the outputs."
            },
            "score": 8
        },
        {
            "id": "9bc3a1b6c12f2859ae95e985b071993135f86ba2",
            "paperId": "9bc3a1b6c12f2859ae95e985b071993135f86ba2",
            "title": "MBIAS: Mitigating Bias in Large Language Models While Retaining Context",
            "abstract": "In addressing the critical need for safety in Large Language Models (LLMs), it is crucial to ensure that the outputs are not only safe but also retain their contextual accuracy. Many existing LLMs are safe fine-tuned either with safety demonstrations, or rely only on adversarial testing. While able to get safe outputs, they often risk losing contextual meaning as they mitigate bias and toxicity. In response, we present MBIAS, a LLM framework instruction fine-tuned on a custom dataset specifically designed for safety interventions. MBIAS aims to address the significant issues of bias and toxicity in LLMs generations that typically manifest as underrepresentation or negative portrayals across various demographics, including inappropriate linguistic mentions and biased content in social media. We experiment on MBIAS for safety interventions using various configurations, and demonstrate more than a 30\\% reduction in overall bias and toxicity while successfully retaining key information. Additionally, a demographic analysis on an out-of-distribution test set confirms the robustness of our approach, with reductions in bias and toxicity exceeding 90\\% across various demographics. The dataset and instruction fine-tuned MBIAS are made available to the research community at https://huggingface.co/newsmediabias/MBIAS.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "4a693d96e65c54efd003fcd40424e0707220a7c2",
            "paperId": "4a693d96e65c54efd003fcd40424e0707220a7c2",
            "title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation",
            "abstract": "Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
            "paperId": "8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "abstract": "Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs to improve zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning."
            },
            "score": 8
        },
        {
            "id": "240bea263d38ffbc35a8bb3eeb1470a2a0120de2",
            "paperId": "240bea263d38ffbc35a8bb3eeb1470a2a0120de2",
            "title": "Challenging Negative Gender Stereotypes: A Study on the Effectiveness of Automated Counter-Stereotypes",
            "abstract": "Gender stereotypes are pervasive beliefs about individuals based on their gender that play a significant role in shaping societal attitudes, behaviours, and even opportunities. Recognizing the negative implications of gender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically counteract and challenge these views. We present AI-generated gender-based counter-stereotypes to (self-identified) male and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness. The strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples, and empathy for the speaker were perceived as less effective. Also, the differences in ratings were more pronounced for stereotypes about the different targets than between the genders of the raters. Alarmingly, many AI-generated counter-stereotypes were perceived as offensive and/or implausible. Our analysis and the collected dataset offer foundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively challenge gender stereotypes in online interactions.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "089d3cde041a78bd9128feed10b4e1ff7c28398f",
            "paperId": "089d3cde041a78bd9128feed10b4e1ff7c28398f",
            "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
            "abstract": "Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access.",
            "year": 2024,
            "citationCount": 2,
            "score": 8
        },
        {
            "id": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
            "paperId": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
            "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
            "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model\u2019s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal\u2014modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT\u20132 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
            "year": 2022,
            "citationCount": 38,
            "score": 8
        },
        {
            "id": "88549b4f48b9709acdfb8b9e41656b6d133c5390",
            "paperId": "88549b4f48b9709acdfb8b9e41656b6d133c5390",
            "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
            "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting."
            },
            "score": 7
        },
        {
            "id": "0d965ed237a3b4592ecefdb618c29f63adedff76",
            "paperId": "0d965ed237a3b4592ecefdb618c29f63adedff76",
            "title": "Towards Debiasing Sentence Representations",
            "abstract": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",
            "year": 2020,
            "citationCount": 176,
            "score": 7
        },
        {
            "id": "5334e1857e910e2c7855c909c9495fb0ea28efbb",
            "paperId": "5334e1857e910e2c7855c909c9495fb0ea28efbb",
            "title": "Does Gender Matter? Towards Fairness in Dialogue Systems",
            "abstract": "Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as \u201cgorillas\u201d. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",
            "year": 2019,
            "citationCount": 118,
            "score": 7
        },
        {
            "id": "011095a0082e5e301f9bf30267b193c1c9e7e370",
            "paperId": "011095a0082e5e301f9bf30267b193c1c9e7e370",
            "title": "Perturbation Augmentation for Fairer NLP",
            "abstract": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
            "year": 2022,
            "citationCount": 39,
            "score": 7
        },
        {
            "id": "01c39795715404593230cb0f75007b48f156039f",
            "paperId": "01c39795715404593230cb0f75007b48f156039f",
            "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
            "abstract": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model\u2019s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data.Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.",
            "year": 2021,
            "citationCount": 19,
            "score": 7
        },
        {
            "id": "0cfd72e3d81f35bccc1e67f5992e112a601ba2ba",
            "paperId": "0cfd72e3d81f35bccc1e67f5992e112a601ba2ba",
            "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?",
            "abstract": "As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts shows Alpaca 7B performs best on the bias identification task and it is demonstrated that scaling up LLM size and data diversity could lead to further performance gain."
            },
            "score": 6
        },
        {
            "id": "3d864a8bc5a55ccab9993aa66203d8e70b88148c",
            "paperId": "3d864a8bc5a55ccab9993aa66203d8e70b88148c",
            "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models",
            "abstract": "Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.",
            "year": 2020,
            "citationCount": 176,
            "score": 6
        },
        {
            "id": "babeda48b10a4d638252118f2238d05a06f4ec55",
            "paperId": "babeda48b10a4d638252118f2238d05a06f4ec55",
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
            "year": 2020,
            "citationCount": 661,
            "score": 6
        },
        {
            "id": "e969aa3422a49152c22f3faf734e4561a2a3cf42",
            "paperId": "e969aa3422a49152c22f3faf734e4561a2a3cf42",
            "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
            "abstract": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
            "year": 2020,
            "citationCount": 291,
            "score": 6
        },
        {
            "id": "5e9c85235210b59a16bdd84b444a904ae271f7e7",
            "paperId": "5e9c85235210b59a16bdd84b444a904ae271f7e7",
            "title": "On Measuring Social Biases in Sentence Encoders",
            "abstract": "The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",
            "year": 2019,
            "citationCount": 473,
            "score": 6
        },
        {
            "id": "6f49541509d64cffe4b7b586c3a6c5386a06e442",
            "paperId": "6f49541509d64cffe4b7b586c3a6c5386a06e442",
            "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
            "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
            "year": 2023,
            "citationCount": 3,
            "score": 6
        },
        {
            "id": "5a839db23d769bef6fa224feda6bf17dd0688e8a",
            "paperId": "5a839db23d769bef6fa224feda6bf17dd0688e8a",
            "title": "Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models",
            "abstract": "Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as\"re-judge inconsistency\"in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": null,
            "score": 5
        },
        {
            "id": "130ab5c480860e330b65280a3410f17bb2d50fe1",
            "paperId": "130ab5c480860e330b65280a3410f17bb2d50fe1",
            "title": "Sustainable Modular Debiasing of Language Models",
            "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.",
            "year": 2021,
            "citationCount": 91,
            "score": 5
        },
        {
            "id": "5966d7c7f60898d610812e24c64d4d57855ad86a",
            "paperId": "5966d7c7f60898d610812e24c64d4d57855ad86a",
            "title": "Semantics derived automatically from language corpora contain human-like biases",
            "abstract": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
            "year": 2016,
            "citationCount": 2190,
            "score": 5
        },
        {
            "id": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
            "paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
            "title": "The Capacity for Moral Self-Correction in Large Language Models",
            "abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to\"morally self-correct\"-- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
            "year": 2023,
            "citationCount": 108,
            "score": 5
        },
        {
            "id": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1",
            "paperId": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1",
            "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
            "abstract": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories is presented and it is found that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova, GPT-3."
            },
            "score": 4
        },
        {
            "id": "e4282cab4a435d5249fc8db49fc1c9268438fedb",
            "paperId": "e4282cab4a435d5249fc8db49fc1c9268438fedb",
            "title": "Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West",
            "abstract": "Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. A large body of scholarship on LLM bias exists but it predominantly adopts a Western-centric frame and attends comparatively less to bias levels and potential harms in the Global South. In this paper, we quantify stereotypical bias in popular LLMs according to an Indian-centric frame and compare bias levels between the Indian and Western contexts. To do this, we develop a novel dataset which we call Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context. We finally investigate Instruction Prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for GPT-3.5. The findings of this work highlight the need for including more diverse voices when evaluating LLMs.",
            "year": 2023,
            "citationCount": 10,
            "tldr": null,
            "score": 4
        },
        {
            "id": "79b8689f49fb75c303991f86aa821ff63862d1d5",
            "paperId": "79b8689f49fb75c303991f86aa821ff63862d1d5",
            "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model",
            "abstract": "Existing bias mitigation methods require social-group-specific word pairs (e.g., \u201cman\u201d \u2013 \u201cwoman\u201d) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) \u2014 a theoretical framework developed in social psychology for understanding the content of stereotyping \u2014 can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: \u201cgenuine\u201d \u2013 \u201cfake\u201d; competence: \u201csmart\u201d \u2013 \u201cstupid\u201d), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches.",
            "year": 2023,
            "citationCount": 14,
            "tldr": null,
            "score": 4
        },
        {
            "id": "e1314f15692f5573b972d301bd32b70ec116cd38",
            "paperId": "e1314f15692f5573b972d301bd32b70ec116cd38",
            "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
            "abstract": "Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP, and that Counterfactual Data Augmentation (CDA) also mitigates bias amplification by DP."
            },
            "score": 4
        },
        {
            "id": "468c1d2d8e384472f313ff0487839839727b8934",
            "paperId": "468c1d2d8e384472f313ff0487839839727b8934",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "abstract": "Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An original framework for probing language models for societal biases, using a novel perplexity-based fairness score and a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections is proposed."
            },
            "score": 4
        },
        {
            "id": "124bcb7bd5608507b3c70aae12ea3bcd9a06377a",
            "paperId": "124bcb7bd5608507b3c70aae12ea3bcd9a06377a",
            "title": "Identifying and Neutralizing Gender Bias from Text",
            "abstract": "Gender bias refers to the prejudice and discrimination against a certain group based on their gender. In text, this bias can manifest itself through gendered language, which may incorrectly ascribe characteristics to an individual based on their gender. Gender bias in text often results in the perpetuation of stereotypes which contribute to systemic gender inequalities in broader society (Menegatti and Rubini, 2024). Our project focuses on addressing gender bias in social media posts. We developed a parallel Gender Bias Neutrality Corpus (GBNC) containing 1,049 gender biased sentence pairs and their unbiased corrected version. Additionally, we fine-tuned two LLMs\u2014GPT-3.5 and Llama2\u2014with the objective of more effectively neutralizing the gender bias in a given sentence. Our main finding is that both models saw a significant increase in our selected metrics which indicates that our dataset is an effective tool to develop gender recognition and correction programs.",
            "year": null,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "e369635f66727362c914a691a1cf43023306e9f9",
            "paperId": "e369635f66727362c914a691a1cf43023306e9f9",
            "title": "DrawL: Understanding the Effects of Non-Mainstream Dialects in Prompted Image Generation",
            "abstract": "Text-to-image models are now easy to use and ubiquitous. However, prior work has found that they are prone to recapitulating harmful Western stereotypes. For example, requesting that a model generate an\"African person and their house,\"may produce a person standing next to a straw hut. In this example, the word\"African\"is an explicit descriptor of the person that the prompt is seeking to depict. Here, we examine whether implicit markers, such as dialect, can also affect the portrayal of people in text-to-image outputs. We pair prompts in Mainstream American English with counterfactuals that express grammatical constructions found in dialects correlated with historically marginalized groups. We find that through minimal, syntax-only changes to prompts, we can systematically shift the skin tone and gender of people in the generated images. We conclude with a discussion of whether dialectic distribution shifts like this are harmful or are expected, possibly even desirable, model behavior.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "b9870e130f61ff900fe00dbcc5782c9b31773d32",
            "paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32",
            "title": "Learning to Compress Prompts with Gist Tokens",
            "abstract": "Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of\"gist\"tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.",
            "year": 2023,
            "citationCount": 84,
            "score": 4
        },
        {
            "id": "d9424371662717c8981eef3d501d7ce59c66ce77",
            "paperId": "d9424371662717c8981eef3d501d7ce59c66ce77",
            "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
            "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
            "year": 2022,
            "citationCount": 63,
            "score": 4
        },
        {
            "id": "45e50baac4d341f0cf1a40af096bfa9c3f555235",
            "paperId": "45e50baac4d341f0cf1a40af096bfa9c3f555235",
            "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
            "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs finds that longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time."
            },
            "score": 3
        },
        {
            "id": "6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
            "paperId": "6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
            "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups."
            },
            "score": 3
        },
        {
            "id": "72128b2da0ffb784861889462070570b21017b9f",
            "paperId": "72128b2da0ffb784861889462070570b21017b9f",
            "title": "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
            "abstract": "Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting.Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments.",
            "year": 2022,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper builds on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language."
            },
            "score": 3
        },
        {
            "id": "cf852c18387cfcdcb3cb3bb5ba6a549b35766c33",
            "paperId": "cf852c18387cfcdcb3cb3bb5ba6a549b35766c33",
            "title": "Gender bias and stereotypes in Large Language Models",
            "abstract": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models, and suggests that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."
            },
            "score": 3
        },
        {
            "id": "d2e57d6ba6989ed3e76884f3d0e0e77ba3118528",
            "paperId": "d2e57d6ba6989ed3e76884f3d0e0e77ba3118528",
            "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias",
            "abstract": "As language models (LMs) become increasingly powerful and widely used, it is important to quantify them for sociodemographic bias with potential for harm. Prior measures of bias are sensitive to perturbations in the templates designed to compare performance across social groups, due to factors such as low diversity or limited number of templates. Also, most previous work considers only one NLP task. We introduce Comprehensive Assessment of Language Models (CALM) for robust measurement of two types of universally relevant sociodemographic bias, gender and race. CALM integrates sixteen datasets for question-answering, sentiment analysis and natural language inference. Examples from each dataset are filtered to produce 224 templates with high diversity (e.g., length, vocabulary). We assemble 50 highly frequent person names for each of seven distinct demographic groups to generate 78,400 prompts covering the three NLP tasks. Our empirical evaluation shows that CALM bias scores are more robust and far less sensitive than previous bias measurements to perturbations in the templates, such as synonym substitution, or to random subset selection of templates. We apply CALM to 20 large language models, and find that for 2 language model series, larger parameter models tend to be more biased than smaller ones. The T0 series is the least biased model families, of the 20 LLMs investigated here. The code is available at https://github.com/vipulgupta1011/CALM.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Comprehensive Assessment of Language Models (CALM) is introduced for robust measurement of two types of universally relevant sociodemographic bias, gender and race, and is applied to 20 large language models, finding that for 2 language model series, larger parameter models tend to be more biased than smaller ones."
            },
            "score": 3
        },
        {
            "id": "ab714d43f0eff5439018578e860bbe250706dd0c",
            "paperId": "ab714d43f0eff5439018578e860bbe250706dd0c",
            "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text",
            "abstract": "Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data may end up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework NBIAS that consists of four main layers: data, corpus construction, model development and an evaluation layer. The dataset is constructed by collecting diverse data from various domains, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity BIAS. In the evaluation procedure, we incorporate a blend of quantitative and qualitative measures to gauge the effectiveness of our models. We achieve accuracy improvements ranging from 1% to 8% compared to baselines. We are also able to generate a robust understanding of the model functioning. The proposed approach is applicable to a variety of biases and contributes to the fair and ethical use of textual data.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity BIAS is applied that is applicable to a variety of biases and contributes to the fair and ethical use of textual data."
            },
            "score": 3
        },
        {
            "id": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "abstract": "In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d We release code and model checkpoints to reproduce our experiments.",
            "year": 2021,
            "citationCount": 2526,
            "score": 3
        },
        {
            "id": "bf46c77417ce45babd303eb397a78a83020ef319",
            "paperId": "bf46c77417ce45babd303eb397a78a83020ef319",
            "title": "ABLE: Agency-BeLiefs Embedding to Address Stereotypical Bias through Awareness Instead of Obliviousness",
            "abstract": "Natural Language Processing (NLP) models tend to inherit and amplify stereotypical biases present in their training data, leading to harmful societal consequences. Current efforts to rectify these biases typically revolve around making models oblivious to bias, which is at odds with the idea that humans require increased awareness to tackle these biases better. This prompts a fundamental research question: are bias-oblivious models the only viable solution to combat stereotypical biases? This paper answers this question by proposing the Agency-BeLiefs Embedding (ABLE) model, a novel approach that actively encodes stereotypical biases into the embedding space. ABLE draws upon social psychological theory to acquire and represent stereotypical biases in the form of agency and belief scores rather than directly representing stereotyped groups. Our experimental results showcase ABLE\u2019s effectiveness in learning agency and belief stereotypes while preserving the language model\u2019s proficiency. Furthermore, we underscore the practical significance of incorporating stereotypes within the ABLE model by demonstrating its utility in various downstream tasks. Our approach exemplifies the potential benefits of addressing bias through awareness, as opposed to the prevailing approach of mitigating bias through obliviousness.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "9ec7509cd2011fa2d11f88c8515e018fd6bc27a4",
            "paperId": "9ec7509cd2011fa2d11f88c8515e018fd6bc27a4",
            "title": "\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
            "abstract": "Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate\"harm\"as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "16d83e930a4dab2d49f5d276838ddce79df3f787",
            "paperId": "16d83e930a4dab2d49f5d276838ddce79df3f787",
            "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
            "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",
            "year": 2023,
            "citationCount": 133,
            "score": 3
        },
        {
            "id": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
            "paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b",
            "title": "BBQ: A hand-built bias benchmark for question answering",
            "abstract": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
            "year": 2021,
            "citationCount": 155,
            "score": 3
        },
        {
            "id": "969f45a3adf5e0bcf741447b1c67a0f3a386801a",
            "paperId": "969f45a3adf5e0bcf741447b1c67a0f3a386801a",
            "title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation",
            "abstract": "Automatic evaluation metrics are crucial to the development of generative systems. In recent years, pre-trained language model (PLM) based metrics, such as BERTScore, have been commonly adopted in various generation tasks. However, it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern about the fairness of PLMs as metrics. To that end, this work presents the first systematic study on the social bias in PLM-based metrics. We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely race, gender, religion, physical appearance, age, and socioeconomic status. In-depth analysis suggests that choosing paradigms (matching, regression, or generation) of the metric has a greater impact on fairness than choosing PLMs. In addition, we develop debiasing adapters that are injected into PLM layers, mitigating bias in PLM-based metrics while retaining high performance for evaluating text generation.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In-depth analysis suggests that choosing paradigms (matching, regression, or generation) of the metric has a greater impact on fairness than choosing PLMs, and develops debiasing adapters that are injected into PLM layers, mitigating bias in PLM-based metrics while retaining high performance for evaluating text generation."
            },
            "score": 2
        },
        {
            "id": "3a37fef290d76029c295201cc168c0f8ecb0a0cf",
            "paperId": "3a37fef290d76029c295201cc168c0f8ecb0a0cf",
            "title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
            "abstract": "The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher, and the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size."
            },
            "score": 2
        },
        {
            "id": "2ea64b7c7617f6cc1768373124ca0243d772a90f",
            "paperId": "2ea64b7c7617f6cc1768373124ca0243d772a90f",
            "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language",
            "abstract": "Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",
            "year": 2019,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias, they are not effective at spelling out more detailed explanations in terms of Social Bias Frames."
            },
            "score": 2
        },
        {
            "id": "9803d7e65c539a24fe1a54079c9a2085950b6d31",
            "paperId": "9803d7e65c539a24fe1a54079c9a2085950b6d31",
            "title": "FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models",
            "abstract": "Detecting stereotypes and biases in Large Language Models (LLMs) is crucial for enhancing fairness and reducing adverse impacts on individuals or groups when these models are applied. Traditional methods, which rely on embedding spaces or are based on probability metrics, fall short in revealing the nuanced and implicit biases present in various contexts. To address this challenge, we propose the FairMonitor framework and adopt a static-dynamic detection method for a comprehensive evaluation of stereotypes and biases in LLMs. The static component consists of a direct inquiry test, an implicit association test, and an unknown situation test, including 10,262 open-ended questions with 9 sensitive factors and 26 educational scenarios. And it is effective for evaluating both explicit and implicit biases. Moreover, we utilize the multi-agent system to construst the dynamic scenarios for detecting subtle biases in more complex and realistic setting. This component detects the biases based on the interaction behaviors of LLMs across 600 varied educational scenarios. The experimental results show that the cooperation of static and dynamic methods can detect more stereotypes and biased in LLMs.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "061de7676a0cc0a8abf07362710719e203b3f0b9",
            "paperId": "061de7676a0cc0a8abf07362710719e203b3f0b9",
            "title": "Are Text Classifiers Xenophobic? A Country-Oriented Bias Detection Method with Least Confounding Variables",
            "abstract": "Classical bias detection methods used in Machine Learning are themselves biased because of the different confounding variables implied in the assessment of the initial biases. First they are using templates that are syntactically simple and distant from the target data on which the model will deployed. Second, current methods are assessing biases in pre-trained language models or in dataset, but not directly on the fine-tuned classifier that can actually produce harms. We propose a simple method to detect the biases of a specific fine-tuned classifier on any type of unlabeled data. The idea is to study the classifier behavior by creating counterfactual examples directly on the target data distribution and quantify the amount of changes. In this work, we focus on named entity perturbations by applying a Named Entity Recognition on target-domain data and modifying them accordingly to most common names or location of a target group (gender and country), and this for several morphosynctactically different languages spoken in relation with the countries of the target groups. We used our method on two models available open-source that are likely to be deployed by industry, and on two tasks and domains. We first assess the bias of a multilingual sentiment analysis model trained over multiple-languages tweets and available open-source, and then a multilingual stance recognition model trained over several languages and assessed over English language. Finally we propose to link the perplexity of each example with the bias of the model, by looking at the change in label distribution with respect to the language of the target group. Our work offers a fine-grained analysis of the interactions between names and languages, revealing significant biases in multilingual models.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "a8faa332ac5e3e8d3f798b89858732f8ab95a470",
            "paperId": "a8faa332ac5e3e8d3f798b89858732f8ab95a470",
            "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes",
            "abstract": "With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "6754f9046ed3f3b95cd50c2cdc4b0171ced6bc77",
            "paperId": "6754f9046ed3f3b95cd50c2cdc4b0171ced6bc77",
            "title": "Uncovering Name-Based Biases in Large Language Models Through Simulated Trust Game",
            "abstract": "Gender and race inferred from an individual's name are a notable source of stereotypes and biases that subtly influence social interactions. Abundant evidence from human experiments has revealed the preferential treatment that one receives when one's name suggests a predominant gender or race. As large language models acquire more capabilities and begin to support everyday applications, it becomes crucial to examine whether they manifest similar biases when encountering names in a complex social interaction. In contrast to previous work that studies name-based biases in language models at a more fundamental level, such as word representations, we challenge three prominent models to predict the outcome of a modified Trust Game, a well-publicized paradigm for studying trust and reciprocity. To ensure the internal validity of our experiments, we have carefully curated a list of racially representative surnames to identify players in a Trust Game and rigorously verified the construct validity of our prompts. The results of our experiments show that our approach can detect name-based biases in both base and instruction-tuned models.",
            "year": 2024,
            "citationCount": 1,
            "score": 2
        },
        {
            "id": "68ccf70f9e3193eb11687bb8e12509641dd40652",
            "paperId": "68ccf70f9e3193eb11687bb8e12509641dd40652",
            "title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "54aebf4f0fe2f9a503b05c83f9e04dc8db6b7631",
            "paperId": "54aebf4f0fe2f9a503b05c83f9e04dc8db6b7631",
            "title": "QUEEREOTYPES: A Multi-Source Italian Corpus of Stereotypes towards LGBTQIA+ Community Members",
            "abstract": "The paper describes a dataset composed of two sub-corpora from two different sources in Italian. The QUEEREOTYPES corpus includes social media texts regarding LGBTQIA+ individuals, behaviors, ideology and events. The texts were collected from Facebook and Twitter in 2018 and were annotated for the presence of stereotypes, and orthogonal dimensions (such as hate speech, aggressiveness, offensiveness, and irony in one sub-corpus, and stance in the other). The resource was developed by Natural Language Processing researchers together with activists from an Italian LGBTQIA+ not-for-profit organization. The creation of the dataset allows the NLP community to study stereotypes against marginalized groups, individuals and, ultimately, to develop proper tools and measures to reduce the online spread of such stereotypes. A test for the robustness of the language resource has been performed by means of 5-fold cross-validation experiments. Finally, text classification experiments have been carried out with a fine-tuned version of AlBERTo (a BERT-based model pre-trained on Italian tweets) and mBERT, obtaining good results on the task of stereotype detection, suggesting that stereotypes towards different targets might share common traits.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "a7741b2753134653861ec0b82f617875c0ebd96e",
            "paperId": "a7741b2753134653861ec0b82f617875c0ebd96e",
            "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
            "abstract": "Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.",
            "year": 2023,
            "citationCount": 23,
            "score": 2
        },
        {
            "id": "db55cf346d7851cde50c1b25ab8fbcdaef8e2ee0",
            "paperId": "db55cf346d7851cde50c1b25ab8fbcdaef8e2ee0",
            "title": "Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs",
            "abstract": "Large language models (LLMs) exhibit excellent ability to understand human languages, but do they also understand their own language that appears gibberish to us? In this work we delve into this question, aiming to uncover the mechanisms underlying such behavior in LLMs. We employ the Greedy Coordinate Gradient optimizer to craft prompts that compel LLMs to generate coherent responses from seemingly nonsensical inputs. We call these inputs LM Babel and this work systematically studies the behavior of LLMs manipulated by these prompts. We find that the manipulation efficiency depends on the target text's length and perplexity, with the Babel prompts often located in lower loss minima compared to natural prompts. We further examine the structure of the Babel prompts and evaluate their robustness. Notably, we find that guiding the model to generate harmful texts is not more difficult than into generating benign texts, suggesting lack of alignment for out-of-distribution prompts.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "085d1dbae0159686503269d56fb2fc1091373355",
            "paperId": "085d1dbae0159686503269d56fb2fc1091373355",
            "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
            "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable \\textit{fairness-fluency} trade-offs. However, it is still under-explored to what extent the fluency \\textit{has to} be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "53d212ea368e75cc152c3cb287343da22849915e",
            "paperId": "53d212ea368e75cc152c3cb287343da22849915e",
            "title": "Large Language Models are Inconsistent and Biased Evaluators",
            "abstract": "The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low\"inter-sample\"agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.",
            "year": 2024,
            "citationCount": 3,
            "score": 1
        },
        {
            "id": "f7ff29cf1a4e954ce43a060df61a995ec58c2442",
            "paperId": "f7ff29cf1a4e954ce43a060df61a995ec58c2442",
            "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
            "abstract": "Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated\"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN. We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset.",
            "year": 2024,
            "citationCount": 1,
            "score": 1
        },
        {
            "id": "21d8c4163947fa22427724b61f54cc2081591f3a",
            "paperId": "21d8c4163947fa22427724b61f54cc2081591f3a",
            "title": "On the social bias of speech self-supervised models",
            "abstract": "Self-supervised learning (SSL) speech models have achieved remarkable performance in various tasks, yet the biased outcomes, especially affecting marginalized groups, raise significant concerns. Social bias refers to the phenomenon where algorithms potentially amplify disparate properties between social groups present in the data used for training. Bias in SSL models can perpetuate injustice by automating discriminatory patterns and reinforcing inequitable systems. This work reveals that prevalent SSL models inadvertently acquire biased associations. We probe how various factors, such as model architecture, size, and training methodologies, influence the propagation of social bias within these models. Finally, we explore the efficacy of debiasing SSL models through regularization techniques, specifically via model compression. Our findings reveal that employing techniques such as row-pruning and training wider, shallower models can effectively mitigate social bias within SSL model.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "9e69930d2191d31674af1bc8592a2fee32ea4ab7",
            "paperId": "9e69930d2191d31674af1bc8592a2fee32ea4ab7",
            "title": "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals",
            "abstract": "With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different models under this counterfactual generation setting at scale, producing over 57 million responses from popular LVLMs. Our multi-dimensional analysis reveals that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of depicted individuals. We additionally explore the relationship between social bias in LVLMs and their corresponding LLMs, as well as inference-time strategies to mitigate bias.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "5c7f465d162aade4a4c0eefb02fd7aadeebdaf58",
            "paperId": "5c7f465d162aade4a4c0eefb02fd7aadeebdaf58",
            "title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.",
            "year": 2024,
            "citationCount": 12,
            "score": 1
        },
        {
            "id": "6ca24da0beafd904c836234effb943f43882481b",
            "paperId": "6ca24da0beafd904c836234effb943f43882481b",
            "title": "Are Models Biased on Text without Gender-related Language?",
            "abstract": "Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings? To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language. By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at https://ucinlp.github.io/unstereo-eval.",
            "year": 2024,
            "citationCount": 4,
            "score": 1
        },
        {
            "id": "c0f21f3ab029a8916f7430003938f6f6f60bd31c",
            "paperId": "c0f21f3ab029a8916f7430003938f6f6f60bd31c",
            "title": "Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels",
            "abstract": "When prompting a language model (LM), users often expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles (i.e., a constitution) into a model is resource-intensive, technically challenging, and generally requires human preference labels or examples. We introduce SAMI, an iterative algorithm that finetunes a pretrained language model (without requiring preference labels or demonstrations) to increase the conditional mutual information between constitutions and self-generated responses given queries from a dataset. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a model that writes the principles. To avoid dependence on strong models for writing principles, we align a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct), achieving a 65% win rate on summarization. Finally, we investigate whether SAMI generalizes to diverse summarization principles (e.g.,\"summaries should be scientific\") and scales to stronger models (llama3-70b), finding that it achieves win rates of up to 68% for learned and 67% for held-out principles compared to the base model. Our results show that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.",
            "year": 2024,
            "citationCount": 3,
            "score": 1
        },
        {
            "id": "0c3d8ea68ed9114d74eceaffc5c32cd0ee31d993",
            "paperId": "0c3d8ea68ed9114d74eceaffc5c32cd0ee31d993",
            "title": "Understanding Privacy Risks of Embeddings Induced by Large Language Models",
            "abstract": "Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations. One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation. However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models. The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns. To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed. Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution. This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use. We further discuss preliminary strategies to mitigate this risk.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "5cbc3f8433576fcfd977a2f58af6968661cb9d8c",
            "paperId": "5cbc3f8433576fcfd977a2f58af6968661cb9d8c",
            "title": "Discovering Bias in Latent Space: An Unsupervised Debiasing Approach",
            "abstract": "The question-answering (QA) capabilities of foundation models are highly sensitive to prompt variations, rendering their performance susceptible to superficial, non-meaning-altering changes. This vulnerability often stems from the model's preference or bias towards specific input characteristics, such as option position or superficial image features in multi-modal settings. We propose to rectify this bias directly in the model's internal representation. Our approach, SteerFair, finds the bias direction in the model's representation space and steers activation values away from it during inference. Specifically, we exploit the observation that bias often adheres to simple association rules, such as the spurious association between the first option and correctness likelihood. Next, we construct demonstrations of these rules from unlabeled samples and use them to identify the bias directions. We empirically show that SteerFair significantly reduces instruction-tuned model performance variance across prompt modifications on three benchmark tasks. Remarkably, our approach surpasses a supervised baseline with 100 labels by an average of 10.86% accuracy points and 12.95 score points and matches the performance with 500 labels.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "445232b6d9ca35d277a182de2c404778cfe5b06f",
            "paperId": "445232b6d9ca35d277a182de2c404778cfe5b06f",
            "title": "Evidence-guided Inference for Neutralized Zero-shot Transfer",
            "abstract": "Human annotation is costly and impractical when it comes to scarcely labeled data. Besides, the presence of biased language in well-known benchmarks notably misleads predictive models to perform incredibly well, not because of the model capability but due to the hidden false correlations in the linguistic corpus. Motivated by this, we propose a neutralized Knowledge Transfer framework (NKT) to equip pre-trained language models with neutralized transferability. Specifically, we construct debiased multi-source corpora (CV and EL) for two exemplary knowledge transfer tasks: claim verification and evidence learning, respectively. To counteract biased language, we design a neutralization mechanism in the presence of label skewness. We also design a label adaptation mechanism in light of the mixed label systems in the multi-source corpora. In extensive experiments, the proposed NKT framework shows effective transferability contrarily to the disability of dominant baselines, particularly in the zero-shot cross-domain transfer setting.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "32733e5d5a18bfb2f5d79639ab5d1727cc84783d",
            "paperId": "32733e5d5a18bfb2f5d79639ab5d1727cc84783d",
            "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
            "abstract": "The efficacy of an large language model (LLM) generated text detector depends substantially on the availability of sizable training data. White-box zero-shot detectors, which require no such data, are nonetheless limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose an simple but effective black-box zero-shot detection approach, predicated on the observation that human-written texts typically contain more grammatical errors than LLM-generated texts. This approach entails computing the Grammar Error Correction Score (GECScore) for the given text to distinguish between human-written and LLM-generated text. Extensive experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.7% and showing strong robustness against paraphrase and adversarial perturbation attacks.",
            "year": 2024,
            "citationCount": 1,
            "score": 1
        },
        {
            "id": "14ed9eddc1c2cd7381fa9c3f47f961d2c32a057f",
            "paperId": "14ed9eddc1c2cd7381fa9c3f47f961d2c32a057f",
            "title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness",
            "abstract": "While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs' bias levels from stages of pretraining and debiasing.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "db8b244aed72fbf43dc6dc072cf3bf7309a23ea7",
            "paperId": "db8b244aed72fbf43dc6dc072cf3bf7309a23ea7",
            "title": "Quantitative Certification of Bias in Large Language Models",
            "abstract": "Large Language Models (LLMs) can produce responses that exhibit social biases and support stereotypes. However, conventional benchmarking is insufficient to thoroughly evaluate LLM bias, as it can not scale to large sets of prompts and provides no guarantees. Therefore, we propose a novel certification framework QuaCer-B (Quantitative Certification of Bias) that provides formal guarantees on obtaining unbiased responses from target LLMs under large sets of prompts. A certificate consists of high-confidence bounds on the probability of obtaining biased responses from the LLM for any set of prompts containing sensitive attributes, sampled from a distribution. We illustrate the bias certification in LLMs for prompts with various prefixes drawn from given distributions. We consider distributions of random token sequences, mixtures of manual jailbreaks, and jailbreaks in the LLM's embedding space to certify its bias. We certify popular LLMs with QuaCer-B and present novel insights into their biases.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "b0dfe2abeea8a9d1b815ffa9294494abf9241d0b",
            "paperId": "b0dfe2abeea8a9d1b815ffa9294494abf9241d0b",
            "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
            "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \\textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \\url{https://github.com/ledllm/ledllm}.",
            "year": 2024,
            "citationCount": 1,
            "score": 1
        },
        {
            "id": "0ec42647a631146d6e87731da1f28aa9fa62b146",
            "paperId": "0ec42647a631146d6e87731da1f28aa9fa62b146",
            "title": "Genshin: General Shield for Natural Language Processing with Large Language Models",
            "abstract": "Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "80a5b9ca71a81b8f032f60cc5a011e238f0e455b",
            "paperId": "80a5b9ca71a81b8f032f60cc5a011e238f0e455b",
            "title": "DUPE: Detection Undermining via Prompt Engineering for Deepfake Text",
            "abstract": "As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well. The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments. Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors. Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty. In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays. We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates. Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
            "paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
            "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
            "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
            "year": 2021,
            "citationCount": 445,
            "score": 1
        },
        {
            "id": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
            "year": 2019,
            "citationCount": 33226,
            "score": 1
        },
        {
            "id": "4099c4d272c12081b562392606e6d567e4ae7031",
            "paperId": "4099c4d272c12081b562392606e6d567e4ae7031",
            "title": "Masked Language Model Scoring",
            "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model\u2019s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL\u2019s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",
            "year": 2019,
            "citationCount": 369,
            "score": 1
        },
        {
            "id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "citationCount": 19233,
            "score": 1
        },
        {
            "id": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "citationCount": 14915,
            "score": 1
        },
        {
            "id": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models",
            "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
            "year": 2016,
            "citationCount": 1995,
            "score": 1
        },
        {
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "citationCount": 75333,
            "score": 1
        },
        {
            "id": "899f346ba482888897cb7b2426341f0b89d0c467",
            "paperId": "899f346ba482888897cb7b2426341f0b89d0c467",
            "title": "A Simple yet Effective Self-Debiasing Framework for Transformer Models",
            "abstract": "Current Transformer-based natural language understanding (NLU) models heavily rely on dataset biases, while failing to handle real-world out-of-distribution (OOD) instances. Many methods have been proposed to deal with this issue, but they ignore the fact that the features learned in different layers of Transformer-based NLU models are different. In this paper, we first conduct preliminary studies to obtain two conclusions: 1) both low- and high-layer sentence representations encode common biased features during training; 2) the low-layer sentence representations encode fewer unbiased features than the highlayer ones. Based on these conclusions, we propose a simple yet effective self-debiasing framework for Transformer-based NLU models. Concretely, we first stack a classifier on a selected low layer. Then, we introduce a residual connection that feeds the low-layer sentence representation to the top-layer classifier. In this way, the top-layer sentence representation will be trained to ignore the common biased features encoded by the low-layer sentence representation and focus on task-relevant unbiased features. During inference, we remove the residual connection and directly use the top-layer sentence representation to make predictions. Extensive experiments and indepth analyses on NLU tasks show that our framework performs better than several competitive baselines, achieving a new SOTA on all OOD test sets.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective self-debiasing framework for Transformer-based NLU models, which performs better than several competitive baselines, achieving a new SOTA on all OOD test sets."
            },
            "score": 1
        },
        {
            "id": "61ca0040d81c5ed71d3f9b9e5f7b528275048440",
            "paperId": "61ca0040d81c5ed71d3f9b9e5f7b528275048440",
            "title": "Debiasing Pre-trained Contextualised Embeddings",
            "abstract": "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",
            "year": 2021,
            "citationCount": 103,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings and finds that applying token-level debiasing for all tokens and across all layers of a contextualisedembedding model produces the best performance."
            },
            "score": 1
        },
        {
            "id": "97d6174cc2eca17ab9f1c14349d202abc728130b",
            "paperId": "97d6174cc2eca17ab9f1c14349d202abc728130b",
            "title": "Reducing the Vision and Language Bias for Temporal Sentence Grounding",
            "abstract": "Temporal sentence grounding (TSG) is an important yet challenging task in multimedia information retrieval. Although previous TSG methods have achieved decent performance, they tend to capture the selection biases of frequently appeared video-query pairs in the dataset rather than present robust multimodal reasoning abilities, especially for the rarely appeared pairs. In this paper, we study the above issue of selection biases and accordingly propose a Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both vision and language modalities for enhancing the model generalization ability. Specifically, we propose to alleviate the issue from two perspectives: 1) Feature distillation. We built a multi-modal debiasing branch to firstly capture the vision and language biases, and then apply a bias identification module to explicitly recognize the true negative biases and remove them from the benign multi-modal representations. 2) Contrastive sample generation. We construct two types of negative samples to enforce the model to accurately learn the aligned multi-modal semantics and make complete semantic reasoning. We apply the proposed model to both commonly and rarely appeared TSG cases, and demonstrate its effectiveness by achieving the state-of-the-art performance on three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).",
            "year": 2022,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Debiasing-TSG (D- TSG) model is proposed to filter and remove the negative biases in both vision and language modalities for enhancing the model generalization ability and achieves the state-of-the-art performance on three benchmark datasets."
            },
            "score": 1
        },
        {
            "id": "df157cb42b574c3f46b269504c18375bfa5bc5b1",
            "paperId": "df157cb42b574c3f46b269504c18375bfa5bc5b1",
            "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
            "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.",
            "year": 2021,
            "citationCount": 76,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network."
            },
            "score": 1
        },
        {
            "id": "71e9d7fa1d403dd70742af30c713a38af17215a0",
            "paperId": "71e9d7fa1d403dd70742af30c713a38af17215a0",
            "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
            "abstract": "Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT\u2019s internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT\u2019s next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated."
            },
            "score": 1
        },
        {
            "id": "ea667d3f5df2954c7365b8d1218889e2fc514829",
            "paperId": "ea667d3f5df2954c7365b8d1218889e2fc514829",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",
            "year": 2021,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and shows its effectiveness in sentence completion and summarization tasks, and proposes lexical co-occurrence-based bias penalization in the decoder units in generation frameworks."
            },
            "score": 1
        },
        {
            "id": "2ff522a22d744938bf5150a022904166d4dd45f8",
            "paperId": "2ff522a22d744938bf5150a022904166d4dd45f8",
            "title": "Evaluating Gender Bias in Hindi-English Machine Translation",
            "abstract": "With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",
            "year": 2021,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work attempts to evaluate and quantify the gender bias within a Hindi-English machine translation system by implementing a modified version of the existing TGBI metric based on the grammatical considerations for Hindi."
            },
            "score": 1
        },
        {
            "id": "a6f7a120f3bdbabe97f8563ce25bfef3fa652400",
            "paperId": "a6f7a120f3bdbabe97f8563ce25bfef3fa652400",
            "title": "RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank",
            "abstract": "Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and pushing dissimilar ones away. However, these methods fail to capture the fine-grained ranking information among the sentences, where each sentence is only treated as either positive or negative. In many real-world scenarios, one needs to distinguish and rank the sentences based on their similarities to a query sentence, e.g., very relevant, moderate relevant, less relevant, irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for unsupervised sentence representation learning, which incorporates ranking consistency and ranking distillation with contrastive learning into a unified framework. In particular, we learn semantically discriminative sentence representations by simultaneously ensuring ranking consistency between two representations with different dropout masks, and distilling listwise ranking knowledge from the teacher. An extensive set of experiments are conducted on both semantic textual similarity (STS) and transfer (TR) tasks. Experimental results demonstrate the superior performance of our approach over several state-of-the-art baselines.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel approach, RankCSE, for unsupervised sentence representation learning, which incorporates ranking consistency and ranking distillation with contrastive learning into a unified framework and demonstrates the superior performance of this approach over several state-of-the-art baselines."
            },
            "score": 1
        },
        {
            "id": "f08cebd0b795bc1520f1a868c729abecfb666f04",
            "paperId": "f08cebd0b795bc1520f1a868c729abecfb666f04",
            "title": "L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT",
            "abstract": "The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, Marathi, Kannada, Telugu, Malayalam, Tamil, Gujarati, Odia, Bengali, and Punjabi. The IndicSBERT exhibits strong cross-lingual capabilities and performs significantly better than alternatives like LaBSE, LASER, and paraphrase-multilingual-mpnet-base-v2 on Indic cross-lingual and monolingual sentence similarity tasks. We also release monolingual SBERT models for each of the languages and show that IndicSBERT performs competitively with its monolingual counterparts. These models have been evaluated using embedding similarity scores and classification accuracy.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, Marathi, Kannada, Telugu, Malayalam, Tamil, Gujarati, Odia, Bengali, and Punjabi, and shows the efficacy and applicability of this approach on 10 major Indic languages."
            },
            "score": 1
        },
        {
            "id": "0f8261b01eb0a150904729cf70f78d9e1bc26617",
            "paperId": "0f8261b01eb0a150904729cf70f78d9e1bc26617",
            "title": "Debiased Contrastive Learning of Unsupervised Sentence Representations",
            "abstract": "Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space.However, previous works mostly adopt in-batch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space.To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives.In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space.Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/DCLR.",
            "year": 2022,
            "citationCount": 72,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In DCLR, an instance weighting method is designed to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space and is more effective than competitive baselines."
            },
            "score": 1
        },
        {
            "id": "6529803d2d6190c0ff2735a3acb1854d42d1fe63",
            "paperId": "6529803d2d6190c0ff2735a3acb1854d42d1fe63",
            "title": "A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space",
            "abstract": "Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks. Though the BERT-like pre-trained language models have achieved great success, using their sentence representations directly often results in poor performance on the semantic textual similarity task. Recently, several contrastive learning methods have been proposed for learning sentence representations and have shown promising results. However, most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT-Xent, which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences. So in this paper, we propose a new method ArcCSE, with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences. We conduct extensive experiments which demonstrate that our approach outperforms the previous state-of-the-art on diverse sentence related tasks, including STS and SentEval.",
            "year": 2022,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method ArcCSE, with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences, and demonstrates that this approach outperforms the previous state-of-the-art on diverse sentence related tasks, including STS and SentEval."
            },
            "score": 1
        },
        {
            "id": "2cacc556ca3f362b09e22b875abffc35a7073fee",
            "paperId": "2cacc556ca3f362b09e22b875abffc35a7073fee",
            "title": "Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages",
            "abstract": "Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. A promising approach has been to train one-for-all multilingual models capable of cross-lingual transfer, but these models often suffer from insufficient capacity and interference between unrelated languages. Instead, we move away from this approach and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. To achieve this, we focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We introduce a new teacher-student training scheme which combines supervised and self-supervised training, allowing encoders to take advantage of monolingual training data, which is valuable in the low-resource setting. Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 50 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders, mine bitexts, and validate the bitexts by training NMT systems.",
            "year": 2022,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new teacher-student training scheme is introduced which combines supervised and self-supervised training, allowing encoders to take advantage of monolingual training data, which is valuable in the low-resource setting and significantly outperforms the original LASER encoder."
            },
            "score": 1
        },
        {
            "id": "14e9f752dfabb8ce34e310ed26c26a3f72a00e74",
            "paperId": "14e9f752dfabb8ce34e310ed26c26a3f72a00e74",
            "title": "Learning to Perturb for Contrastive Learning of Unsupervised Sentence Representations",
            "abstract": "Recently, contrastive learning has been shown effective in fine-tuning pre-trained language models (PLM) to learn sentence representations, which incorporates perturbations into unlabeled sentences to augment semantically related positive examples for training. However, previous works mostly adopt heuristic perturbation methods that are independent of the sentence representations. Since the perturbations are unaware of the goal or process of sentence representation learning during training, it is likely to lead to sub-optimal augmentations for conducting constrative learning. To address this issue, we propose a new framework L2P-CSR that adopts a learnable perturbation strategy for improving contrastive learning of sentence representations. In our L2P-CSR, we design a safer perturbation mechanism that only weakens the influence of tokens and features on the sentence representation, which avoids dramatically changing the semantics of the sentence representations. Besides, we devise a gradient-based algorithm to generate adaptive perturbations specially for the dynamically updated sentence representation during training. Such a way is more capable of augmenting high-quality examples that guide the sentence representation learning. Extensive experiments on diverse sentence-related tasks show that our approach outperforms competitive baselines.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs a safer perturbation mechanism that only weakens the influence of tokens and features on the sentence representation, which avoids dramatically changing the semantics of the sentence representations, and devise a gradient-based algorithm to generate adaptive perturbations specially for the dynamically updated sentence representation during training."
            },
            "score": 1
        },
        {
            "id": "6a7d09aeb02d3d3f9f070e4fcbe90216663a163f",
            "paperId": "6a7d09aeb02d3d3f9f070e4fcbe90216663a163f",
            "title": "ESCL: Equivariant Self-Contrastive Learning for Sentence Representations",
            "abstract": "Previous contrastive learning methods for sentence representations often focus on insensitive transformations to produce positive pairs, but neglect the role of sensitive transformations that are harmful to semantic representations. Therefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to make full use of sensitive transformations, which encourages the learned representations to be sensitive to certain types of transformations with an additional equivariant learning task. Meanwhile, in order to improve practicability and generality, ESCL simplifies the implementations of traditional equivariant contrastive methods to share model parameters from the perspective of multi-task learning. We evaluate our ESCL on semantic textual similarity tasks. The proposed method achieves better results while using fewer learning parameters compared to previous methods.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed Equivariant Self-Contrastive Learning method to make full use of sensitive transformations, which encourages the learned representations to be sensitive to certain types of transformations with an additional equivariant learning task, achieves better results while using fewer learning parameters compared to previous methods."
            },
            "score": 1
        },
        {
            "id": "9da6deed86db0c6b156d1961f6ff2ea1e9e830bf",
            "paperId": "9da6deed86db0c6b156d1961f6ff2ea1e9e830bf",
            "title": "Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization",
            "abstract": "Multilingual sentence representations are the foundation for similarity-based bitext mining, which is crucial for scaling multilingual neural machine translation (NMT) system to more languages. In this paper, we introduce MuSR: a one-for-all Multilingual Sentence Representation model that supports more than 220 languages. Leveraging billions of English-centric parallel corpora, we train a multilingual Transformer encoder, coupled with an auxiliary Transformer decoder, by adopting a multilingual NMT framework with CrossConST, a cross-lingual consistency regularization technique proposed in Gao et al. (2023). Experimental results on multilingual similarity search and bitext mining tasks show the effectiveness of our approach. Specifically, MuSR achieves superior performance over LASER3 (Heffernan et al., 2022) which consists of 148 independent multilingual sentence encoders.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MuSR is introduced: a one-for-all Multilingual Sentence Representation model that supports more than 220 languages and achieves superior performance over LASER3 (Heffernan et al., 2022), which consists of 148 independent multilingual sentence encoders."
            },
            "score": 1
        },
        {
            "id": "dbcced1c0f3b01f66f1dc1b820f084d440b28d1e",
            "paperId": "dbcced1c0f3b01f66f1dc1b820f084d440b28d1e",
            "title": "SONAR: Sentence-Level Multimodal and Language-Agnostic Representations",
            "abstract": "We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces SONAR, a new multilingual and multimodal fixed-size sentence embedding space, and provides a text decoder for 200 languages, which allows us to perform text-to-text and speech- to-text machine translation, including for zero-shot language and modality combinations."
            },
            "score": 1
        },
        {
            "id": "14da34b9f3939b120b5d7e13b2b8831e19e1011b",
            "paperId": "14da34b9f3939b120b5d7e13b2b8831e19e1011b",
            "title": "Your Stereotypical Mileage May Vary: Practical Challenges of Evaluating Biases in Multiple Languages and Cultural Contexts",
            "abstract": "Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting The study of bias, fairness and social impact in Natural Language Processing (NLP) lacks resources in languages other than English. Our objective is to support the evaluation of bias in language models in a multilingual setting. We use stereotypes across nine types of biases to build a corpus containing contrasting sentence pairs, one sentence that presents a stereotype concerning an underadvantaged group and another minimally changed sentence, concerning a matching advantaged group. We build on the French CrowS-Pairs corpus and guidelines to provide translations of the existing material into seven additional languages. In total, we produce 11,139 new sentence pairs that cover stereotypes dealing with nine types of biases in seven cultural contexts. We use the final resource for the evaluation of relevant monolingual and multilingual masked language models. We find that language models in all languages favor sentences that express stereotypes in most bias categories. The process of creating a resource that covers a wide range of language types and cultural settings highlights the difficulty of bias evaluation, in particular comparability across languages and contexts.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "762ea394eaed05dacb9854ba5101ac1336af104f",
            "paperId": "762ea394eaed05dacb9854ba5101ac1336af104f",
            "title": "Human vs. Machine Perceptions on Immigration Stereotypes",
            "abstract": "The increasing popularity of natural language processing has led to a race to improve machine learning models that often leaves aside the core study object, the language itself. In this study, we present classification models designed to detect stereotypes related to immigrants, along with both quantitative and qualitative analyses, shedding light on linguistic distinctions in how humans and various models perceive stereotypes. Given the subjective nature of this task, one of the models incorporates the judgments of all annotators by utilizing soft labels. Through a comparative analysis of BERT-based models using both hard and soft labels, along with predictions from GPT-4, we gain a clearer understanding of the linguistic challenges posed by texts containing stereotypes. Our dataset comprises Spanish Twitter posts collected as responses to immigrant-related hoaxes, annotated with binary values indicating the presence of stereotypes, implicitness, and the requirement for conversational context to understand the stereotype. Our findings suggest that both model prediction confidence and inter-annotator agreement are higher for explicit stereotypes, while stereotypes conveyed through irony and other figures of speech prove more challenging to detect than other implicit stereotypes.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "a4dfb031579b52e9bd4f05c5d5e7c7531843a50b",
            "paperId": "a4dfb031579b52e9bd4f05c5d5e7c7531843a50b",
            "title": "Perspectives on Hate: General vs. Domain-Specific Models",
            "abstract": "The rise of online hostility, combined with broad social media use, leads to the necessity of the comprehension of its human impact. However, the process of hate identification is challenging because, on the one hand, the line between healthy disagreement and poisonous speech is not well defined, and, on the other hand, multiple socio-cultural factors or prior beliefs shape people\u2019s perceptions of potentially harmful text. To address disagreements in hate speech identification, Natural Language Processing (NLP) models must capture several perspectives. This paper introduces a strategy based on the Contrastive Learning paradigm for detecting disagreements in hate speech using pre-trained language models. Two approaches are proposed: the General Model, a comprehensive framework, and the Domain-Specific Model, which focuses on more specific hate-related tasks. The source code is available at ://anonymous.4open.science/r/Disagreement-530C.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "e5790aca2bdfbe61ddbb4ac8af3ef9cae163e56e",
            "paperId": "e5790aca2bdfbe61ddbb4ac8af3ef9cae163e56e",
            "title": "Investigating Gender Bias in Turkish Language Models",
            "abstract": "Language models are trained mostly on Web data, which often contains social stereotypes and biases that the models can inherit. This has potentially negative consequences, as models can amplify these biases in downstream tasks or applications. However, prior research has primarily focused on the English language, especially in the context of gender bias. In particular, grammatically gender-neutral languages such as Turkish are underexplored despite representing different linguistic properties to language models with possibly different effects on biases. In this paper, we fill this research gap and investigate the significance of gender bias in Turkish language models. We build upon existing bias evaluation frameworks and extend them to the Turkish language by translating existing English tests and creating new ones designed to measure gender bias in the context of T\\\"urkiye. Specifically, we also evaluate Turkish language models for their embedded ethnic bias toward Kurdish people. Based on the experimental results, we attribute possible biases to different model characteristics such as the model size, their multilingualism, and the training corpora. We make the Turkish gender bias dataset publicly available.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "7a25122b1e8826a3a3951bfa917d5adf2d520601",
            "paperId": "7a25122b1e8826a3a3951bfa917d5adf2d520601",
            "title": "Exploring the Relationship Between Intrinsic Stigma in Masked Language Models and Training Data Using the Stereotype Content Model",
            "abstract": "Much work has gone into developing language models of increasing size, but only recently have we begun to examine them for pernicious behaviour that could lead to harming marginalised groups. Following Lin et al. (2022) in rooting our work in psychological research, we prompt two masked language models (MLMs) of different specialisations in English and Spanish with statements from a questionnaire developed to measure stigma to determine if they treat physical and mental illnesses equally. In both models we find a statistically significant difference in the treatment of physical and mental illnesses across most if not all latent constructs as measured by the questionnaire, and thus they are more likely to associate mental illnesses with stigma. We then examine their training data or data retrieved from the same domain using a computational implementation of the Stereotype Content Model (SCM) (Fiske et al., 2002; Fraser et al., 2021) to interpret the questionnaire results based on the SCM values as reflected in the data. We observe that model behaviour can largely be explained by the distribution of the mentions of illnesses according to their SCM values.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "8d37e69694034bdd181ed48a0a59c9455b5cc208",
            "paperId": "8d37e69694034bdd181ed48a0a59c9455b5cc208",
            "title": "Gendered Grammar or Ingrained Bias? Exploring Gender Bias in Icelandic Language Models",
            "abstract": "Large language models, trained on vast datasets, exhibit increased output quality in proportion to the amount of data that is used to train them. This data-driven learning process has brought forth a pressing issue where these models may not only reflect but also amplify gender bias, racism, religious prejudice, and queerphobia present in their training data that may not always be recent. This study explores gender bias in language models trained on Icelandic, focusing on occupation-related terms. Icelandic is a highly grammatically gendered language that favors the masculine when referring to groups of people with indeterminable genders. Our aim is to explore whether language models merely mirror gender distributions within the corresponding professions or if they exhibit biases tied to their grammatical genders. Results indicate a significant overall predisposition towards the masculine but specific occupation terms consistently lean toward a particular gender, indicating complex interplays of societal and linguistic influences.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "b8e298f3b46722db96e46af6b16634b7dd9d22d5",
            "paperId": "b8e298f3b46722db96e46af6b16634b7dd9d22d5",
            "title": "Do Large Language Models Understand Mansplaining? Well, Actually...",
            "abstract": "Gender bias has been widely studied by the NLP community. However, other more subtle variations of it, such as mansplaining, have yet received little attention. Mansplaining is a discriminatory behaviour that consists of a condescending treatment or discourse towards women. In this paper, we introduce and analyze Well, actually..., a corpus of 886 mansplaining stories experienced by women. We analyze the corpus in terms of features such as offensiveness, sentiment or misogyny, among others. We also explore to what extent Large Language Models (LLMs) can understand and identify mansplaining and other gender-related microaggressions. Specifically, we experiment with ChatGPT-3.5-Turbo and LLaMA-2 (13b and 70b), with both targeted and open questions. Our findings suggest that, although they can identify mansplaining to some extent, LLMs still struggle to point out this attitude and will even reproduce some of the social patterns behind mansplaining situations, for instance by praising men for giving unsolicited advice to women.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "15f50249f554204afbf3c0e70b193e0ea6da1675",
            "paperId": "15f50249f554204afbf3c0e70b193e0ea6da1675",
            "title": "Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification",
            "abstract": "The zero-shot learning capabilities of large language models (LLMs) make them ideal for text classi\ufb01cation without annotation or supervised training. Many studies have shown impressive results across multiple tasks. While tasks, data, and results differ widely, their similarities to human annotation can aid us in tackling new tasks with minimal expenses. We evaluate using 5 state-of-the-art LLMs as \u201cannotators\u201d on 5 different tasks (age, gender, topic, sentiment prediction, and hate speech detection), across 4 languages: English, French, German, and Spanish. No single model excels at all tasks, across languages, or across all labels within a task. However, aggregation techniques designed for human annotators perform substantially better than any one individual model. Overall, though, LLMs do not rival even simple supervised models, so they do not (yet) replace the need for human annotation. We also discuss the trade-offs between speed, accuracy, cost, and bias when it comes to aggregated model labeling versus human annotation.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Overall, LLMs do not rival even simple supervised models, so they do not (yet) replace the need for human annotation, but aggregation techniques designed for human annotators perform substantially better than any one individual model."
            },
            "score": 1
        },
        {
            "id": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "paperId": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities",
            "abstract": "We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image and that aggregating analyses over all concepts can mask these concerns; (ii) model calibration (i.e. the relationship between accuracy and confidence) also differs distinctly by perceived gender, even when evaluating on similar representations of concepts; and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can also contribute to social biases in zero-shot vision settings. Furthermore, biases can further propagate when foundational models like CLIP are used by other models to enable zero-shot capabilities.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates different vision-language models with multiple datasets across a set of concepts and finds all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image."
            },
            "score": 1
        },
        {
            "id": "c4f9f0cc8c138047a61bdb11b1a352e3d1aed035",
            "paperId": "c4f9f0cc8c138047a61bdb11b1a352e3d1aed035",
            "title": "Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers",
            "abstract": "Understanding labour market dynamics requires accurately identifying the skills required for and possessed by the workforce. Automation techniques are increasingly being developed to support this effort. However, automatically extracting skills from job postings is challenging due to the vast number of existing skills. The ESCO (European Skills, Competences, Qualifications and Occupations) framework provides a useful reference, listing over 13,000 individual skills. However, skills extraction remains difficult and accurately matching job posts to the ESCO taxonomy is an open problem. In this work, we propose an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs). We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts. We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM. Using synthetic data achieves an RP@10 score 10 points higher than previous distant supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22 points over previous methods. We also show that Framing the task as mock programming when prompting the LLM can lead to better performance than natural language prompts, especially with weaker LLMs. We demonstrate the potential of integrating large language models at both ends of skills matching pipelines. Our approach requires no human annotations and achieve extremely promising results on skills extraction against ESCO.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs) and achieves extremely promising results on skills extraction against ESCO."
            },
            "score": 1
        },
        {
            "id": "9c1485ae4f96fd8942c124e7b1564fd929e71a42",
            "paperId": "9c1485ae4f96fd8942c124e7b1564fd929e71a42",
            "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Disparities Between Gender Groups",
            "abstract": "We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences when identifying concepts based on the gender of the person co-occurring in the image (ii) model calibration (i.e., the relationship between accuracy and confidence) also differs distinctly by gender, even when evaluating on similar representations of concepts and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can contribute to propagating social biases in zero-shot settings.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that, while language greatly expands the capability of vision tasks, it can contribute to propagating social biases in zero-shot settings."
            },
            "score": 1
        },
        {
            "id": "e8ca5789a5ea38a60b9d68b5570ffe09cdc3e159",
            "paperId": "e8ca5789a5ea38a60b9d68b5570ffe09cdc3e159",
            "title": "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias",
            "abstract": "We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 1
        },
        {
            "id": "23c265ba884b92ecbd9d18641078d964697e4590",
            "paperId": "23c265ba884b92ecbd9d18641078d964697e4590",
            "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
            "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.",
            "year": 2022,
            "citationCount": 142,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: a unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectionalPLM."
            },
            "score": 1
        },
        {
            "id": "497acdc02c50073e714838a8d4f16f7482d37e64",
            "paperId": "497acdc02c50073e714838a8d4f16f7482d37e64",
            "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
            "abstract": "Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) \\textsc{SumAsk} consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the summarize-and-ask prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format and investigates the capabilities of LLMs on zero-shot RE."
            },
            "score": 1
        },
        {
            "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "citationCount": 2089,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples."
            },
            "score": 1
        },
        {
            "id": "823af0830ee85a83884cc44d47689fc587a5fd3c",
            "paperId": "823af0830ee85a83884cc44d47689fc587a5fd3c",
            "title": "Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models",
            "abstract": "In the context of information systems, text sanitization techniques are used to identify and remove sensitive data to comply with security and regulatory requirements. Even though many methods for privacy preservation have been proposed, most of them are focused on the detection of entities from specific domains (e.g., credit card numbers, social security numbers), lacking generality and requiring customization for each desirable domain. Moreover, removing words is, in general, a drastic measure, as it can degrade text coherence and contextual information. Less severe measures include substituting a word for a safe alternative, yet it can be challenging to automatically find meaningful substitutions. We present a zero-shot text sanitization technique that detects and substitutes potentially sensitive information using Large Language Models. Our evaluation shows that our method excels at protecting privacy while maintaining text coherence and contextual information, preserving data utility for downstream tasks.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a zero-shot text sanitization technique that detects and substitutes potentially sensitive information using Large Language Models and shows that this method excels at protecting privacy while maintaining text coherence and contextual information, preserving data utility for downstream tasks."
            },
            "score": 1
        },
        {
            "id": "a7fc585cc4c2b6822646b2c410e0c427a20798f2",
            "paperId": "a7fc585cc4c2b6822646b2c410e0c427a20798f2",
            "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
            "abstract": "Today's large language models (LLMs) typically train on short text segments (e.g.,<4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues. Through theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis further reveals that commonly used techniques like truncating the attention window or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over the original model. Our code will be publicly available upon publication.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Infinite is a simple and effective method for enhancing LLMs' capabilities of handling long contexts, which allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity."
            },
            "score": 1
        },
        {
            "id": "66476832701361c9f9b2a7eb2354ee8cd9f72e67",
            "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67",
            "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
            "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TitanFuzz is demonstrated that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches."
            },
            "score": 1
        },
        {
            "id": "f4e723958a93762befb4d4a039b44a7d752f9917",
            "paperId": "f4e723958a93762befb4d4a039b44a7d752f9917",
            "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
            "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.",
            "year": 2023,
            "citationCount": 131,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the capacity of LLMs that act as the ranking model for recommender systems, and shows that LLMs have promising zero-shot ranking abilities but struggle to perceive the order of historical interactions, and can be biased by popularity or item positions in the prompts."
            },
            "score": 1
        },
        {
            "id": "123acfbccca0460171b6b06a4012dbb991cde55b",
            "paperId": "123acfbccca0460171b6b06a4012dbb991cde55b",
            "title": "Large Language Models Are Zero-Shot Time Series Forecasters",
            "abstract": "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.",
            "year": 2023,
            "citationCount": 88,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values and shows how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions."
            },
            "score": 1
        },
        {
            "id": "631cee335dbae8f883f426b119686058c4b26951",
            "paperId": "631cee335dbae8f883f426b119686058c4b26951",
            "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models",
            "abstract": "Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model\u2019s zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The reference dataset should have semantic diversity but no need to be labeled, seen in pre-training, or matched image-text pairs. In parameter space, we prevent a large parameter shift by averaging weights during the training. We propose a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods, where tasks are from various domains instead of class-separated in a single dataset. Our method outperforms other methods in the traditional class-incremental learning setting and the MTIL by 9.7% average score. Our code locates at https: //github.com/Thunderbeee/ZSCL.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method ZSCL is proposed to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space and a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods."
            },
            "score": 1
        },
        {
            "id": "b0bac6aca93021105c8a4f165184a097a249fbce",
            "paperId": "b0bac6aca93021105c8a4f165184a097a249fbce",
            "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
            "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple method is proposed by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions, and it is shown that this method consistently improves the robustness of instruction-tuned models."
            },
            "score": 1
        },
        {
            "id": "4f91b303a2d87749757300a4987cd471b5f0345e",
            "paperId": "4f91b303a2d87749757300a4987cd471b5f0345e",
            "title": "Im2Vide0: A Zero-Shot approach using diffusion models for natural language conditioned Image-to-Video",
            "abstract": "Recent breakthroughs in the study of Denoising Diffusion Models have constituted a driving factor in advancing the field of Computer Vision through the aid of generative Machine Learning. Challenging problems, such as image or video synthesis, become intertwined with Natural Language Processing techniques to push the boundaries of computational capabilities and foster human ingenuity. We present an approach to solving text-guided video completion, a difficult task of producing the temporal future progression of an image, conditioned by textual description. Our proposed framework applies the latest academic leaps in Text-to-Image and Text-to-Video generation to perform Image Animation guided by text, an insufficiently explored topic due to its strenuous nature. We present how adapting generalized pre-trained Text-to-Image models can achieve good performance solving the described problem in a Zero-Shot context. Our experimental evaluation shows that the proposed solution is robust, capable of animating any source image guided by a language prompt, by inheriting the generality of the backbone pre-trained Text-to-Image model.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an approach to solving text-guided video completion, a difficult task of producing the temporal future progression of an image, conditioned by textual description, and presents how adapting generalized pre-trained Text-to-Image models can achieve good performance solving the described problem in a Zero-Shot context."
            },
            "score": 1
        },
        {
            "id": "019ff6a9ad920becc05230bc498d2a80d6daef20",
            "paperId": "019ff6a9ad920becc05230bc498d2a80d6daef20",
            "title": "Zero-Shot In-Distribution Detection in Multi-Object Settings Using Vision-Language Foundation Models",
            "abstract": "Extracting in-distribution (ID) images from noisy images scraped from the Internet is an important preprocessing for constructing datasets, which has traditionally been done manually. Automating this preprocessing with deep learning techniques presents two key challenges. First, images should be collected using only the name of the ID class without training on the ID data. Second, as we can see why COCO was created, it is crucial to identify images containing not only ID objects but also both ID and out-of-distribution (OOD) objects as ID images to create robust recognizers. In this paper, we propose a novel problem setting called zero-shot in-distribution (ID) detection, where we identify images containing ID objects as ID images (even if they contain OOD objects), and images lacking ID objects as OOD images without any training. To solve this problem, we leverage the powerful zero-shot capability of CLIP and present a simple and effective approach, Global-Local Maximum Concept Matching (GL-MCM), based on both global and local visual-text alignments of CLIP features. Extensive experiments demonstrate that GL-MCM outperforms comparison methods on both multi-object datasets and single-object ImageNet benchmarks. The code will be available via https://github.com/AtsuMiyai/GL-MCM.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel problem setting called zero-shot in-distribution (ID) detection, where images containing ID objects are identified as ID images (even if they contain OOD objects), and images lacking ID objects as OOD images without any training."
            },
            "score": 1
        },
        {
            "id": "5d56f58baf377f191721062ceb64ff1b029f67b2",
            "paperId": "5d56f58baf377f191721062ceb64ff1b029f67b2",
            "title": "Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias",
            "abstract": "Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS. However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results. We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. From this perspective, we carefully design a novel and large zero-shot TTS system called Mega-TTS, which is trained with large-scale wild data and models different attributes in different ways: 1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well. Phase can be appropriately constructed by the GAN-based vocoder and does not need to be modeled by the language model. 2) We model the timbre using global vectors since timbre is a global attribute that changes slowly over time. 3) We further use a VQGAN-based acoustic model to generate the spectrogram and a latent code language model to fit the distribution of prosody, since prosody changes quickly over time in a sentence, and language models can capture both local and long-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K hours of speech and evaluate its performance on unseen speakers. Experimental results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module. Audio samples are available at https://mega-tts.github.io/demo-page.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module."
            },
            "score": 1
        },
        {
            "id": "204a2467c35f16e120af9088b1336de1b51015c4",
            "paperId": "204a2467c35f16e120af9088b1336de1b51015c4",
            "title": "INTERSECTIONAL OPPRESSIONS THROUGH LANGUAGE: CASE STUDIES IN RACE, GENDER AND CLASS IN TO KILL A MOCKINGBIRD AND MY FAIR LADY",
            "abstract": "This study aims to investigate the impact of many dimensions on language use in Harper Lee\u2019s To kill a mockingbird and George Cukor\u2019s My fair lady. Regarding the first one, we distinctly focus on racially charged terminology, further exploring gender and class biases portrayed in the second one. Our methodology involves in-depth discourse analysis and critical examination of linguistic nuances, societal implications and narrative contexts within the works. The main hypothesis is that language acts as both a reflection and a catalyst for societal norms, playing a pivotal role in shaping perceptions, either for inclusivity or exclusion within specific communities.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "96e6fd9538548cd8b919bbf17f0b56536657f768",
            "paperId": "96e6fd9538548cd8b919bbf17f0b56536657f768",
            "title": "\u201cDon\u2019t respond\u201d: sexting and scrolling in First Nations\u2019 queer literature",
            "abstract": "Queer and trans First Nations literatures offer a complex range of perspectives on social media use. In this piece, written as a letter addressing an anonymous brotherboy character called Benny, who is based on a person that catfished and harassed me online, I examine three Indigenous books that present complex, critical, or disillusioned accounts of social media use, exploring the forms of deception, harassment, racism, and creativity enabled by digital media. I engage loosely with the practice of ficto-criticism to produce this article. Ficto-critical writing, a method of anthropological and cultural studies, subverts traditional academic writing; presenting a hallucinatory form of self-narration and anthropological writing. Using this interdisciplinary and experimental approach, this article experiments with the concept of anonymity and privacy, key themes in the writing of queer First Nations authors on the topic of the internet.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "02ed641819e97980f7167ff34f08ef723a510194",
            "paperId": "02ed641819e97980f7167ff34f08ef723a510194",
            "title": "Many Mickles Make a Muckle: Evidence That Gender Stereotypes Reemerge Spontaneously Via Cultural Evolution.",
            "abstract": "We explore whether societal gender stereotypes re-emerge as social information is repeatedly passed from person to person. We examined whether peoples' memories of personality attributes associated with female and male social targets became increasingly consistent with societal gender stereotypes as information was passed down social transmission chains. After passing through the memories of just four generations of participants, our initially gender-balanced micro-societies became rife with traditional gender stereotypes. While we found some evidence of the re-emergence of gender stereotypes in Experiment 1, we found the effects were stronger when targets appeared in a feminine-stereotyped occupational context (Experiment 2), and a masculine-stereotyped occupational context (Experiment 3); conversely, the re-emergence of gender stereotypes was attenuated when targets appeared in a single gender context (Experiment 4). The current findings demonstrate that gender schematic memory bias, if widely shared, might cause gender stereotypes to be maintained through cultural evolution.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "ec4837963cf10acac2943afc89413b4f3ca21d57",
            "paperId": "ec4837963cf10acac2943afc89413b4f3ca21d57",
            "title": "Gender Identities and Construction in Ekegusi Proverbs",
            "abstract": "Gender inequality has been a menace among the Abagusii community. In most cases, women are discriminated against for major pertinent issues. Ekegusii proverbs are used to give a true picture of the values and norms of the community. The study applied Critical Discourse Analysis (CDA) by Fairclaugh (1989) which addresses unfairness, injustice and inequalities. A descriptive qualitative design was used to make the research more effective. The researcher adopted purposive sampling to select gender-related proverbs to make this research relevant. The study concludes that gender ideologies and latent structures make a significant contribution to gender discrimination, especially when they are accepted as entirely legitimate and normal. The study recommends that gendered proverbs be consciously improved to portray gender neutrality, equality, and contemporariness. \n\u00a0",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "103003122129c600c43e23e5229f1b2ec5355e1d",
            "paperId": "103003122129c600c43e23e5229f1b2ec5355e1d",
            "title": "The sociolinguistic navigation of sexual normativities among same-gender-attracted men in contemporary Chengdu, China",
            "abstract": "In this article, the navigation of normativities by same-gender-attracted, male-identified individuals in Chengdu, China is considered with reference to their linguistic identity work. It foregrounds the relevance of plural normativities, which often arise from intersecting structural forces with diverse cultural roots, to better acknowledge the rich diversity of the broader sexual minority community in China. In turn, it argues that individual performances of sexual identity cannot be accurately accounted for using essentialist notions of culture. Furthermore, it is suggested that a sociocultural discourse analysis approach can provide an important and novel perspective on sexual identity in the region. Taking up this perspective questions and challenges assumptions of a characteristic orientation to normality in \u2018Chinese culture\u2019 and complicates oversimplified conceptualisations of normativity.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "0a47bc68400b7b357df2745ff6883135c4ce7745",
            "paperId": "0a47bc68400b7b357df2745ff6883135c4ce7745",
            "title": "The War of the Sexes Glossary: How Social Media Could Destroy American Marriage",
            "abstract": "We viewed highly influential social media channels on the internet regularly using non-participant observation as our method. We observed hundreds of short videos on YouTube and other platforms, as a sociocultural anthropologic field study, from January 2023 to January 2024. Given the well documented decline in marriage globally, we sought to understand common themes shared by social media content providers. Our non-participant observations led us to develop the War of the Sexes Glossary. Generation Z appears to have solidified a worldview that marriage is unnecessary. Emil Brunner\u2019s predictions from 1945 have come true, where he argued that \u201cIf the social basis, marriage, is rotten, the whole community is rotten.\u201d The Manosphere, in direct response to feminists\u2019 rhetoric, has fostered three main complaints prohibiting them from marriage commitment: \u201cNo Fault Divorce,\u201d \u201cPresumptive Paternity\u201d and \u201cMy Body, My Choice.\u201d We offer practicable solutions to policy makers on how to mitigate this dangerous worldview that, if not corrected, could destroy marriage in America.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "07bad6311bc83192ad64607e343aaa40fa0bc1bd",
            "paperId": "07bad6311bc83192ad64607e343aaa40fa0bc1bd",
            "title": "Queerness\u2014What Would the Queers Do?",
            "abstract": "In this essay, S\u00e9bastien Tremblay explores the potential of queerness as an analytical concept in the context of writing a global history of sexualities. It acknowledges the shift towards transnational and global perspectives in queer history, challenging Eurocentric concepts and promoting inclusivity of diverse sexual identities and experiences. Debuting his inquiry and analysis in Germany, Tremblay highlights the methodological inquiries arising from researching sexual entanglements without relying on fixed categories of analysis. He discusses how queer theory's focus on fluidity and ambiguity can aid historians in avoiding rigid concepts, historical norms, and eventually colonial biases. He therefore concludes by emphasising the importance of engaging with fluid concepts such as queerness in writing an open global history critical of Eurocentrism.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "57c43c62c5883e958bd74c4a642017b8c8473780",
            "paperId": "57c43c62c5883e958bd74c4a642017b8c8473780",
            "title": "Unveiling Masculinity: A Critical Analysis of Gender Representation in Men's Fragrance Advertisements",
            "abstract": "The aim of this study is to examine how male sexuality is visually portrayed in advertisements for men's perfume and to investigate the notions of masculinity, gender, and sexuality depicted therein. Through a visual text analysis approach rooted in social semiotic theory with literature study, the study examines 27 advertisements predominantly from Western countries (Specifically France and Italy), analyzing elements such as visual composition, gendered representations, and societal implications. The analysis of the sampled images reveals stereotypical representations of male and female sexuality, conveyed through the use of sexuality and gender concepts. For male consumers, exposure to these representations may lead to the internalization of these ideals, influencing their self-image and behavior. These representations not only maintain existing power differentials but also hinder progress towards gender equality and challenging societal norms.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "0d82d5add01e37f46501e3c78f2658eb633fbfa9",
            "paperId": "0d82d5add01e37f46501e3c78f2658eb633fbfa9",
            "title": "Parishioners\u2019 and Non-Parishioners\u2019 Perceptions of Priests: Homilies Informed by an Intergroup Perspective are Linked to More Positive Perceptions",
            "abstract": "Guided by the common ingroup identity model (CIIM; Gaertner & Dovidio, 2000; Gaertner, Dovidio, Anastasio, Bachman, & Rust, 1993), this study utilized a sample of undergraduate students (N = 175) who identified as Catholic or non-Catholic, and who were assigned randomly to one of the three homily conditions (i.e., \u201cyou\u201d language, \u201cwe Christians\u201d language, or \u201cwe everyone\u201d language). Consistent with predictions, the results indicated that, compared to non-Catholics, Catholics had significantly more positive perceptions of the priest, as did individuals\u2014Catholic and non-Catholic\u2014who read the \u201cwe Christians\u201d language homily as compared to those who read the \u201cyou\u201d language homily.",
            "year": 2017,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "ca734b2497a2beeca0be6d85a1664f5064feb3f1",
            "paperId": "ca734b2497a2beeca0be6d85a1664f5064feb3f1",
            "title": "\u201cO Feminismo Finalmente Venceu\u201d [Feminism Finally Won]: Misogynistic and Antifeminist Metapragmatics Disguised as Freedom of Expression",
            "abstract": "ABSTRACT In this research,1 we aim to analyze how misogyny establishes itself in an interaction on X (formerly Twitter) to legitimize hate speech, under the argument of opinion defense. Theoretically, in light of Interactional Sociolinguistics and Pragmatics, we conceive that impoliteness strategies contribute to the construction of linguistic-discursive violence scenarios in online-mediated interactions, as they both reduce interlocutive distance and generate sexist, misogynistic, patriarchal, and antifeminist metapragmatics. Methodologically, we adopted a qualitative approach to analyze an interaction on X based on the inclusion and exclusion criteria proposed in this study, using the netnographic method. Analytically, we observed that interlocutors C, E, and F frequently used impoliteness strategies in constructing opinion-based discourses to attack feminism and legitimize hate speech.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "ddea71bb0d42a0ec0c490cabf7c9e48850b7d610",
            "paperId": "ddea71bb0d42a0ec0c490cabf7c9e48850b7d610",
            "title": "Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation",
            "abstract": "Large Language Models (LLMs) have made significant strides in information acquisition. However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries. Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge. Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention. Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm. The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features. The concepts explicitly constrain LLMs to focus solely on vital information in the inference process. We conduct extensive experiments on open-domain question-answering datasets to empirically evaluate the proposed method's effectiveness. The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs. This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information. To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression.",
            "year": 2024,
            "citationCount": 2,
            "score": 1
        },
        {
            "id": "411114f989a3d1083d90afd265103132fee94ebe",
            "paperId": "411114f989a3d1083d90afd265103132fee94ebe",
            "title": "Mixtral of Experts",
            "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",
            "year": 2024,
            "citationCount": 269,
            "score": 1
        },
        {
            "id": "ea762e479a0c46ef0bb7d3f78a1da10db234053f",
            "paperId": "ea762e479a0c46ef0bb7d3f78a1da10db234053f",
            "title": "LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following",
            "abstract": "E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&A. These tasks enable the models to comprehensively understand precise e-commerce authoring knowledge by interleaving features covering typical service aspects of customers, sellers, and platforms. The GPT-3.5 is introduced as a teacher model, which expands the seed instructions to form a training set for the LLaMA-E models with various scales. The experimental results show that the proposed LLaMA-E models achieve state-of-the-art results in quantitative and qualitative evaluations, also exhibiting the advantage in zero-shot scenes. To the best of our knowledge, this study is the first to serve the LLMs to specific e-commerce authoring scenarios.",
            "year": 2023,
            "citationCount": 4,
            "score": 1
        },
        {
            "id": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "year": 2023,
            "citationCount": 4723,
            "score": 1
        },
        {
            "id": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report",
            "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
            "year": 2023,
            "citationCount": 4653,
            "score": 1
        },
        {
            "id": "88b62496cbc52072bfa8f4b29d172b0477b701bc",
            "paperId": "88b62496cbc52072bfa8f4b29d172b0477b701bc",
            "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
            "abstract": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.",
            "year": 2022,
            "citationCount": 156,
            "score": 1
        },
        {
            "id": "3d849136e0070f6d038dd96985ed67ead5aedb69",
            "paperId": "3d849136e0070f6d038dd96985ed67ead5aedb69",
            "title": "Locally Typical Sampling",
            "abstract": "Today\u2019s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process\u2014which allows for an information-theoretic analysis\u2014can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.",
            "year": 2022,
            "citationCount": 52,
            "score": 1
        }
    ]
}