{
    "topic_description": "novel prompting methods to improve large language models\u2019 performance on multilingual tasks or low-resource languages and vernacular languages",
    "all_queries": [
        "KeywordQuery(\"multilingual prompting low-resource languages\")",
        "PaperQuery(\"0b29ff236bb8f547d017bf747ad74ad2b8303851\")",
        "GetReferences(\"97992c13baa6185c03d9e672f53185bc59822596\")",
        "KeywordQuery(\"cross-lingual transfer learning language models\")",
        "PaperQuery(\"f5eb12858d20f2caf17aa924bc15a16425bc81b4\")",
        "KeywordQuery(\"multilingual language model alignment\")",
        "KeywordQuery(\"multilingual language model few-shot learning\")",
        "PaperQuery(\"1ea077aa7cfe8f7d1a13305b37d3cb831b5889dc\")"
    ],
    "paper_bank": [
        {
            "id": "97992c13baa6185c03d9e672f53185bc59822596",
            "paperId": "97992c13baa6185c03d9e672f53185bc59822596",
            "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
            "abstract": "Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method, CoD, is presented, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs, and indicates that augmenting ChatGPT with CoD elicits large gains for MNMT."
            },
            "score": 9
        },
        {
            "id": "c1f9b85ac8145808767a52954af8fb6d40fa7879",
            "paperId": "c1f9b85ac8145808767a52954af8fb6d40fa7879",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "abstract": "While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely\"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.",
            "year": 2024,
            "citationCount": 3,
            "score": 9
        },
        {
            "id": "0b29ff236bb8f547d017bf747ad74ad2b8303851",
            "paperId": "0b29ff236bb8f547d017bf747ad74ad2b8303851",
            "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
            "abstract": "Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages."
            },
            "score": 8
        },
        {
            "id": "9f171c208a3275b931d770ff424860f7b6ab1c0e",
            "paperId": "9f171c208a3275b931d770ff424860f7b6ab1c0e",
            "title": "The Power of Question Translation Training in Multilingual Reasoning: Broadened Scope and Deepened Insights",
            "abstract": "Bridging the significant gap between large language model's English and non-English performance presents a great challenge. While some previous studies attempt to mitigate this gap with translated training data, the recently proposed question alignment approach leverages the model's English expertise to improve multilingual performance with minimum usage of expensive, error-prone translation. In this paper, we explore how broadly this method can be applied by examining its effects in reasoning with executable code and reasoning with common sense. We also explore how to apply this approach efficiently to extremely large language models using proxy-tuning. Experiment results on multilingual reasoning benchmarks mGSM, mSVAMP and xCSQA demonstrate that the question alignment approach can be used to boost multilingual performance across diverse reasoning scenarios, model families, and sizes. For instance, when applied to the LLaMA2 models, our method brings an average accuracy improvements of 12.2% on mGSM even with the 70B model. To understand the mechanism of its success, we analyze representation space, chain-of-thought and translation data scales, which reveals how question translation training strengthens language alignment within LLMs and shapes their working patterns.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "year": 2023,
            "citationCount": 36,
            "score": 8
        },
        {
            "id": "df24843dcaa65d412e798114b9dc1e9db53d837c",
            "paperId": "df24843dcaa65d412e798114b9dc1e9db53d837c",
            "title": "Using natural language prompts for machine translation",
            "abstract": "We explore the use of natural language prompts for controlling various aspects of the outputs generated by machine translation models. We demonstrate that natural language prompts allow us to influence properties like formality or specific dialect of the output. We show that using language names to control the output language of multilingual translation models enables positive transfer for unseen language pairs. This unlocks the ability to translate into languages not seen during fine-tuning by using their English names. We investigate how scale, number of pre-training steps, number of languages in fine-tuning, and language similarity affect this phenomenon.",
            "year": 2022,
            "citationCount": 21,
            "score": 8
        },
        {
            "id": "f17945bb329439dc01e4aeacc6864fd1596698cc",
            "paperId": "f17945bb329439dc01e4aeacc6864fd1596698cc",
            "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions",
            "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "cb5cfc2dd4965262d2ce302362b1f2dbfa4a5419",
            "paperId": "cb5cfc2dd4965262d2ce302362b1f2dbfa4a5419",
            "title": "LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging",
            "abstract": "We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.",
            "year": 2022,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt, and is the first to demonstrate instruction fine- tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation"
            },
            "score": 8
        },
        {
            "id": "9fd4f8886f9c50fc13ab28a001e1f751042b148b",
            "paperId": "9fd4f8886f9c50fc13ab28a001e1f751042b148b",
            "title": "Edinburgh Research Explorer Prompting Large Language Model for Machine Translation: A Case Study",
            "abstract": "Research on prompting has shown it to have excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.",
            "year": null,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "0d2934b1ae60b763b8da43f080f6841fb10aed0b",
            "paperId": "0d2934b1ae60b763b8da43f080f6841fb10aed0b",
            "title": "ProMap: Effective Bilingual Lexicon Induction via Language Model Prompting",
            "abstract": "Bilingual Lexicon Induction (BLI), where words are translated between two languages, is an important NLP task. While noticeable progress on BLI in rich resource languages using static word embeddings has been achieved. The word translation performance can be further improved by incorporating information from contextualized word embeddings. In this paper, we introduce ProMap, a novel approach for BLI that leverages the power of prompting pretrained multilingual and multidialectal language models to address these challenges. To overcome the employment of subword tokens in these models, ProMap relies on an effective padded prompting of language models with a seed dictionary that achieves good performance when used independently. We also demonstrate the effectiveness of ProMap in re-ranking results from other BLI methods such as with aligned static word embeddings. When evaluated on both rich-resource and low-resource languages, ProMap consistently achieves state-of-the-art results. Furthermore, ProMap enables strong performance in few-shot scenarios (even with less than 10 training examples), making it a valuable tool for low-resource language translation. Overall, we believe our method offers both exciting and promising direction for BLI in general and low-resource languages in particular. ProMap code and data are available at \\url{https://github.com/4mekki4/promap}.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To overcome the employment of subword tokens in these models, ProMap relies on an effective padded prompting of language models with a seed dictionary that achieves good performance when used independently, and consistently achieves state-of-the-art results."
            },
            "score": 7
        },
        {
            "id": "1ea077aa7cfe8f7d1a13305b37d3cb831b5889dc",
            "paperId": "1ea077aa7cfe8f7d1a13305b37d3cb831b5889dc",
            "title": "Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator",
            "abstract": "Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines by remarkable improvements. MPT is more prominent compared with vanilla prompting when transferring to languages quite distinct from source language.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, Multilingual Prompt Translator (MPT), is proposed, where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge."
            },
            "score": 7
        },
        {
            "id": "f5eb12858d20f2caf17aa924bc15a16425bc81b4",
            "paperId": "f5eb12858d20f2caf17aa924bc15a16425bc81b4",
            "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback",
            "abstract": "To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "2dd7fbb2519676255c26615085a377fde82211fa",
            "paperId": "2dd7fbb2519676255c26615085a377fde82211fa",
            "title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language",
            "abstract": "Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "d5444633c7826af0dd149b4c9d367c191b4b4192",
            "paperId": "d5444633c7826af0dd149b4c9d367c191b4b4192",
            "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
            "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "293499319bdd460cb3fca1f0f5eb330e64bf3ff9",
            "paperId": "293499319bdd460cb3fca1f0f5eb330e64bf3ff9",
            "title": "Towards Making the Most of ChatGPT for Machine Translation",
            "abstract": "ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g., low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this paper, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose an optimal temperature setting and two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information can further improve ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.",
            "year": 2023,
            "citationCount": 115,
            "score": 7
        },
        {
            "id": "0704a0bd41cdb31def6ce8236466152d0917515f",
            "paperId": "0704a0bd41cdb31def6ce8236466152d0917515f",
            "title": "GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning",
            "abstract": "The emergence of Large Language Models (LLMs) with capabilities like In-Context Learning (ICL) has ushered in new possibilities for data generation across various domains while minimizing the need for extensive data collection and modeling techniques. Researchers have explored ways to use this generated synthetic data to optimize smaller student models for reduced deployment costs and lower latency in downstream tasks. However, ICL-generated data often suffers from low quality as the task specificity is limited with few examples used in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning approach, extending the WeakDAP framework, applied to a dataset generated through ICL with just one example in the target language using AlexaTM 20B Seq2Seq LLM. Through our approach, we iteratively identify high-quality data to enhance model performance, especially for low-resource multilingual setting in the context of Extractive Question Answering task. Our framework outperforms the machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points for Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it surpasses the performance of model trained on an English-only dataset by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the same dataset. Notably, our approach uses a pre-trained LLM for generation with no fine-tuning (FT), utilizing just a single annotated example in ICL to generate data, providing a cost-effective development process.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GeMQuAD is proposed - a semi-supervised learning approach, extending the WeakDAP framework, applied to a dataset generated through ICL with just one example in the target language using AlexaTM 20B Seq2Seq LLM, providing a cost-effective development process."
            },
            "score": 7
        },
        {
            "id": "8dd5f717fee66e0ac968ad485323315f1b3b9165",
            "paperId": "8dd5f717fee66e0ac968ad485323315f1b3b9165",
            "title": "Towards Robust In-Context Learning for Machine Translation with Large Language Models",
            "abstract": "Using large language models (LLMs) for machine translation via in-context learning (ICL) has become an interesting research direction of machine translation (MT) in recent years. Its main idea is to retrieve a few translation pairs as demonstrations from an additional datastore (parallel corpus) to guide translation without updating the LLMs. However, the underlying noise of retrieved demonstrations usually dramatically deteriorate the performance of LLMs. In this paper, we propose a robust method to enable LLMs to achieve robust translation with ICL. The method incorporates a multi-view approach, considering both sentence- and word-level information, to select demonstrations that effectively avoid noise. At the sentence level, a margin-based score is designed to avoid semantic noise. At the word level, word embeddings are utilized to evaluate the related tokens and change the weight of words in demonstrations. By considering both sentence- and word-level similarity, the proposed method provides fine-grained demonstrations that effectively prompt the translation of LLMs. Experimental results demonstrate the effectiveness of our method, particularly in domain adaptation.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "c0e62e324dfc7bd45ec0a5ce4055823d1f0c03f1",
            "paperId": "c0e62e324dfc7bd45ec0a5ce4055823d1f0c03f1",
            "title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval",
            "abstract": "There has been limited success for dense retrieval models in multilingual retrieval, due to uneven and scarce training data available across multiple languages. Synthetic training data generation is promising (e.g., InPars or Promptagator), but has been investigated only for English. Therefore, to study model capabilities across both cross-lingual and monolingual retrieval tasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high to very-low resource) languages for fine-tuning multilingual dense retrievers without requiring any human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting), where the large language model (LLM) generates a textual summary prior to the query generation step. SAP assists the LLM in generating informative queries in the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve (cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for expensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X models are available at https://github.com/google-research-datasets/SWIM-IR.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed SWIM-IR, a synthetic retrieval training dataset containing 33 languages for fine-tuning multilingual dense retrievers without requiring any human supervision, and its models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever-X."
            },
            "score": 6
        },
        {
            "id": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4",
            "paperId": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4",
            "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
            "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting are demonstrated and it is shown that the method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages."
            },
            "score": 6
        },
        {
            "id": "538bcc850b49aa0bdeb98d9cc095c3933e7e6e2e",
            "paperId": "538bcc850b49aa0bdeb98d9cc095c3933e7e6e2e",
            "title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages",
            "abstract": "Connectionist Temporal Classification (CTC) models are popular for their balance between speed and performance for Automatic Speech Recognition (ASR). However, these CTC models still struggle in other areas, such as personalization towards custom words. A recent approach explores Contextual Adapters, wherein an attention-based biasing model for CTC is used to improve the recognition of custom entities. While this approach works well with enough data, we showcase that it isn't an effective strategy for low-resource languages. In this work, we propose a supervision loss for smoother training of the Contextual Adapters. Further, we explore a multilingual strategy to improve performance with limited training data. Our method achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language. Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a supervision loss for smoother training of the Contextual Adapters, a multilingual strategy to improve performance with limited training data, and achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language."
            },
            "score": 6
        },
        {
            "id": "2ac5442a32988f86730e460b3198f475592ae410",
            "paperId": "2ac5442a32988f86730e460b3198f475592ae410",
            "title": "Improving Low-Resource Languages in Pre-Trained Multilingual Language Models",
            "abstract": "Pre-trained multilingual language models are the foundation of many NLP approaches, including cross-lingual transfer solutions. However, languages with small available monolingual corpora are often not well-supported by these models leading to poor performance. We propose an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models. We perform experiments on nine languages, using contextual word retrieval and zero-shot named entity recognition to measure both intrinsic cross-lingual word representation quality and downstream task performance, showing improvements on both tasks. Our results show that it is possible to improve pre-trained multilingual language models by relying only on non-parallel resources.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models."
            },
            "score": 6
        },
        {
            "id": "fcba7fcd30d9857fe43e86b155a23b2bbaf88b92",
            "paperId": "fcba7fcd30d9857fe43e86b155a23b2bbaf88b92",
            "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment",
            "abstract": "Multilingual proficiency presents a significant challenge for large language models (LLMs). English-centric models are usually suboptimal in other languages, particularly those that are linguistically distant from English. This performance discrepancy mainly stems from the imbalanced distribution of training data across languages during pre-training and instruction tuning stages. To address this problem, we propose a novel approach called CrossIn, which utilizes a mixed composition of cross-lingual instruction tuning data. Our method leverages the compressed representation shared by various languages to efficiently enhance the model's task-solving capabilities and multilingual proficiency within a single process. In addition, we introduce a multi-task and multi-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental results demonstrate that our method substantially improves performance across tasks and languages, and we provide extensive insights into the impact of cross-lingual data volume and the integration of translation data on enhancing multilingual consistency and accuracy.",
            "year": 2024,
            "citationCount": 1,
            "score": 6
        },
        {
            "id": "ac2d026bf968faafbc72a7a0b00d7cd6b6cee9fe",
            "paperId": "ac2d026bf968faafbc72a7a0b00d7cd6b6cee9fe",
            "title": "Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?",
            "abstract": "Recently, Large Language Models (LLMs) have shown impressive language capabilities. However, most of the existing LLMs are all English-centric, which have very unstable and unbalanced performance across different languages. Multilingual alignment is an effective method to enhance the LLMs' multilingual capabilities. In this work, we explore the multilingual alignment paradigm which utilizes translation data and comprehensively investigate the spontaneous multilingual improvement of LLMs. We find that LLMs only instruction-tuned on question translation data without annotated answers are able to get significant multilingual performance enhancement even across a wide range of languages unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to comprehensively analyze the LLM's performance in the multilingual scenario.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "c3fa1251d805217efc59ceb399c146b92450f4a3",
            "paperId": "c3fa1251d805217efc59ceb399c146b92450f4a3",
            "title": "mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models",
            "abstract": "Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model mCoT achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "b49d8ae7d9f9b0f52dbb57e1dd75ca13087c9d53",
            "paperId": "b49d8ae7d9f9b0f52dbb57e1dd75ca13087c9d53",
            "title": "SUTRA: Scalable Multilingual Language Model Architecture",
            "abstract": "In this paper, we introduce SUTRA, multilingual Large Language Model architecture capable of understanding, reasoning, and generating text in over 50 languages. SUTRA's design uniquely decouples core conceptual understanding from language-specific processing, which facilitates scalable and efficient multilingual alignment and learning. Employing a Mixture of Experts framework both in language and concept processing, SUTRA demonstrates both computational efficiency and responsiveness. Through extensive evaluations, SUTRA is demonstrated to surpass existing models like GPT-3.5, Llama2 by 20-30% on leading Massive Multitask Language Understanding (MMLU) benchmarks for multilingual tasks. SUTRA models are also online LLMs that can use knowledge from the internet to provide hallucination-free, factual and up-to-date responses while retaining their multilingual capabilities. Furthermore, we explore the broader implications of its architecture for the future of multilingual AI, highlighting its potential to democratize access to AI technology globally and to improve the equity and utility of AI in regions with predominantly non-English languages. Our findings suggest that SUTRA not only fills pivotal gaps in multilingual model capabilities but also establishes a new benchmark for operational efficiency and scalability in AI applications.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "14005fa355419392fb3d1e1a59b2cc89f41dac28",
            "paperId": "14005fa355419392fb3d1e1a59b2cc89f41dac28",
            "title": "FLOR: On the Effectiveness of Language Adaptation",
            "abstract": "Large language models have amply proven their great capabilities, both in downstream tasks and real-life settings. However, low- and mid-resource languages do not have access to the necessary means to train such models from scratch, and often have to rely on multilingual models despite being underrepresented in the training data. For the particular case of the Catalan language, we prove that continued pre-training with vocabulary adaptation is a better alternative to take the most out of already pre-trained models, even if these have not seen any Catalan data during their pre-training phase. We curate a 26B tokens corpus and use it to further pre-train BLOOM, giving rise to the FLOR models. We perform an extensive evaluation to assess the effectiveness of our method, obtaining consistent gains across Catalan and Spanish tasks. The models, training data, and evaluation framework are made freely available under permissive licenses.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "1cac4f365c3ee4f97fe5486fa0d7739d24d23f62",
            "paperId": "1cac4f365c3ee4f97fe5486fa0d7739d24d23f62",
            "title": "Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training",
            "abstract": "Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the speech domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual speech encoder, aiming to harness the capabilities of LLMs for speech recognition and beyond. Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the speech modality. Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM. While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style. Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including speech translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the speech domain.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
            "paperId": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
            "title": "In-context Examples Selection for Machine Translation",
            "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
            "year": 2022,
            "citationCount": 110,
            "score": 6
        },
        {
            "id": "197ba7bbfdbb052b0770088815c110774220f397",
            "paperId": "197ba7bbfdbb052b0770088815c110774220f397",
            "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
            "abstract": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM\u2019s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM\u2019s MT output which reveals some interesting properties and prospects for future work.",
            "year": 2022,
            "citationCount": 104,
            "score": 6
        },
        {
            "id": "12d1e5cdbe3753f3c3fc50c642043beb32e4f75d",
            "paperId": "12d1e5cdbe3753f3c3fc50c642043beb32e4f75d",
            "title": "ALIGN-MLM: Word Embedding Alignment is Crucial for Multilingual Pre-training",
            "abstract": "Multilingual pre-trained models exhibit zero-shot cross-lingual transfer, where a model fine-tuned on a source language achieves surprisingly good performance on a target language. While studies have attempted to understand transfer, they focus only on MLM, and the large number of differences between natural languages makes it hard to disentangle the importance of different properties. In this work, we specifically highlight the importance of word embedding alignment by proposing a pre-training objective (ALIGN-MLM) whose auxiliary loss guides similar words in different languages to have similar word embeddings. ALIGN-MLM either outperforms or matches three widely adopted objectives (MLM, XLM, DICT-MLM) when we evaluate transfer between pairs of natural languages and their counterparts created by systematically modifying specific properties like the script. In particular, ALIGN-MLM outperforms XLM and MLM by 35 and 30 F1 points on POS-tagging for transfer between languages that differ both in their script and word order (left-to-right v.s. right-to-left). We also show a strong correlation between alignment and transfer for all objectives (e.g., rho=0.727 for XNLI), which together with ALIGN-MLM's strong performance calls for explicitly aligning word embeddings for multilingual models.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a pre-training objective (ALIGN-MLM) whose auxiliary loss guides similar words in different languages to have similar word embeddings, and shows a strong correlation between alignment and transfer for all objectives."
            },
            "score": 6
        },
        {
            "id": "940fc621079ea349109202c7d705461b50d541d8",
            "paperId": "940fc621079ea349109202c7d705461b50d541d8",
            "title": "Cross-lingual Few-Shot Learning on Unseen Languages",
            "abstract": "Large pre-trained language models (LMs) have demonstrated the ability to obtain good performance on downstream tasks with limited examples in cross-lingual settings. However, this was mostly studied for relatively resource-rich languages, where at least enough unlabeled data is available to be included in pre-training a multilingual language model. In this paper, we explore the problem of cross-lingual transfer in unseen languages, where no unlabeled data is available for pre-training a model. We use a downstream sentiment analysis task across 12 languages, including 8 unseen languages, to analyze the effectiveness of several few-shot learning strategies across the three major types of model architectures and their learning dynamics. We also compare strategies for selecting languages for transfer and contrast findings across languages seen in pre-training compared to those that are not. Our findings contribute to the body of knowledge on cross-lingual models for low-resource settings that is paramount to increasing coverage, diversity, and equity in access to NLP technology. We show that, in few-shot learning, linguistically similar and geographically similar languages are useful for cross-lingual adaptation, but taking the context from a mixture of random source languages is surprisingly more effective. We also compare different model architectures and show that the encoder-only model, XLM-R, gives the best downstream task performance.",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses a downstream sentiment analysis task across 12 languages, including 8 unseen languages, to analyze the effectiveness of several few-shot learning strategies across the three major types of model architectures and their learning dynamics and shows that taking the context from a mixture of random source languages is surprisingly more effective."
            },
            "score": 6
        },
        {
            "id": "73853e7363879ac279bda41f8d6e5e67e664c7b9",
            "paperId": "73853e7363879ac279bda41f8d6e5e67e664c7b9",
            "title": "\"Diversity and Uncertainty in Moderation\" are the Key to Data Selection for Multilingual Few-shot Transfer",
            "abstract": "Few-shot transfer often shows substantial gain over zero-shot transfer~\\cite{lauscher2020zero}, which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretrained model-based systems. This paper explores various strategies for selecting data for annotation that can result in a better few-shot transfer. The proposed approaches rely on multiple measures such as data entropy using $n$-gram language model, predictive entropy, and gradient embedding. We propose a loss embedding method for sequence labeling tasks, which induces diversity and uncertainty sampling similar to gradient embedding. The proposed data selection strategies are evaluated and compared for POS tagging, NER, and NLI tasks for up to 20 languages. Our experiments show that the gradient and loss embedding-based strategies consistently outperform random data selection baselines, with gains varying with the initial performance of the zero-shot transfer. Furthermore, the proposed method shows similar trends in improvement even when the model is fine-tuned using a lower proportion of the original task-specific labeled training data for zero-shot transfer.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A loss embedding method is proposed for sequence labeling tasks, which induces diversity and uncertainty sampling similar to gradient embedding, and shows similar trends in improvement even when the model is fine-tuned using a lower proportion of the original task-specific labeled training data for zero-shot transfer."
            },
            "score": 6
        },
        {
            "id": "f5ca432a882efc538e14e624ef831f2bab07fdbb",
            "paperId": "f5ca432a882efc538e14e624ef831f2bab07fdbb",
            "title": "Does Meta-learning Help mBERT for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?",
            "abstract": "Few-shot Question Generation (QG) is an important and challenging problem in the Natural Language Generation (NLG) domain. Multilingual BERT (mBERT) has been successfully used in various Natural Language Understanding (NLU) applications. However, the question of how to utilize mBERT for few-shot QG, possibly with cross-lingual transfer, remains. In this paper, we try to explore how mBERT performs in few-shot QG (cross-lingual transfer) and also whether applying meta-learning on mBERT further improves the results. In our setting, we consider mBERT as the base model and fine-tune it using a seq-to-seq language modeling framework in a cross-lingual setting. Further, we apply the model agnostic meta-learning approach to our base model. We evaluate our model for two low-resource Indian languages, Bengali and Telugu, using the TyDi QA dataset. The proposed approach consistently improves the performance of the base model in few-shot settings and even works better than some heavily parameterized models. Human evaluation also confirms the effectiveness of our approach.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper considers mBERT as the base model and fine-tune it using a seq-to-seq language modeling framework in a cross-lingual setting and applies the model agnostic meta-learning approach to its base model."
            },
            "score": 6
        },
        {
            "id": "4e1a4d6804c7983c659feb7e41d49ad8c21aaa43",
            "paperId": "4e1a4d6804c7983c659feb7e41d49ad8c21aaa43",
            "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
            "abstract": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs, is introduced to encourage research in developing more effective multilingual text simplification models and evaluation metrics."
            },
            "score": 5
        },
        {
            "id": "7673114da5d82381cf8e75408089c98f73fad06d",
            "paperId": "7673114da5d82381cf8e75408089c98f73fad06d",
            "title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds",
            "abstract": "The advent of Multilingual Language Models (MLLMs) and Large Language Models (LLMs) has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both fine-tuning MLLMs and managing the complete MT workflow for low-resources languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM, which streamlines all processes involved in the fine-tuning of MLLMs for MT. This open-source application is tailored for developers, translators, and users who are engaged in MT. It is particularly useful for newcomers to the field, as it significantly streamlines the configuration of the development environment. An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service directly within the application. As a multilingual tool, we used adaptMLLM to fine-tune models for two low-resource language pairs: English to Irish (EN\u2194 GA) and English to Marathi (EN\u2194MR). Compared with baselines from the LoResMT2021 Shared Task, the adaptMLLM system demonstrated significant improvements. In the EN\u2192GA direction, an improvement of 5.2 BLEU points was observed and an increase of 40.5 BLEU points was recorded in the GA\u2192EN direction representing relative improvements of 14% and 117%, respectively. Significant improvements in the translation performance of the EN\u2194MR pair were also observed notably in the MR\u2192EN direction with an increase of 21.3 BLEU points which corresponds to a relative improvement of 68%. Finally, a fine-grained human evaluation of the MLLM output on the EN\u2192GA pair was conducted using the Multidimensional Quality Metrics and Scalar Quality Metrics error taxonomies. The application and models are freely available.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The development of adaptMLLM is developed, which streamlines all processes involved in the fine-tuning of MLLMs for MT and is particularly useful for newcomers to the field, as it significantly streamlines the configuration of the development environment."
            },
            "score": 5
        },
        {
            "id": "6d1f1caed7d43fc76ec7ea7bd555026d8c477e58",
            "paperId": "6d1f1caed7d43fc76ec7ea7bd555026d8c477e58",
            "title": "Negative Stances Detection from Multilingual Data Streams in Low-resource Languages on Social Media using BERT and CNN based Transfer Learning Model",
            "abstract": "Online social media allows users to connect with a large number of people across the globe and facilitate the exchange of information efficiently. These platforms cater to many of our day-to-day needs. However, at the same time, social media have been increasingly used to transmit negative stances like derogatory language, hate speech, and cyberbullying. The task of identifying the negative stances from social media posts or comments, or tweets is termed negative stance detection. One of the major challenges associated with negative stance detection is that most of the content published on social media is often in a multi-lingual format. This work aims to identify negative stances from multilingual data streams in low-resource languages on social media using a hybrid transfer learning and deep convolutional neural network approach. The proposed work strats by preprocessing the multi-lingual datasets by removing irrelevant information like special characters, hyperlinks, etc. The processed dataset is then passed through a pre-trained BERT (bidirectional encoder representations from transformers) model to generate embeddings by fine-tuning the model as per the dataset under consideration. The generated word embeddings are then passed to a deep convolutional neural network for extracting the latent features from the texts and removing the unessential information. This helps our model to achieve robustness and effectiveness for efficient learning on the given dataset and make appropriate predictions on zero-shot data. The paper utilizes several optimization strategies for examining the impact of fine-tuning different BERT layers on the model\u2019s performance. ntensive experiments on a variety of languages, namely, English, French, Italian, Danish, Arabic, Spanish, Indonesian, German, and Portuguese are performed. The experimental results demonstrate the effectiveness and efficiency of the proposed framework.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to identify negative stances from multilingual data streams in low-resource languages on social media using a hybrid transfer learning and deep convolutional neural network approach and demonstrates the effectiveness and efficiency of the proposed framework."
            },
            "score": 5
        },
        {
            "id": "83608aadb17443beba22c4087030c8704ccd7c64",
            "paperId": "83608aadb17443beba22c4087030c8704ccd7c64",
            "title": "SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages",
            "abstract": "In recent years, multilingual machine translation models have achieved promising performance on low-resource language pairs by sharing information between similar languages, thus enabling zero-shot translation. To overcome the \u201ccurse of multilinguality\u201d, these models often opt for scaling up the number of parameters, which makes their use in resource-constrained environments challenging. We introduce SMaLL-100, a distilled version of the M2M-100(12B) model, a massively multilingual machine translation model covering 100 languages. We train SMaLL-100 with uniform sampling across all language pairs and therefore focus on preserving the performance of low-resource languages. We evaluate SMaLL-100 on different low-resource benchmarks: FLORES-101, Tatoeba, and TICO-19 and demonstrate that it outperforms previous massively multilingual models of comparable sizes (200-600M) while improving inference latency and memory usage. Additionally, our model achieves comparable results to M2M-100 (1.2B), while being 3.6x smaller and 4.3x faster at inference.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces SMaLL-100, a distilled version of the M2M-100(12B) model, a massively multilingual machine translation model covering 100 languages and demonstrates that it outperforms previous massively mult bilingual models of comparable sizes while improving inference latency and memory usage."
            },
            "score": 5
        },
        {
            "id": "9a66a6b97601f207ac5e640122fce6ed407f494f",
            "paperId": "9a66a6b97601f207ac5e640122fce6ed407f494f",
            "title": "Quantifying Multilingual Performance of Large Language Models Across Languages",
            "abstract": "The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "7d61b2dc1893638e0846263489c940496c01a89d",
            "paperId": "7d61b2dc1893638e0846263489c940496c01a89d",
            "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
            "abstract": "As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "a43892e936da8cd3a2ed07091447c53eeed30945",
            "paperId": "a43892e936da8cd3a2ed07091447c53eeed30945",
            "title": "Tagengo: A Multilingual Chat Dataset",
            "abstract": "Open source large language models (LLMs) have shown great improvements in recent times. However, many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state-of-the-art open source English LLM to chat multilingually. We evaluate our model on MT-Bench chat benchmarks in 6 languages, finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "63179e3321c29b455b16fdbfdf59017113b66071",
            "paperId": "63179e3321c29b455b16fdbfdf59017113b66071",
            "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning",
            "abstract": "Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model\u2019s cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings. We also conduct studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that is created by translating the English-only Schema-Guided Dialogue (SGD) dataset into 105 other languages and develops an efficient prompt-tuning-based method for learning alignment prompts."
            },
            "score": 5
        },
        {
            "id": "6942bde24d01c412bdd53414cc88459afa5fac7d",
            "paperId": "6942bde24d01c412bdd53414cc88459afa5fac7d",
            "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions",
            "abstract": "Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs\u2019 ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions and shows that multilingual LLMs have stronger translation abilities than previously demonstrated."
            },
            "score": 5
        },
        {
            "id": "cc3593bcfc002b0d19516129bc3d65859c88af65",
            "paperId": "cc3593bcfc002b0d19516129bc3d65859c88af65",
            "title": "Few-shot learning for automated content analysis: Efficient coding of arguments and claims in the debate on arms deliveries to Ukraine",
            "abstract": "Pre-trained language models (PLM) based on transformer neural networks developed in the field of natural language processing (NLP) offer great opportunities to improve automatic content analysis in communication science, especially for the coding of complex semantic categories in large datasets via supervised machine learning. However, three characteristics so far impeded the widespread adoption of the methods in the applying disciplines: the dominance of English language models in NLP research, the necessary computing resources, and the effort required to produce training data to fine-tune PLMs. In this study, we address these challenges by using a multilingual transformer model in combination with the adapter extension to transformers, and few-shot learning methods. We test our approach on a realistic use case from communication science to automatically detect claims and arguments together with their stance in the German news debate on arms deliveries to Ukraine. In three experiments, we evaluate (1) data preprocessing strategies and model variants for this task, (2) the performance of different few-shot learning methods, and (3) how well the best setup performs on varying training set sizes in terms of validity, reliability, replicability and reproducibility of the results. We find that our proposed combination of transformer adapters with pattern exploiting training provides a parameter-efficient and easily shareable alternative to fully fine-tuning PLMs. It performs on par in terms of validity, while overall, provides better properties for application in communication studies. The results also show that pre-fine-tuning for a task on a near-domain dataset leads to substantial improvement, in particular in the few-shot setting. Further, the results indicate that it is useful to bias the dataset away from the viewpoints of specific prominent individuals.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multilingual transformer model in combination with the adapter extension to transformers, and few-shot learning methods provides a parameter-efficient and easily shareable alternative to fully fine-tuning PLMs and provides better properties for application in communication studies."
            },
            "score": 5
        },
        {
            "id": "8ee77f496e74ebc92161629cbd7de58ac9508a33",
            "paperId": "8ee77f496e74ebc92161629cbd7de58ac9508a33",
            "title": "LLMs for Low Resource Languages in Multilingual, Multimodal and Dialectal Settings",
            "abstract": "The recent breakthroughs in Artificial Intelligence (AI) can be attributed to the remarkable performance of Large Language Models (LLMs) across a spectrum of research areas (e.g., machine translation, question-answering, automatic speech recognition, text-to-speech generation) and application domains (e.g., business, law, healthcare, education, and psychology). The success of these LLMs largely de- pends on specific training techniques, most notably instruction tuning, RLHF, and subsequent prompting to achieve the desired output. As the development of such LLMs continues to increase in both closed and open settings, evaluation has become crucial for understanding their generalization capabilities across different tasks, modalities, languages, and dialects. This evaluation process is tightly coupled with prompting, which plays a key role in obtain- ing better outputs. There has been attempts to evaluate such models focusing on diverse tasks, languages, and dialects, which suggests that the capabilities of LLMs are still limited to medium-to-low-resource languages due to the lack of representative datasets. The tutorial offers an overview of this emerging research area. We explore the capabilities of LLMs in terms of their performance, zero- and few-shot settings, fine-tuning, instructions tuning, and close vs. open models with a special emphasis on low-resource settings. In addition to LLMs for standard NLP tasks, we will focus on speech and multimodality.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The capabilities of LLMs in terms of their performance, zero- and few-shot settings, fine-tuning, instructions tuning, and close vs. open models with a special emphasis on low-resource settings are explored."
            },
            "score": 4
        },
        {
            "id": "a517575328ca3b8289fa95bd9f71669e1cf7127a",
            "paperId": "a517575328ca3b8289fa95bd9f71669e1cf7127a",
            "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
            "abstract": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that languages unseen during the pre-training of multilingual language models, languages from under-represented families, and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on the authors' topic classification dataset."
            },
            "score": 4
        },
        {
            "id": "b1e08e85d6f268b45c003bb4a9dec5b6d1d3f7e7",
            "paperId": "b1e08e85d6f268b45c003bb4a9dec5b6d1d3f7e7",
            "title": "Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs",
            "abstract": "In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. We then propose simple and effective methods to build multilingual graphs from the colexification patterns: ColexNet and ColexNet+. ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+, concept nodes are additionally linked through intermediate nodes, each representing an ngram in one of 1,334 languages. We use ColexNet+ to train $\\overrightarrow{\\mbox{ColexNet+}}$, high-quality multilingual embeddings that are well-suited for transfer learning. In our experiments, we first show that ColexNet achieves high recall on CLICS, a dataset of crosslingual colexifications. We then evaluate $\\overrightarrow{\\mbox{ColexNet+}}$ on roundtrip translation, sentence retrieval and sentence classification and show that our embeddings surpass several transfer learning baselines. This demonstrates the benefits of using colexification as a source of information in multilingual NLP.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The benefits of using colexification as a source of information in multilingual NLP are demonstrated by using ColexNet+ to train high-quality multilingual embeddings that are well-suited for transfer learning."
            },
            "score": 4
        },
        {
            "id": "cbb16cc87b033ed7494902a0891826b83fa9807c",
            "paperId": "cbb16cc87b033ed7494902a0891826b83fa9807c",
            "title": "CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages",
            "abstract": "This work introduces CAPIVARA, a cost-efficient framework designed to enhance the performance of multilingual CLIP models in low-resource languages. While CLIP has excelled in zero-shot vision-language tasks, the resource-intensive nature of model training remains challenging. Many datasets lack linguistic diversity, featuring solely English descriptions for images. CAPIVARA addresses this by augmenting text data using image captioning and machine translation to generate multiple synthetic captions in low-resource languages. We optimize the training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the computational cost. Through extensive experiments, CAPIVARA emerges as state of the art in zero-shot tasks involving images and Portuguese texts. We show the potential for significant improvements in other low-resource languages, achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a single GPU for 2 hours. Our model and code is available at https://github.com/hiaac-nlp/CAPIVARA.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CAPIVARA emerges as state of the art in zero-shot tasks involving images and Portuguese texts and shows the potential for significant improvements in other low-resource languages, achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a single GPU for 2 hours."
            },
            "score": 4
        },
        {
            "id": "7bdbce7e6de063936c60be5afc71c30f513fc8c0",
            "paperId": "7bdbce7e6de063936c60be5afc71c30f513fc8c0",
            "title": "UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource Languages",
            "abstract": "Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment Analysis for African Languages, provides insight into how a multilingual large language model can be a resource for sentiment analysis in languages not seen during pretraining. The shared task provides datasets of a variety of African languages from different language families. The languages are to various degrees related to languages used during pretraining, and the language data contain various degrees of code-switching. We experiment with both monolingual and multilingual datasets for the final fine-tuning, and find that with the provided datasets that contain samples in the thousands, monolingual fine-tuning yields the best results.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "022b5b58058e5af5745f4d783f64bb77b38ffdda",
            "paperId": "022b5b58058e5af5745f4d783f64bb77b38ffdda",
            "title": "Evaluating and Mitigating Linguistic Discrimination in Large Language Models",
            "abstract": "By training on text in various languages, large language models (LLMs) typically possess multilingual support and demonstrate remarkable capabilities in solving tasks described in different languages. However, LLMs can exhibit linguistic discrimination due to the uneven distribution of training data across languages. That is, LLMs are hard to keep the consistency of responses when faced with the same task but depicted in different languages. In this study, we first explore the consistency in the LLMs' outputs responding to queries in various languages from two aspects: safety and quality. We conduct this analysis with two datasets (AdvBench and NQ) based on four LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro). The results show that LLMs exhibit stronger human alignment capabilities with queries in English, French, Russian, and Spanish (only 1.04\\% of harmful queries successfully jailbreak on average) compared to queries in Bengali, Georgian, Nepali and Maithili (27.7\\% of harmful queries jailbreak successfully on average). Moreover, for queries in English, Danish, Czech and Slovenian, LLMs tend to produce responses with a higher quality (with 0.1494 $F_1$ score on average) compared to the other languages. Upon these findings, we propose LDFighter, a similarity-based voting, to mitigate the linguistic discrimination in LLMs. LDFighter ensures consistent service for different language speakers. We evaluate LDFighter with both benign queries and harmful queries. The results show that LDFighter not only significantly reduces the jailbreak success rate but also improve the response quality on average, demonstrating its effectiveness.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "65d9cc9350455eb893bfdad3d1c1bb967291a7da",
            "paperId": "65d9cc9350455eb893bfdad3d1c1bb967291a7da",
            "title": "Multilingual Generation in Abstractive Summarization: A Comparative Study",
            "abstract": "The emergence of pre-trained models marks a significant juncture for the multilingual generation, offering unprecedented capabilities to comprehend and produce text across multiple languages. These models display commendable efficiency in high-resource languages. However, their performance notably falters in low-resource languages due to the extensive linguistic diversity encountered. Moreover, the existing works lack thorough analysis impairs the discovery of effective multilingual strategies, further complicating the advancement of current multilingual generation systems. This paper aims to appraise the efficacy of multilingual generation tasks, with a focus on summarization, through three resource availability scenarios: high-resource, low-resource, and zero-shot. We classify multilingual generation methodologies into three foundational categories based on their underlying modeling principles: Fine-tuning, Parameter-isolation, and Constraint-based approaches. Following this classification, we conduct a comprehensive comparative study of these methodologies across different resource contexts using two datasets that span six languages. This analysis provides insights into the unique advantages and limitations of each method. In addition, we introduce an innovative yet simple automatic metric LANGM designed to mitigate the prevalent problem of spurious correlations associated with language mixing. LANGM accurately measures the degree of code-mixing at the language level. Finally, we highlight several challenges and suggest potential avenues for future inquiry, aiming to spur further advancements within the field of multilingual text generation.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "47cda129fc742647739591351fb3f40676018cf6",
            "paperId": "47cda129fc742647739591351fb3f40676018cf6",
            "title": "Tele-FLM Technical Report",
            "abstract": "Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications. However, there is a notable paucity of detailed, open-sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial-and-error cost and computational resources. In this report, we introduce Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that features a stable, efficient pre-training paradigm and enhanced factual judgment capabilities. Tele-FLM demonstrates superior multilingual language modeling abilities, measured by BPB on textual corpus. Besides, in both English and Chinese foundation model evaluation, it is comparable to strong open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B and DeepSeek-67B. In addition to the model weights, we share the core designs, engineering practices, and training details, which we expect to benefit both the academic and industrial communities.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "dfd8944d39b378489b878d6e105d040fa0e524db",
            "paperId": "dfd8944d39b378489b878d6e105d040fa0e524db",
            "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
            "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually improving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, instruction semantics can surprisingly be ignored when given in-context exemplars. Second, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Third, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages.",
            "year": 2023,
            "citationCount": 60,
            "score": 4
        },
        {
            "id": "ac3cdb50606f7770eef8e4cd951840a4f71287a0",
            "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0",
            "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
            "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.",
            "year": 2021,
            "citationCount": 522,
            "score": 4
        },
        {
            "id": "79dc4eea8557862015fe5edc58b700e1637c0fb1",
            "paperId": "79dc4eea8557862015fe5edc58b700e1637c0fb1",
            "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
            "abstract": "Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.",
            "year": 2024,
            "citationCount": 2,
            "score": 4
        },
        {
            "id": "4a719447c25e4a3ed0620df9fcdfbb9285617f50",
            "paperId": "4a719447c25e4a3ed0620df9fcdfbb9285617f50",
            "title": "Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers",
            "abstract": "Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer. We provide evidence that alignment is actually significantly correlated with cross-lingual transfer across languages, models and random seeds. We show that fine-tuning can have a significant impact on alignment, depending mainly on the downstream task and the model. Finally, we show that realignment can, in some instances, improve cross-lingual transfer, and we identify conditions in which realignment methods provide significant improvements. Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. For example, for POS-tagging, between English and Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even outperforming XLM-R Large by 1.7. We thus advocate for further research on realignment methods for smaller multilingual models as an alternative to scaling.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs."
            },
            "score": 4
        },
        {
            "id": "745cde3c527d834124ed9c219135285be907b83f",
            "paperId": "745cde3c527d834124ed9c219135285be907b83f",
            "title": "Multilingual Few-Shot Learning via Language Model Retrieval",
            "abstract": "Transformer-based language models have achieved remarkable success in few-shot in-context learning and drawn a lot of research interest. However, these models' performance greatly depends on the choice of the example prompts and also has high variability depending on how samples are chosen. In this paper, we conduct a comprehensive study of retrieving semantically similar few-shot samples and using them as the context, as it helps the model decide the correct label without any gradient update in the multilingual and cross-lingual settings. We evaluate the proposed method on five natural language understanding datasets related to intent detection, question classification, sentiment analysis, and topic classification. The proposed method consistently outperforms random sampling in monolingual and cross-lingual tasks in non-English languages.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper conducts a comprehensive study of retrieving semantically similar few-shot samples and using them as the context, as it helps the model decide the correct label without any gradient update in the multilingual and cross-lingual settings."
            },
            "score": 4
        },
        {
            "id": "914254fac74a2da051cccf6ca16afcaad416a079",
            "paperId": "914254fac74a2da051cccf6ca16afcaad416a079",
            "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
            "year": 2022,
            "citationCount": 67,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling tasks, are more efficient few-shot learners than decoder-only models on various tasks."
            },
            "score": 4
        },
        {
            "id": "210980149a6b41d3e8d95c12daa41d6aa391681f",
            "paperId": "210980149a6b41d3e8d95c12daa41d6aa391681f",
            "title": "Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
            "abstract": "Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work builds upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem, and identifies a common set of features that influence zero- shot performance across a variety of tasks."
            },
            "score": 4
        },
        {
            "id": "add0dd0ab2a408354fa2d89cc492b4d2ab26845d",
            "paperId": "add0dd0ab2a408354fa2d89cc492b4d2ab26845d",
            "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
            "abstract": "Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining. However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018) to 10 Indigenous languages of the Americas. We conduct experiments with XLM-R, testing multiple zero-shot and translation-based approaches. Additionally, we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models. We find that XLM-R\u2019s zero-shot performance is poor for all 10 languages, with an average performance of 38.48%. Continued pretraining offers improvements, with an average accuracy of 43.85%. Surprisingly, training on poorly translated data by far outperforms all other methods with an accuracy of 49.12%.",
            "year": 2021,
            "citationCount": 68,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "384e058c262e7f29b50cc4feeb0bd731f6e5377e",
            "paperId": "384e058c262e7f29b50cc4feeb0bd731f6e5377e",
            "title": "Is ChatGPT A Good Translator? A Preliminary Study",
            "abstract": "This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and \ufb01nd that the candidate prompts generally work well and show minor performance differences. By evaluating on a number of benchmark test sets 1 , we \ufb01nd that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind signi\ufb01cantly on low-resource or distant languages. For distant languages, we explore an interesting strategy named pivot prompting that asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, which improves the translation performance signi\ufb01cantly. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but is potentially a good translator for spoken language.",
            "year": 2023,
            "citationCount": 260,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT performs competitively with commercial translation products on high-resource European languages but lags behind signi\ufb01cantly on low-resource or distant languages and translation robustness."
            },
            "score": 3
        },
        {
            "id": "780c99d13537370f63c03feeb1343bed9d98a4f9",
            "paperId": "780c99d13537370f63c03feeb1343bed9d98a4f9",
            "title": "Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine",
            "abstract": "This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further, we explore an interesting strategy named $\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words, ChatGPT has already become a good translator. Please refer to our Github project for more details: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator",
            "year": 2023,
            "citationCount": 168,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting strategy named $\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably."
            },
            "score": 3
        },
        {
            "id": "7e922d0ab437db455c78dcb5e4ad63375020a4bf",
            "paperId": "7e922d0ab437db455c78dcb5e4ad63375020a4bf",
            "title": "Less is Enough: Less-Resourced Multilingual AMR Parsing",
            "abstract": "This paper investigates the efficacy of multilingual models for the task of text-to-AMR parsing, focusing on English, Spanish, and Dutch. We train and evaluate models under various configurations, including monolingual and multilingual settings, both in full and reduced data scenarios. Our empirical results reveal that while monolingual models exhibit superior performance, multilingual models are competitive across all languages, offering a more resource-efficient alternative for training and deployment. Crucially, our findings demonstrate that AMR parsing benefits from transfer learning across languages even when having access to significantly smaller datasets. As a tangible contribution, we provide text-to-AMR parsing models for the aforementioned languages as well as multilingual variants, and make available the large corpora of translated data for Dutch, Spanish (and Irish) that we used for training them in order to foster AMR research in non-English languages. Additionally, we open-source the training code and offer an interactive interface for parsing AMR graphs from text.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "dee6fc87aa9be963a528c6a57340d88cc571e9d0",
            "paperId": "dee6fc87aa9be963a528c6a57340d88cc571e9d0",
            "title": "Evaluating Code-Switching Translation with Large Language Models",
            "abstract": "Recent advances in large language models (LLMs) have shown they can match or surpass finetuned models on many natural language processing tasks. Currently, more studies are being carried out to assess whether this performance carries over across different languages. In this paper, we present a thorough evaluation of LLMs for the less well-researched code-switching translation setting, where inputs include a mixture of different languages. We benchmark the performance of six state-of-the-art LLMs across seven datasets, with GPT-4 and GPT-3.5 displaying strong ability relative to supervised translation models and commercial engines. GPT-4 was also found to be particularly robust against different code-switching conditions. Several methods to further improve code-switching translation are proposed including leveraging in-context learning and pivot translation. Through our code-switching experiments, we argue that LLMs show promising ability for cross-lingual understanding.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "845877140a313b5ed0421609a33873512956f2c0",
            "paperId": "845877140a313b5ed0421609a33873512956f2c0",
            "title": "Evaluating Large Language Models: ChatGPT-4, Mistral 8x7B, and Google Gemini Benchmarked Against MMLU",
            "abstract": "This study was designed to explore the capabilities of contemporary large language models (LLMs) \u2014 specifically, ChatGPT-4, Google Gemini, and Mistral 8x7B \u2014 in processing and generating text across di ff erent languages, with a focused comparison on English and Japanese. By employing a rigorous benchmarking methodology anchored in the Massive Multitask Language Understanding (MMLU) framework, we sought to quantitatively assess the performance of these models in a variety of linguistic tasks designed to challenge their understanding, reasoning, and language generation capabilities. Our methodology encompassed a diverse range of tests, from simple grammatical assessments to complex reasoning and comprehension challenges, enabling a comprehensive evaluation of each model\u2019s linguistic proficiency and adaptability. The key finding of our investigation reveals significant disparities in language performance among the evaluated LLMs, with ChatGPT-4 demonstrating superior proficiency in English, Google Gemini excelling in Japanese, and Mistral 8x7B showcasing a balanced performance across both languages. These results highlight the influence of training data diversity, model architecture, and linguistic focus in shaping the abilities of LLMs to understand and generate human language. Furthermore, our study underscores the critical need for incorporating a more diverse and inclusive range of linguistic data in the training processes of future LLMs. We advocate for the advancement of language technologies that are capable of bridging linguistic gaps, enhancing cross-cultural communication, and fostering a more equitable digital landscape for users worldwide.",
            "year": null,
            "citationCount": 1,
            "score": 3
        },
        {
            "id": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
            "paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
            "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
            "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
            "year": 2022,
            "citationCount": 167,
            "score": 3
        },
        {
            "id": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904",
            "paperId": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904",
            "title": "Few-shot Learning with Multilingual Generative Language Models",
            "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
            "year": 2021,
            "citationCount": 179,
            "score": 3
        },
        {
            "id": "a992a1ff27c1bd04a964bf7ed82c6db19fd7f671",
            "paperId": "a992a1ff27c1bd04a964bf7ed82c6db19fd7f671",
            "title": "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?",
            "abstract": "Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of all these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 training instances, and that fine-tuning on a single translation direction effectively enables LLMs to translate in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with English on the target side can lead to task misinterpretation, which hinders translations into non-English languages. A similar problem arises when noise is introduced into the target side of parallel data, especially when the target language is well-represented in the LLM's pre-training. In contrast, noise in an under-represented language has a less pronounced effect. Our findings suggest that attaining successful alignment hinges on teaching the model to maintain a\"superficial\"focus, thereby avoiding the learning of erroneous biases beyond translation.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "857dc6795ac3c81453922c6c3037b11bad200f55",
            "paperId": "857dc6795ac3c81453922c6c3037b11bad200f55",
            "title": "Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks",
            "abstract": "Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks. However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity. In addition these benchmarks lack quality assessment against the respective state-of the art models. This study presents an in-depth examination of prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B, across 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analysed. Our experiments show that SOTA models surpass all the encoder-decoder pre-trained language models in all Urdu NLP tasks with zero-shot learning. Our results further show that LLMs with fewer parameters, but more language specific data in the base model perform better than larger computational models, but low language data.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "63d717b49e53da1e57e99db1cba73a84ea17280c",
            "paperId": "63d717b49e53da1e57e99db1cba73a84ea17280c",
            "title": "Every word counts: A multilingual analysis of individual human alignment with model attention",
            "abstract": "Human fixation patterns have been shown to correlate strongly with Transformer-based attention. Those correlation analyses are usually carried out without taking into account individual differences between participants and are mostly done on monolingual datasets making it difficult to generalise findings. In this paper, we analyse eye-tracking data from speakers of 13 different languages reading both in their native language (L1) and in English as language learners (L2). We find considerable differences between languages but also that individual reading behaviour such as skipping rate, total reading time and vocabulary knowledge (LexTALE) influence the alignment between humans and models to an extent that should be considered in future studies.",
            "year": 2022,
            "citationCount": 7,
            "tldr": null,
            "score": 3
        },
        {
            "id": "1403e6b9adf7712c35ae56327d52fe54603b87e1",
            "paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1",
            "title": "Few-shot Learning with Multilingual Language Models",
            "abstract": "Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few-and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model suc-ceeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in 5 languages and find it has limitations similar to comparably sized GPT-3 models.",
            "year": 2021,
            "citationCount": 143,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed analysis of where the model succeeds and fails is presented, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form."
            },
            "score": 3
        },
        {
            "id": "3b16a709a5b18e52b0b6741cbc3c0e68a03ecd8e",
            "paperId": "3b16a709a5b18e52b0b6741cbc3c0e68a03ecd8e",
            "title": "The unreasonable effectiveness of few-shot learning for machine translation",
            "abstract": "We demonstrate the potential of few-shot translation systems, trained with unpaired language data, for both high and low-resource language pairs. We show that with only 5 examples of high-quality translation data shown at inference, a transformer decoder-only model trained solely with self-supervised learning, is able to match specialized supervised state-of-the-art models as well as more general commercial translation systems. In particular, we outperform the best performing system on the WMT'21 English - Chinese news translation task by only using five examples of English - Chinese parallel data at inference. Moreover, our approach in building these models does not necessitate joint multilingual training or back-translation, is conceptually simple and shows the potential to extend to the multilingual setting. Furthermore, the resulting models are two orders of magnitude smaller than state-of-the-art language models. We then analyze the factors which impact the performance of few-shot translation systems, and highlight that the quality of the few-shot demonstrations heavily determines the quality of the translations generated by our models. Finally, we show that the few-shot paradigm also provides a way to control certain attributes of the translation -- we show that we are able to control for regional varieties and formality using only a five examples at inference, paving the way towards controllable machine translation systems.",
            "year": 2023,
            "citationCount": 62,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that with only 5 examples of high-quality translation data shown at inference, a transformer decoder-only model trained solely with self-supervised learning, is able to match specialized supervised state-of-the-art models as well as more general commercial translation systems."
            },
            "score": 3
        },
        {
            "id": "ef74ec406378390cfaa2a939e5141ee943fba01b",
            "paperId": "ef74ec406378390cfaa2a939e5141ee943fba01b",
            "title": "Few-Shot Regularization to Tackle Catastrophic Forgetting in Multilingual Machine Translation",
            "abstract": "Increasing the number of tasks supported by a machine learning model without forgetting previously learned tasks is the goal of any lifelong learning system. In this work, we study how to mitigate the effects of the catastrophic forgetting problem to sequentially train a multilingual neural machine translation model using minimal past information. First, we describe the catastrophic forgetting phenomenon as a function of the number of tasks learned (language pairs) and the ratios of past data used during the learning of the new task. Next, we explore the importance of applying oversampling strategies for scenarios where only minimal amounts of past data are available. Finally, we derive a new loss function that minimizes the forgetting of previously learned tasks by actively re-weighting past samples and penalizing weights that deviate too much from the original model. Our work suggests that by using minimal amounts of past data and a simple regularization function, we can significantly mitigate the effects of the catastrophic forgetting phenomenon without increasing the computational costs.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work derives a new loss function that minimizes the forgetting of previously learned tasks by actively re-weighting past samples and penalizing weights that deviate too much from the original model."
            },
            "score": 3
        },
        {
            "id": "205eab69e430b4da93ecf3fd9115f0919b448040",
            "paperId": "205eab69e430b4da93ecf3fd9115f0919b448040",
            "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
            "abstract": "Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by\"reading\"a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research. Our models can be applied from https://welm.weixin.qq.com/docs/api/.",
            "year": 2022,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations and has basic skills at explaining and calibrating the decisions from itself."
            },
            "score": 3
        },
        {
            "id": "1415479215dcfec5e2ee0d33f1e1565ae2c65bb9",
            "paperId": "1415479215dcfec5e2ee0d33f1e1565ae2c65bb9",
            "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation",
            "abstract": "Natural language understanding and generation models follow one of the two dominant architectural paradigms: language models (LMs) that process concatenated sequences in a single stack of layers, and encoder-decoder models (EncDec) that utilize separate layer stacks for input and output processing. In machine translation, EncDec has long been the favoured approach, but with few studies investigating the performance of LMs. In this work, we thoroughly examine the role of several architectural design choices on the performance of LMs on bilingual, (massively) multilingual and zero-shot translation tasks, under systematic variations of data conditions and model sizes. Our results show that: (i) Different LMs have different scaling properties, where architectural differences often have a significant impact on model performance at small scales, but the performance gap narrows as the number of parameters increases, (ii) Several design choices, including causal masking and language-modeling objectives for the source sequence, have detrimental effects on translation quality, and (iii) When paired with full-visible masking for source sequences, LMs could perform on par with EncDec on supervised bilingual and multilingual translation tasks, and improve greatly on zero-shot directions by facilitating the reduction of off-target translations.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Different LMs have different scaling properties, and when paired with full-visible masking for source sequences, LMs could perform on par with EncDec on supervised bilingual and multilingual translation tasks, and improve greatly on zero-shot directions by facilitating the reduction of off-target translations."
            },
            "score": 3
        },
        {
            "id": "5f2de1bd6fa1b6b0077a4777263698c659992349",
            "paperId": "5f2de1bd6fa1b6b0077a4777263698c659992349",
            "title": "Disentangling Pretrained Representation to Leverage Low-Resource Languages in Multilingual Machine Translation",
            "abstract": "Multilingual neural machine translation aims to encapsulate multiple languages into a single model. However, it requires an enormous dataset, leaving the low-resource language (LRL) underdeveloped. As LRLs may benefit from shared knowledge of multilingual representation, we aspire to find effective ways to integrate unseen languages in a pre-trained model. Nevertheless, the intricacy of shared representation among languages hinders its full utilisation. To resolve this problem, we employed target language prediction and a central language-aware layer to improve representation in integrating LRLs. Focusing on improving LRLs in the linguistically diverse country of Indonesia, we evaluated five languages using a parallel corpus of 1,000 instances each, with experimental results measured by BLEU showing zero-shot improvement of 7.4 from the baseline score of 7.1 to a score of 15.5 at best. Further analysis showed that the gains in performance are attributed more to the disentanglement of multilingual representation in the encoder with the shift of the target language-specific representation in the decoder.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
            "year": 2023,
            "citationCount": 246,
            "score": 2
        },
        {
            "id": "7c1c95f5fb7fce563171fcc0060c850390753b3c",
            "paperId": "7c1c95f5fb7fce563171fcc0060c850390753b3c",
            "title": "Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models",
            "abstract": "Despite the success of multilingual sequence-to-sequence pre-training, most existing approaches rely on document-level monolingual corpora in many different languages, sentence-level bilingual corpora,\\footnote{In this paper, we use `bilingual corpora' to denote parallel corpora with `bilingual translation pairs' in many different language pairs, each consisting of two sentences/documents with the same meaning written in different languages. We use `trilingual corpora' to denote parallel corpora with `trilingual translation pairs' in many different language combinations, each consisting of three sentences/documents.} and sometimes synthetic document-level bilingual corpora. This hampers the performance with cross-lingual document-level tasks such as document-level translation. Therefore, we propose to mine and leverage document-level trilingual parallel corpora to improve sequence-to-sequence multilingual pre-training. We present \\textbf{Tri}angular Document-level \\textbf{P}re-training (\\textbf{TRIP}), which is the first in the field to accelerate the conventional monolingual and bilingual objectives into a trilingual objective with a novel method called Grafting. Experiments show that TRIP achieves several strong state-of-the-art (SOTA) scores on three multilingual document-level machine translation benchmarks and one cross-lingual abstractive summarization benchmark, including consistent improvements by up to 3.11 d-BLEU points and 8.9 ROUGE-L points.",
            "year": 2022,
            "citationCount": 3,
            "score": 2
        },
        {
            "id": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
            "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
            "year": 2022,
            "citationCount": 1542,
            "score": 2
        },
        {
            "id": "07955e96cbd778d0ae2a68f09d073b866dd84c2a",
            "paperId": "07955e96cbd778d0ae2a68f09d073b866dd84c2a",
            "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
            "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
            "year": 2022,
            "citationCount": 208,
            "score": 2
        },
        {
            "id": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a",
            "paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a",
            "title": "No Language Left Behind: Scaling Human-Centered Machine Translation",
            "abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",
            "year": 2022,
            "citationCount": 642,
            "score": 2
        },
        {
            "id": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
            "year": 2022,
            "citationCount": 2302,
            "score": 2
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3963,
            "score": 2
        },
        {
            "id": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "citationCount": 26071,
            "score": 2
        },
        {
            "id": "bbcf7f15c4f470bc765ac5e8520a99b42e193166",
            "paperId": "bbcf7f15c4f470bc765ac5e8520a99b42e193166",
            "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models",
            "abstract": "Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\\% of the best-performing proprietary model GPT-4o performance. Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "ae3d8af12b176d16961b53850c98d98449f9940e",
            "paperId": "ae3d8af12b176d16961b53850c98d98449f9940e",
            "title": "Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model",
            "abstract": "Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level Multilingual Knowledge Distillation (MMKD), a novel method for improving multilingual language models. Specifically, we employ a teacher-student framework to adopt rich semantic representation knowledge in English BERT. We propose token-, word-, sentence-, and structure-level alignment objectives to encourage multiple levels of consistency between source-target pairs and correlation similarity between teacher and student models. We conduct experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and XQuAD. Experimental results show that MMKD outperforms other baseline models of similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X. Especially, MMKD obtains significant performance gains on low-resource languages.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Multi-level Multilingual Knowledge Distillation (MMKD) is proposed, a novel method for improving multilingual language models that employs a teacher-student framework to adopt rich semantic representation knowledge in English BERT."
            },
            "score": 2
        },
        {
            "id": "607fa844e35b70a3a72d86c639312fe8670ba342",
            "paperId": "607fa844e35b70a3a72d86c639312fe8670ba342",
            "title": "XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge",
            "abstract": "Cross-lingual pre-training has achieved great successes using monolingual and bilingual plain text corpora. However, most pre-trained models neglect multilingual knowledge, which is language agnostic but comprises abundant cross-lingual structure alignment. In this paper, we propose XLM-K, a cross-lingual language model incorporating multilingual knowledge in pre-training. XLM-K augments existing multilingual pre-training with two knowledge tasks, namely Masked Entity Prediction Task and Object Entailment Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly demonstrate significant improvements over existing multilingual language models. The results on MLQA and NER exhibit the superiority of XLM-K in knowledge related tasks. The success in XNLI shows a better cross-lingual transferability obtained in XLM-K. What is more, we provide a detailed probing analysis to confirm the desired knowledge captured in our pre-training regimen. The code is available at https://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.",
            "year": 2021,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "XLM-K is proposed, a cross-lingual language model incorporating multilingual knowledge in pre-training with two knowledge tasks, namely Masked Entity Prediction Task and Object Entailment Task and a detailed probing analysis is provided to confirm the desired knowledge captured in the pre- training regimen."
            },
            "score": 2
        },
        {
            "id": "91ab78ad5bdcf8665cb5b1e34fc164228d9bdf4c",
            "paperId": "91ab78ad5bdcf8665cb5b1e34fc164228d9bdf4c",
            "title": "Leveraging Highly Accurate Word Alignment for Low Resource Translation by Pretrained Multilingual Model",
            "abstract": "Recently, there has been a growing interest in pretraining models in the field of natural language processing. As opposed to training models from scratch, pretrained models have been shown to produce superior results in low-resource translation tasks. In this paper, we introduced the use of pretrained seq2seq models for preordering and translation tasks. We utilized manual word alignment data and mBERT-based generated word alignment data for training preordering and compared the effectiveness of various types of mT5 and mBART models for preordering. For the translation task, we chose mBART as our baseline model and evaluated several input manners. Our approach was evaluated on the Asian Language Treebank dataset, consisting of 20,000 parallel data in Japanese, English and Hindi, where Japanese is either on the source or target side. We also used in-house 3,000 parallel data in Chinese and Japanese. The results indicated that mT5-large trained with manual word alignment achieved a preordering performance exceeding 0.9 RIBES score on Ja-En and Ja-Zh pairs. Moreover, our proposed approach significantly outperformed the baseline model in most translation directions of Ja-En, Ja-Zh, and Ja-Hi pairs in at least one of BLEU/COMET scores.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduced the use of pretrained seq2seq models for preordering and translation tasks and indicated that mT5-large trained with manual word alignment achieved a preordering performance exceeding 0.9 RIBES score on Ja-En and Ja-Zh pairs."
            },
            "score": 2
        },
        {
            "id": "d67f810d1a80c014567b83a2a786eb1f8eba38cf",
            "paperId": "d67f810d1a80c014567b83a2a786eb1f8eba38cf",
            "title": "A Progressive Framework of Vision-language Knowledge Distillation and Alignment for Multilingual Scene",
            "abstract": "Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks. However, most of them are only applicable to the English context. Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages. Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices. In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context. In this framework, we collect high-quality Chinese and English text-image pairs and design two training stages, including multilingual vision-language feature distillation and alignment. During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively. Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model's multilingual performance. Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude. The evaluation demonstrates the effectiveness of our designed training mechanism.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a conceptually simple yet effective multilingual CLIP Compression framework and trains a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context, which achieves superior performance in the English context and competitive performance in the Chinese context."
            },
            "score": 2
        },
        {
            "id": "12b058387e6ef557eda2228848c0961c52721142",
            "paperId": "12b058387e6ef557eda2228848c0961c52721142",
            "title": "Transformer-Based Multilingual Language Models in Cross-Lingual Plagiarism Detection",
            "abstract": "In this work, we study the effectiveness of 6 pretrained Transformer-based language models in the task of crosslingual sentence alignment. We evaluate and compare the models on 10 language pairs, defining the task as a binary classification of two input sentences in English and another Indo-European language. The main objective is to determine the best model in terms of processing speed, accuracy, and cross-lingual transferability. For the latter, we test the language models in three different settings: (i) when one fine-tuned model is used for all considered languages, (ii) when a separately fine-tuned model is used for each group of closely related languages, and (iii) when a separate model is fine-tuned for each language pair. For the experiments we use challenging translation identification datasets containing paraphrases and near-paraphrases.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates and compares the effectiveness of 6 pretrained Transformer-based language models in the task of crosslingual sentence alignment to determine the best model in terms of processing speed, accuracy, and cross-lingual transferability."
            },
            "score": 2
        },
        {
            "id": "03c1eda1173a3eae8a87a7889057a955114b758f",
            "paperId": "03c1eda1173a3eae8a87a7889057a955114b758f",
            "title": "Collaborative Multilingual Continuous Sign Language Recognition: A Unified Framework",
            "abstract": "Current continuous sign language recognition systems generally target on a single language. When it comes to the multilingual problem, existing solutions often build separate models based on the same network and then train them with their corresponding sign language corpora. Observing that different sign languages share some low-level visual patterns, we argue that it is beneficial to optimize the recognition model in a collaborative way. With this motivation, we propose the first unified framework for multilingual continuous sign language recognition. Our framework consists of a shared visual encoder for visual information encoding, multiple language-dependent sequential modules for long-range temporal dependency learning aimed at different languages, and a universal sequential module to learn the commonality of all languages. An additional language embedding is introduced to distinguish different languages within the shared temporal encoders. Further, we present a max-probability decoding method to obtain the alignment between sign videos and sign words for visual encoder refinement. We evaluate our approach on three continuous sign language recognition benchmarks, i.e., RWTH-PHOENIX-Weather, CSL and GSL-SD. The experimental results reveal that our method outperforms the individually trained recognition models. Our method also demonstrates better performance compared with state-of-the-art algorithms.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the first unified framework for multilingual continuous sign language recognition, which consists of a shared visual encoder for visual information encoding, multiple language-dependent sequential modules for long-range temporal dependency learning aimed at different languages, and a universal sequential module to learn the commonality of all languages."
            },
            "score": 2
        },
        {
            "id": "4a78f23be51f314240d17fa0a340785ee88bf032",
            "paperId": "4a78f23be51f314240d17fa0a340785ee88bf032",
            "title": "Cross-lingual Language Model Pretraining for Retrieval",
            "abstract": "Existing research on cross-lingual retrieval cannot take good advantage of large-scale pretrained language models such as multilingual BERT and XLM. We hypothesize that the absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are key factors of this issue. In this paper, we introduce two novel retrieval-oriented pretraining tasks to further pretrain cross-lingual language models for downstream retrieval tasks such as cross-lingual ad-hoc retrieval (CLIR) and cross-lingual question answering (CLQA). We construct distant supervision data from multilingual Wikipedia using section alignment to support retrieval-oriented language model pretraining. We also propose to directly finetune language models on part of the evaluation collection by making Transformers capable of accepting longer sequences. Experiments on multiple benchmark datasets show that our proposed model can significantly improve upon general multilingual language models in both the cross-lingual retrieval setting and the cross-lingual transfer setting.",
            "year": 2021,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces two novel retrieval-oriented pretraining tasks to further pretrain cross-lingual language models for downstream retrieval tasks such as cross-lingsual ad-hoc retrieval (CLIR), and proposes to directly finetune language models on part of the evaluation collection by making Transformers capable of accepting longer sequences."
            },
            "score": 2
        },
        {
            "id": "7e7569c064ea64c023ca429b083fcd574fcfa3a8",
            "paperId": "7e7569c064ea64c023ca429b083fcd574fcfa3a8",
            "title": "Joint Multilingual Knowledge Graph Completion and Alignment",
            "abstract": "Knowledge graph (KG) alignment and completion are usually treated as two independent tasks. While recent work has leveraged entity and relation alignments from multiple KGs, such as alignments between multilingual KGs with common entities and relations, a deeper understanding of the ways in which multilingual KG completion (MKGC) can aid the creation of multilingual KG alignments (MKGA) is still limited. Motivated by the observation that structural inconsistencies -- the main challenge for MKGA models -- can be mitigated through KG completion methods, we propose a novel model for jointly completing and aligning knowledge graphs. The proposed model combines two components that jointly accomplish KG completion and alignment. These two components employ relation-aware graph neural networks that we propose to encode multi-hop neighborhood structures into entity and relation representations. Moreover, we also propose (i) a structural inconsistency reduction mechanism to incorporate information from the completion into the alignment component, and (ii) an alignment seed enlargement and triple transferring mechanism to enlarge alignment seeds and transfer triples during KGs alignment. Extensive experiments on a public multilingual benchmark show that our proposed model outperforms existing competitive baselines, obtaining new state-of-the-art results on both MKGC and MKGA tasks. We publicly release the implementation of our model at https://github.com/vinhsuhi/JMAC",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel model for jointly completing and aligning knowledge graphs that outperforms existing competitive baselines, obtaining new state-of-the-art results on both MKGC and MKGA tasks."
            },
            "score": 2
        },
        {
            "id": "14e650cb5578be5e9f88bc2c5f22cdf15e61eee6",
            "paperId": "14e650cb5578be5e9f88bc2c5f22cdf15e61eee6",
            "title": "Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention",
            "abstract": "Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces additional alignment models and increases the complexity of the whole framework; (b) training inefficiency: the trained model will only be used for the completion of one target KG, although the data from all KGs are used simultaneously. To address these drawbacks, we propose a novel multilingual KGC framework with language-sensitive multi-graph attention such that the missing links on all given KGs can be inferred by a universal knowledge completion model. Specifically, we first build a relational graph neural network by sharing the embeddings of aligned nodes to transfer language-independent knowledge. Meanwhile, a language-sensitive multi-graph attention (LSMGA) is proposed to deal with the information inconsistency among different KGs. Experimental results show that our model achieves significant improvements on the DBP-5L and E-PKG datasets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel multilingual KGC framework with language-sensitive multi-graph attention such that the missing links on all given KGs can be inferred by a universal knowledge completion model, which achieves significant improvements on the DBP-5L and E-PKG datasets."
            },
            "score": 2
        },
        {
            "id": "0f3e398d57323e5573700cd48a59829487508159",
            "paperId": "0f3e398d57323e5573700cd48a59829487508159",
            "title": "Towards an image-term co-occurence model for multilingual terminology alignment and cross-language image indexing",
            "abstract": "This thesis addresses the potential that the relation between terms and images in multilingual specialized documentation has for glossary compilation, terminology alignment, and image indexing. It takes advantage of the recurrent use of these two modes of communication (i.e., text and images) in digital documents to build a bimodal co-occurrence model which aims at dynamically compiling glossaries of a wider coverage. The model relies on the developments of content-based image retrieval (CBIR) and text processing techniques. CBIR is used to make two images from different origin match, and text processing supports term recognition, artifact noun classification, and image-term association. The model aligns one image with its denominating term from collateral text, and then aligns this image with another image of the same artifact from a different document, which also enables the alignment of the two equivalent denominating terms. The ultimate goal of the model is to tackle the limitations and drawbacks of current static terminological repositories by generating bimodal, bilingual glossaries that reflect real usage, even when terms and images may originate from noisy corpora.",
            "year": 2014,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis addresses the potential that the relation between terms and images in multilingual specialized documentation has for glossary compilation, terminology alignment, and image indexing by building a bimodal co-occurrence model which aims at dynamically compiling glossaries of a wider coverage."
            },
            "score": 2
        },
        {
            "id": "62aede410521658b172de6d124db7cedda08492a",
            "paperId": "62aede410521658b172de6d124db7cedda08492a",
            "title": "Mu2SLAM: Multitask, Multilingual Speech and Language Models",
            "abstract": "We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The authors' model improves by more than 6\\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity onXNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks."
            },
            "score": 2
        },
        {
            "id": "7e9d953c92a7666e648d4741a9cb67a977a99731",
            "paperId": "7e9d953c92a7666e648d4741a9cb67a977a99731",
            "title": "AltDiffusion: A Multilingual Text-to-Image Diffusion Model",
            "abstract": "Large Text-to-Image(T2I) diffusion models have shown a remarkable capability to produce photorealistic and diverse images based on text inputs. However, existing works only support limited language input, e.g., English, Chinese, and Japanese, leaving users beyond these languages underserved and blocking the global expansion of T2I models. Therefore, this paper presents AltDiffusion, a novel multilingual T2I diffusion model that supports eighteen different languages. Specifically, we first train a multilingual text encoder based on the knowledge distillation. Then we plug it into a pretrained English-only diffusion model and train the model with a two-stage schema to enhance the multilingual capability, including concept alignment and quality improvement stage on a large-scale multilingual dataset. Furthermore, we introduce a new benchmark, which includes Multilingual-General-18(MG-18) and Multilingual-Cultural-18(MC-18) datasets, to evaluate the capabilities of T2I diffusion models for generating high-quality images and capturing culture-specific concepts in different languages. Experimental results on both MG-18 and MC-18 demonstrate that AltDiffusion outperforms current state-of-the-art T2I models, e.g., Stable Diffusion in multilingual understanding, especially with respect to culture-specific concepts, while still having comparable capability for generating high-quality images. All source code and checkpoints could be found in https://github.com/superhero-7/AltDiffuson.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on both MG-18 and MC-18 demonstrate that AltDiffusion outperforms current state-of-the-art T2I models, e.g., Stable Diffusion in multilingual understanding, especially with respect to culture-specific concepts, while still having comparable capability for generating high-quality images."
            },
            "score": 2
        },
        {
            "id": "c4b95abe16439fddd1db33e9aa386bec8a667e39",
            "paperId": "c4b95abe16439fddd1db33e9aa386bec8a667e39",
            "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
            "abstract": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities.However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks.In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks.We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features.We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset.We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.",
            "year": 2021,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multilingual language model with 24 languages with entity representations is trained and it is shown that the model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks."
            },
            "score": 2
        },
        {
            "id": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a",
            "paperId": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a",
            "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
            "abstract": "We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PaLI-X, a multilingual vision and language model, advances the state-of-the-art on most vision-and-language benchmarks considered and observes emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix."
            },
            "score": 2
        },
        {
            "id": "55b9a2ade0a49e9cf10b71528d69dfee4e826025",
            "paperId": "55b9a2ade0a49e9cf10b71528d69dfee4e826025",
            "title": "Cedille: A large autoregressive French language model",
            "abstract": "Scaling up the size and training of autoregressive language models has enabled novel ways of solving Natural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale language models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages other than English remain largely unexplored. Here, we introduce Cedille, a large open source auto-regressive language model, specifically trained for the French language. Our results show that Cedille outperforms existing French language models and is competitive with GPT-3 on a range of French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity exhibited by these models, showing that Cedille marks an improvement in language model safety thanks to dataset filtering.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cedille, a large open source auto-regressive language model, specifically trained for the French language, is introduced and shows that Cedille outperforms existing French language models and is competitive with GPT-3 on a range of French zero-shot benchmarks."
            },
            "score": 2
        },
        {
            "id": "748c6718cf292af98cd8bed438fb3088add0a1ea",
            "paperId": "748c6718cf292af98cd8bed438fb3088add0a1ea",
            "title": "Knowledge Enhanced Pre-training for Cross-lingual Dense Retrieval",
            "abstract": "In recent years, multilingual pre-trained language models (mPLMs) have achieved significant progress in cross-lingual dense retrieval. However, most mPLMs neglect the importance of knowledge. Knowledge always conveys similar semantic concepts in a language-agnostic manner, while query-passage pairs in cross-lingual retrieval also share common factual information. Motivated by this observation, we introduce KEPT, a novel mPLM that effectively leverages knowledge to learn language-agnostic semantic representations. To achieve this, we construct a multilingual knowledge base using hyperlinks and cross-language page alignment data annotated by Wiki. From this knowledge base, we mine intra- and cross-language pairs by extracting symmetrically linked segments and multilingual entity descriptions. Subsequently, we adopt contrastive learning with the mined pairs to pre-train KEPT. We evaluate KEPT on three widely-used benchmarks, considering both zero-shot cross-lingual transfer and supervised multilingual fine-tuning scenarios. Extensive experimental results demonstrate that KEPT achieves strong multilingual and cross-lingual retrieval performance with significant improvements over existing mPLMs.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "c953895bf665023c17c9825e0978fc15b8cc3687",
            "paperId": "c953895bf665023c17c9825e0978fc15b8cc3687",
            "title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation",
            "abstract": "Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to apply vanilla KD methods to transfer knowledge from closed-source Multilingual Neural Machine Translation (MNMT) models based on LLMs. In this scenario, the soft labels and training data are not accessible, making it difficult to achieve effective knowledge transfer. To address this issue, this paper proposes a Teacher Assistant enhanced Knowledge Distillation (TAeKD) method to augment the knowledge transfer capacity from closed-source MNMT models. Specifically, TAeKD designs a fusion model that integrates translation outputs from multiple closed-source models to generate soft labels and training samples. Furthermore, a quality assessment learning mechanism is introduced to enhance the generalization of the fusion model and elevate the quality of the fusion data used to train the student model. To facilitate research on knowledge transfer from MNMT models, we also introduce FuseData, a benchmark consisting of a blend of translations from multiple closed-source systems. The experimental results show that TAeKD outperforms the previous state-of-the-art KD methods on both WMT22 and FLORES-101 test sets.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "313fde020ae6ec4ed7f7be6634349b6b33d76adb",
            "paperId": "313fde020ae6ec4ed7f7be6634349b6b33d76adb",
            "title": "TransMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data",
            "abstract": "Transliterating related languages that use different scripts into a common script shows effectiveness in improving crosslingual transfer in downstream tasks. However, this methodology often makes pretraining a model from scratch unavoidable, as transliteration brings about new subwords not covered in existing multilingual pretrained language models (mPLMs). This is not desired because it takes a lot of computation budget for pretraining. A more promising way is to make full use of available mPLMs. To this end, this paper proposes a simple but effective framework: Transliterate-Merge-Initialize (TransMI), which can create a strong baseline well-suited for data that is transliterated into a common script by exploiting an mPLM and its accompanied tokenizer. TransMI has three stages: (a) transliterate the vocabulary of an mPLM into a common script; (b) merge the new vocabulary with the original vocabulary; and (c) initialize the embeddings of the new subwords. We applied TransMI to three recent strong mPLMs, and our experiments demonstrate that TransMI not only preserves their ability to handle non-transliterated data, but also enables the models to effectively process transliterated data: the results show a consistent improvement of 3% to 34%, varying across different models and tasks. We make our code and models publicly available at \\url{https://github.com/cisnlp/TransMI}.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "89707ffdb6d29e404a54bb5140e4a303b5bb7347",
            "paperId": "89707ffdb6d29e404a54bb5140e4a303b5bb7347",
            "title": "Transformer Meets External Context: A Novel Approach to Enhance Neural Machine Translation",
            "abstract": "\u2014Most neural machine translation (NMT) systems rely on parallel data, comprising text in the source language and its corresponding translation in the target language. While it\u2019s acknowledged that context enhances NMT models, this work proposes a novel approach by incorporating external context, specifically explanations of source text meanings, akin to how human translators leverage context for comprehension. The suggested methodology innovatively addresses the challenge of incorporating lengthy contextual information into NMT systems. By employing state-of-the-art transformer-based models, external context is integrated, thereby enriching the translation process. A key aspect of the approach lies in the utilization of diverse text summarization techniques, strategically employed to efficiently distill extensive contextual details into the NMT framework. This novel solution not only overcomes the obstacle posed by lengthy context but also enhances the translation quality, marking a advancement in the field of NMT. Furthermore, the data-centric approach ensures robustness and effectiveness, yielding improvements in translation quality, as evidenced by a considerable boost in BLEU score points ranging from 0.46 to 1.87 over baseline models. Additionally, we make our dataset publicly available, facilitating further research in this domain.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "32ab679959017a18a2e5eedc9c5cd4f713792824",
            "paperId": "32ab679959017a18a2e5eedc9c5cd4f713792824",
            "title": "MoNMT: Modularly Leveraging Monolingual and Bilingual Knowledge for Neural Machine Translation",
            "abstract": "The effective use of monolingual and bilingual knowledge represents a critical challenge within the neural machine translation (NMT) community. In this paper, we propose a modular strategy that facilitates the cooperation of these two types of knowledge in translation tasks, while avoiding the issue of catastrophic forgetting and exhibiting superior model generalization and robustness. Our model is comprised of three functionally independent modules: an encoding module, a decoding module, and a transferring module. The former two acquire large-scale monolingual knowledge via self-supervised learning, while the latter is trained on parallel data and responsible for transferring latent features between the encoding and decoding modules. Extensive experiments in multi-domain translation tasks indicate our model yields remarkable performance, with up to 7 BLEU improvements in out-of-domain tests over the conventional pretrain-and-finetune approach. Our codes are available at https://github.com/NLP2CT/MoNMT.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "8221f1597000543432b7021ca79dbc51a7a63f9c",
            "paperId": "8221f1597000543432b7021ca79dbc51a7a63f9c",
            "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "abstract": "Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.",
            "year": 2023,
            "citationCount": 236,
            "score": 1
        },
        {
            "id": "89d7a037646d37bc80bcd0a0b55d2a1550369c42",
            "paperId": "89d7a037646d37bc80bcd0a0b55d2a1550369c42",
            "title": "Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation",
            "abstract": "Recently, $k$NN-MT has shown the promising capability of directly incorporating the pre-trained neural machine translation (NMT) model with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, it heavily relies on high-quality in-domain parallel corpora, limiting its capability on unsupervised domain adaptation, where in-domain parallel corpora are scarce or nonexistent. In this paper, we propose a novel framework that directly uses in-domain monolingual sentences in the target language to construct an effective datastore for $k$-nearest-neighbor retrieval. To this end, we first introduce an autoencoder task based on the target language, and then insert lightweight adapters into the original NMT model to map the token-level representation of this task to the ideal representation of translation task. Experiments on multi-domain datasets demonstrate that our proposed approach significantly improves the translation accuracy with target-side monolingual data, while achieving comparable performance with back-translation.",
            "year": 2021,
            "citationCount": 23,
            "score": 1
        },
        {
            "id": "d94aa165d21a34942328d31f61eb45b57c8ea32f",
            "paperId": "d94aa165d21a34942328d31f61eb45b57c8ea32f",
            "title": "Lexical-Constraint-Aware Neural Machine Translation via Data Augmentation",
            "abstract": "Leveraging lexical constraint is extremely significant in domain-specific machine translation and interactive machine translation. Previous studies mainly focus on extending beam search algorithm or augmenting the training corpus by replacing source phrases with the corresponding target translation. These methods either suffer from the heavy computation cost during inference or depend on the quality of the bilingual dictionary pre-specified by user or constructed with statistical machine translation. In response to these problems, we present a conceptually simple and empirically effective data augmentation approach in lexical constrained neural machine translation. Specifically, we make constraint-aware training data by first randomly sampling the phrases of the reference as constraints, and then packing them together into the source sentence with a separation symbol. Extensive experiments on several language pairs demonstrate that our approach achieves superior translation results over the existing systems, improving translation of constrained sentences without hurting the unconstrained ones.",
            "year": 2020,
            "citationCount": 31,
            "score": 1
        },
        {
            "id": "c1c98ef93fb6474837961ef300cf3d8e7d3a0cd0",
            "paperId": "c1c98ef93fb6474837961ef300cf3d8e7d3a0cd0",
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To facilitate research on few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. BUFFET is designed to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. Using BUFFET, we perform thorough evaluations of state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples. Our analysis suggests various avenues for future research in few-shot cross-lingual transfer, such as improved pretraining, understanding, and future evaluations.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few- shot examples and instructions and suggests various avenues for future research in few-shot cross-lingual transfer, such as improved pretraining, understanding, and future evaluations."
            },
            "score": 1
        },
        {
            "id": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
            "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
            "year": 2019,
            "citationCount": 4991,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time."
            },
            "score": 1
        },
        {
            "id": "c644956d5cfdb7ad7ea24a420608b9b58c148e3d",
            "paperId": "c644956d5cfdb7ad7ea24a420608b9b58c148e3d",
            "title": "Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog",
            "abstract": "One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for machine learning models for this task is time-consuming, it is desirable to make use of existing data in a high-resource language to train models in low-resource languages. However, development of such models has largely been hindered by the lack of multilingual training data. In this paper, we present a new data set of 57k annotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods: (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.",
            "year": 2018,
            "citationCount": 245,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a new data set of 57k annotated utterances in English, Spanish, Spanish and Thai and uses this data set to evaluate three different cross-lingual transfer methods, finding that given several hundred training examples in the the target language, the latter two methods outperform translating the training data."
            },
            "score": 1
        },
        {
            "id": "085b360d3c08aaf997f45a78e27f2629f5625205",
            "paperId": "085b360d3c08aaf997f45a78e27f2629f5625205",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "abstract": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
            "year": 2020,
            "citationCount": 93,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon, and the state-of-the-art in XNLI for the translate-test and zero-shot approaches is improved."
            },
            "score": 1
        },
        {
            "id": "a38977a8506ff8d92f286e6b4b8ec0d549199924",
            "paperId": "a38977a8506ff8d92f286e6b4b8ec0d549199924",
            "title": "Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models",
            "abstract": "Character-based Neural Network Language Models (NNLM) have the advantage of smaller vocabulary and thus faster training times in comparison to NNLMs based on multi-character units. However, in low-resource scenarios, both the character and multi-character NNLMs suffer from data sparsity. In such scenarios, cross-lingual transfer has improved multi-character NNLM performance by allowing information transfer from a source to the target language. In the same vein, we propose to use cross-lingual transfer for character NNLMs applied to low-resource Automatic Speech Recognition (ASR). However, applying cross-lingual transfer to character NNLMs is not as straightforward. We observe that relatedness of the source language plays an important role in cross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR tasks for two target languages: Finnish (with English and Estonian as source) and Swedish (with Danish, Norwegian, and English as source). Prior work has observed no difference between using the related or unrelated language for multi-character NNLMs. We, however, show that for character-based NNLMs, only pretraining with a related language improves the ASR performance, and using an unrelated language may deteriorate it. We also observe that the benefits are larger when there is much lesser target data than source data.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For character-based NNLMs, only pretraining with a related language improves the ASR performance, and using an unrelated language may deteriorate it, and the benefits are larger when there is much lesser target data than source data."
            },
            "score": 1
        },
        {
            "id": "5f89dfb129c20e35aeeb69639ee964c98b05c0de",
            "paperId": "5f89dfb129c20e35aeeb69639ee964c98b05c0de",
            "title": "Speech Recognition for Turkic Languages Using Cross-Lingual Transfer Learning from Kazakh",
            "abstract": "This paper investigates the effectiveness of transfer learning in building automatic speech recognition models for nine Turkic languages (Azerbaijani, Bashkir, Chuvash, Kyrgyz, Sakha, Tatar, Turkish, Uyghur, and Uzbek), by leveraging large-scale training data from the Kazakh language. The performance of the models built using transfer learning from Kazakh was compared with the performance of the models for three non-Turkic languages (Indonesian, Japanese, and Swedish) to which transfer learning from Kazakh was also applied. We also compared the performance of the models with the results of models trained on English data. A total of 64 models were created. Most of the models built using transfer learning from Kazakh performed better than the monolingual baselines, with the most notable improvement observed for the Sakha model, which achieved a 45.5% and 22.8% reduction in the word error rate and character error rate on the test set, respectively. The datasets and codes used to train the models are available for download from https://github.com/IS2AI/CLTL_Turkic_ASR.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Most of the models built using transfer learning from Kazakh performed better than the monolingual baselines, with the most notable improvement observed for the Sakha model, which achieved a 45.5% and 22.8% reduction in the word error rate and character error rate on the test set, respectively."
            },
            "score": 1
        },
        {
            "id": "780b8dd2e648ca71d0409077761d59ed8d47be62",
            "paperId": "780b8dd2e648ca71d0409077761d59ed8d47be62",
            "title": "Cross-lingual Transfer Learning for Detecting Negative Campaign in Israeli Municipal Elections: a Case Study",
            "abstract": "Political competitions are complex settings where candidates use campaigns to promote their chances to be elected. As we can recently observe, some candidates choose to focus on a negative campaign that emphasizes the negative aspects of the competing person and is aimed at offending opponents or the opponent\u2019s supporters . The big challenge in this area is the lack of annotated datasets for training efficient classifiers. Therefore, transfer learning from other relevant domains and other languages could be very useful for this task. Considering the recent success of meta-learning in domain adaptation, we apply it to our task of utilizing available datasets from different domains and languages. This work explores the negative campaign detection task from multiple perspectives: the efficiency of different text representations and classification models, and the effect of transfer learning from offensive language detection in different languages for negative campaign detection in Hebrew. We demonstrate that the lack of training data for negative campaign detection in a low-resourced language such as Hebrew can be compensated to some extent by available datasets for offensive language detection in the same and other languages. We report an empirical case study for political campaigns in Israeli municipal elections. 1",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that the lack of training data for negative campaign detection in a low-resourced language such as Hebrew can be compensated to some extent by available datasets for offensive language detection in the same and other languages."
            },
            "score": 1
        },
        {
            "id": "07ecfeb52ff0768c3067cac5309d8150701d5906",
            "paperId": "07ecfeb52ff0768c3067cac5309d8150701d5906",
            "title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model",
            "abstract": "Phrase break prediction is a crucial task for improving the prosody naturalness of a text-to-speech (TTS) system. However, most proposed phrase break prediction models are monolingual, trained exclusively on a large amount of labeled data. In this paper, we address this issue for low-resource languages with limited labeled data using cross-lingual transfer. We investigate the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model. We use manually collected datasets in four Indo-European languages: one high-resource language and three with limited resources. Our findings demonstrate that cross-lingual transfer learning can be a particularly effective approach, especially in the few-shot setting, for improving performance in low-resource languages. This suggests that cross-lingual transfer can be inexpensive and effective for developing TTS front-end in resource-poor languages.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model and demonstrates that cross-lingsual transfer learning can be a particularly effective approach, especially in the few- shot setting, for improving performance in low-resource languages."
            },
            "score": 1
        },
        {
            "id": "73f6927da9258a9637c7acffc9b201a9c0db38e4",
            "paperId": "73f6927da9258a9637c7acffc9b201a9c0db38e4",
            "title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment",
            "abstract": "Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves cross-lingual transfer results, especially in low-resource scenarios, while only keeping and fine-tuning an extremely small number of parameters compared to the full model (e.g., Our framework only requires 0.16\\% additional parameters of a full-model for each language in the few-shot learning scenario). The codes are available at \\url{https://github.com/eric-ai-lab/PECTVLM}. The codes are available at \\url{https://github.com/eric-ai-lab/PECTVLM}.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter- efficient fine-tuning methods for parameter-fficient cross-lingsual transfer."
            },
            "score": 1
        },
        {
            "id": "053d5b85d5a24440755a81da1dee8ee0aa930189",
            "paperId": "053d5b85d5a24440755a81da1dee8ee0aa930189",
            "title": "Cross-Lingual Transfer Learning for Misinformation Detection: Investigating Performance Across Multiple Languages",
            "abstract": "Detection of misinformation on social media requires human-annotated datasets to achieve truthful results. However, the annotation process is time-consuming due to the difficulty of labeling the veracity of the claims. Furthermore, most of the annotated misinformation detection datasets in the social media domain predominantly reside in English. To overcome this problem, we investigate the performance of cross-lingual transfer learning for misinformation detection across various languages, including, Arabic, Chinese, Turkish, and Polish. For this purpose, we analyze three different experimental setups on multilingual pre-trained language models in five natural languages (English, Arabic, Chinese, Turkish, and Polish). The results show that the multi-lingual mDeBERTa model can be applicable with fine-tuning in a widely-used language, i.e., English, and tested on a low-resource Turkish language with a successful recovery ratio, i.e., the metric shows the percentage of the recovered baseline score. For each model, we observe higher and more robust transfer ability between Polish and Arabic. Furthermore, it is possible to claim that contextual similarities outweigh language similarities, due to unsuccessful transfer learning ability between the English-Polish language pair.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that the multi-lingual mDeBERTa model can be applicable with fine-tuning in a widely-used language, and tested on a low-resource Turkish language with a successful recovery ratio, i.e., the metric shows the percentage of the recovered baseline score."
            },
            "score": 1
        },
        {
            "id": "b7270830eb423d6970490e7ab20c14732e9d7ae2",
            "paperId": "b7270830eb423d6970490e7ab20c14732e9d7ae2",
            "title": "Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning",
            "abstract": "Most Transformer language models are primarily pretrained on English text, limiting their use for other languages. As the model sizes grow, the performance gap between English and other languages with fewer compute and data resources increases even further. Consequently, more resource-efficient training methods are needed to bridge the gap for languages with fewer resources available. To address this problem, we introduce a cross-lingual and progressive transfer learning approach, called CLP-Transfer, that transfers models from a source language, for which pretrained models are publicly available, like English, to a new target language. As opposed to prior work, which focused on the cross-lingual transfer between two languages, we extend the transfer to the model size. Given a pretrained model in a source language, we aim for a same-sized model in a target language. Instead of training a model from scratch, we exploit a smaller model that is in the target language but requires much fewer resources. Both small and source models are then used to initialize the token embeddings of the larger model based on the overlapping vocabulary of the source and target language. All remaining weights are reused from the model in the source language. This approach outperforms the sole cross-lingual transfer and can save up to 80% of the training steps compared to the random initialization.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A cross-lingual and progressive transfer learning approach that transfers models from a source language, for which pretrained models are publicly available, like English, to a new target language, called CLP-Transfer is introduced."
            },
            "score": 1
        },
        {
            "id": "c45c627aa40a982788e1a1e3b1b2a2091b2a71d0",
            "paperId": "c45c627aa40a982788e1a1e3b1b2a2091b2a71d0",
            "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training",
            "abstract": "Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding space usually requires sentence-level or word-level parallel corpora, which are expensive to be obtained for low-resource languages. An alternative is to make the multilingual encoders more robust; when fine-tuning the encoder using downstream task, we train the encoder to tolerate noise in the contextual embedding spaces such that even if the representations of different languages are not aligned well, the model can still achieve good performance on zero-shot cross-lingual transfer. In this work, we propose a learning strategy for training robust models by drawing connections between adversarial examples and the failure cases of zero-shot cross-lingual transfer. We adopt two widely used robust training methods, adversarial training and randomized smoothing, to train the desired robust model. The experimental results demonstrate that robust training improves zero-shot cross-lingual transfer on text classification tasks. The improvement is more significant in the generalized cross-lingual transfer setting, where the pair of input sentences belong to two different languages.",
            "year": 2021,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a learning strategy for training robust models by drawing connections between adversarial examples and the failure cases of zero-shot cross-lingual transfer, and adopts two widely used robust training methods, adversarial training and randomized smoothing, to train the desired robust model."
            },
            "score": 1
        },
        {
            "id": "d581a99af33b0faa37f2578a49a57451b8b444db",
            "paperId": "d581a99af33b0faa37f2578a49a57451b8b444db",
            "title": "Cross-Lingual Transfer Learning for Arabic Task-Oriented Dialogue Systems Using Multilingual Transformer Model mT5",
            "abstract": "Due to the promising performance of pre-trained language models for task-oriented dialogue systems (DS) in English, some efforts to provide multilingual models for task-oriented DS in low-resource languages have emerged. These efforts still face a long-standing challenge due to the lack of high-quality data for these languages, especially Arabic. To circumvent the cost and time-intensive data collection and annotation, cross-lingual transfer learning can be used when few training data are available in the low-resource target language. Therefore, this study aims to explore the effectiveness of cross-lingual transfer learning in building an end-to-end Arabic task-oriented DS using the mT5 transformer model. We use the Arabic task-oriented dialogue dataset (Arabic-TOD) in the training and testing of the model. We present the cross-lingual transfer learning deployed with three different approaches: mSeq2Seq, Cross-lingual Pre-training (CPT), and Mixed-Language Pre-training (MLT). We obtain good results for our model compared to the literature for Chinese language using the same settings. Furthermore, cross-lingual transfer learning deployed with the MLT approach outperform the other two approaches. Finally, we show that our results can be improved by increasing the training dataset size.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to explore the effectiveness of cross-lingual transfer learning in building an end-to-end Arabic task-oriented DS using the mT5 transformer model, and presents the cross-centre transfer learning deployed with three different approaches: mSeq2Seq, Cross-lingUAL Pre- training (CPT), and Mixed-Language Pre-training (MLT)."
            },
            "score": 1
        },
        {
            "id": "3aab461cab4f0844eaa46a8addacd25f09d384e2",
            "paperId": "3aab461cab4f0844eaa46a8addacd25f09d384e2",
            "title": "Cross-Lingual Transfer Learning in NLP: Enhancing English Language Learning for Non-Native Speakers",
            "abstract": "In the evolving landscape of language education, this paper delves into the intersection of Natural Language Processing (NLP) and education technology to address the unique challenges faced by non-native speakers, particularly children, in acquiring English proficiency. Leveraging the potential of Cross-Lingual Transfer Learning, the proposed methodology, implemented in Python, aims to enhance language learning outcomes through the innovative use of the NNCES (NNCES) corpus. The NNCES corpus, featuring 50 Telugu-speaking children aged 8 to 12 engaged in English language learning, serves as a rich dataset for exploring cross-lingual transfer learning strategies. The paper introduces Multilingual Transfer Learning with Domain Adaptation (MTL-DA) that fine-tunes pre-trained multilingual models on the NNCES corpus. This strategic adaptation enables models to discern linguistic nuances, phonetic variations, and semantic context inherent in NNCES. The methodology involves a comprehensive pipeline, encompassing dataset collection, preprocessing, feature extraction, and model training. Min-Max Normalization is applied to acoustic features, and Mel-frequency cepstral coefficients (MFCCs) and word embeddings from pre-trained models are integrated into a holistic feature vector. The fine-tuned models exhibit superior performance in English language learning tasks, showcasing an accuracy of $99.6 \\%$. Comparative analysis with existing methods reveals a significant improvement, with the proposed method surpassing others by $3.6 \\%$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Multilingual Transfer Learning with Domain Adaptation (MTL-DA) that fine-tunes pre-trained multilingual models on the NNCES corpus enables models to discern linguistic nuances, phonetic variations, and semantic context inherent in NNCES."
            },
            "score": 1
        },
        {
            "id": "dba8cab78fb3bcf81b5c85525305ad410021b951",
            "paperId": "dba8cab78fb3bcf81b5c85525305ad410021b951",
            "title": "Meta-learning For Vision-and-language Cross-lingual Transfer",
            "abstract": "Current pre-trained vison-language models (PVLMs) achieve excellent performance on a range of multi-modal datasets. Recent work has aimed at building multilingual models, and a range of novel multilingual multi-modal datasets have been proposed. Current PVLMs typically perform poorly on these datasets when used for multi-modal zero-shot or few-shot cross-lingual transfer, especially for low-resource languages. To alleviate this problem, we propose a novel meta-learning fine-tuning framework. Our framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner. Experiments show that our method boosts the performance of current state-of-the-art PVLMs in both zero-shot and few-shot cross-lingual transfer on a range of vision-language understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co)",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner by proposing a novel meta-learning fine-tuning framework."
            },
            "score": 1
        },
        {
            "id": "0d52e5d236a8bfb8f7de11945d1c3bf06654d085",
            "paperId": "0d52e5d236a8bfb8f7de11945d1c3bf06654d085",
            "title": "Distilling Efficient Language-Specific Models for Cross-Lingual Transfer",
            "abstract": "Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs' language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which retain the capacity of the original MMTs for cross-lingual transfer. This is achieved by distilling the MMT bilingually, i.e., using data from only the source and target language of interest. Specifically, we use a two-phase distillation approach, termed BiStil: (i) the first phase distils a general bilingual model from the MMT, while (ii) the second, task-specific phase sparsely fine-tunes the bilingual\"student\"model using a task-tuned variant of the original MMT as its\"teacher\". We evaluate this distillation technique in zero-shot cross-lingual transfer across a number of standard cross-lingual benchmarks. The key results indicate that the distilled models exhibit minimal degradation in target language performance relative to the base MMT despite being significantly smaller and faster. Furthermore, we find that they outperform multilingually distilled models such as DistilmBERT and MiniLMv2 while having a very modest training budget in comparison, even on a per-language basis. We also show that bilingual models distilled from MMTs greatly outperform bilingual models trained from scratch. Our code and models are available at https://github.com/AlanAnsell/bistil.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The key results indicate that the distilled models exhibit minimal degradation in target language performance relative to the base MMT despite being significantly smaller and faster, and are found to outperform multilingually distilled models such as DistilmBERT and MiniLMv2 while having a very modest training budget in comparison."
            },
            "score": 1
        },
        {
            "id": "dbbd46753d98f2792076e038f1da2fd3e74b4eec",
            "paperId": "dbbd46753d98f2792076e038f1da2fd3e74b4eec",
            "title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?",
            "abstract": "Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability\u2014which we argue is of use for machine translation but detrimental elsewhere.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies."
            },
            "score": 1
        },
        {
            "id": "26299d5fdc5137291dc6a091573b3d18aba1d1c2",
            "paperId": "26299d5fdc5137291dc6a091573b3d18aba1d1c2",
            "title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer",
            "abstract": "The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and achieves competitive results on question answering.",
            "year": 2020,
            "citationCount": 500,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MAD-X is proposed, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations and introduces a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language."
            },
            "score": 1
        },
        {
            "id": "3ee876f0c0b0ae549b4e9a67836b5f8351d70266",
            "paperId": "3ee876f0c0b0ae549b4e9a67836b5f8351d70266",
            "title": "MultiLexBATS: Multilingual Dataset of Lexical Semantic Relations",
            "abstract": "Understanding the relation between the meanings of words is an important part of comprehending natural language. Prior work has either focused on analysing lexical semantic relations in word embeddings or probing pretrained language models (PLMs), with some exceptions. Given the rarity of highly multilingual benchmarks, it is unclear to what extent PLMs capture relational knowledge and are able to transfer it across languages. To start addressing this question, we propose MultiLexBATS, a multilingual parallel dataset of lexical semantic relations adapted from BATS in 15 languages including low-resource languages, such as Bambara, Lithuanian, and Albanian. As experiment on cross-lingual transfer of relational knowledge, we test the PLMs\u2019 ability to (1) capture analogies across languages, and (2) predict translation targets. We find considerable differences across relation types and languages with a clear preference for hypernymy and antonymy as well as romance languages.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "e100989ca03cecf8ad865287d7a77719ddb8d796",
            "paperId": "e100989ca03cecf8ad865287d7a77719ddb8d796",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "abstract": "It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "b3e2839ccd5f87ff22f2fe87902349c0dcbe2d97",
            "paperId": "b3e2839ccd5f87ff22f2fe87902349c0dcbe2d97",
            "title": "Leveraging Pretrained Language Models for Enhanced Entity Matching: A Comprehensive Study of Fine-Tuning and Prompt Learning Paradigms",
            "abstract": "Pretrained Language Models (PLMs) acquire rich prior semantic knowledge during the pretraining phase and utilize it to enhance downstream Natural Language Processing (NLP) tasks. Entity Matching (EM), a fundamental NLP task, aims to determine whether two entity records from different knowledge bases refer to the same real-world entity. This study, for the first time, explores the potential of using a PLM to boost the EM task through two transfer learning techniques, namely, fine-tuning and prompt learning. Our work also represents the first application of the soft prompt in an EM task. Experimental results across eleven EM datasets show that the soft prompt consistently outperforms other methods in terms of F1 scores across all datasets. Additionally, this study also investigates the capability of prompt learning in few-shot learning and observes that the hard prompt achieves the highest F1 scores in both zero-shot and one-shot context. These findings underscore the effectiveness of prompt learning paradigms in tackling challenging EM tasks.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "3d430babc49023c9957edc4337436822b8b23f9a",
            "paperId": "3d430babc49023c9957edc4337436822b8b23f9a",
            "title": "Language Translator Using Deep Learning",
            "abstract": "Abstract: Digital translation (MT) is a process of translating text from one language to another using software by including both computer and language information. Initially, the MT system acquires text translation into the source language by simply associating the meaning of the words in the source language with the target language with the help of grammar. However, such methods did not produce good results due to their failure to capture the various sentence structures in the language. This process of translation is time-consuming and requires skilled craftsmen in both languages. Subsequently, integrated translation methods such as mathematical translation (SMT) and neural translation (NMT) technology have been introduced to address the challenges of legal-based approaches. MT has already shown promising results in bilingual translation. In contrast to SMT, which requires sub-components trained separately in translation, NMT uses one large neural network for training. This paper outlines how to train a repetitive neural network for rearrangement for a source to identify. The default encoder helps to reconstruct the vectors of the target language. Therefore, powerful hardware (GPU) support is required. The GPU improves system performance by reducing training time.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "ded539a2005aca58537ae33d1e20fb2fad3b5ec8",
            "paperId": "ded539a2005aca58537ae33d1e20fb2fad3b5ec8",
            "title": "mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?",
            "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of\"anchor tokens\"(i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach - multilingual pretraining with unified output space - that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        }
    ]
}