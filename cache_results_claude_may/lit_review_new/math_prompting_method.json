{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "all_queries": [
        "KeywordQuery(\"large language models mathematical problem solving prompting\")",
        "PaperQuery(\"5e5f6e3620a9b472cfa8cb81b264f350cd77d5d4\")",
        "GetReferences(\"2f3822eb380b5e753a6d579f31dfc3ec4c4a0820\")",
        "KeywordQuery(\"chain of thought prompting math reasoning\")",
        "PaperQuery(\"d2a01f9b2a565070ce64ff38eb7cdc26f3ed992a\")",
        "KeywordQuery(\"program synthesis math word problems\")",
        "PaperQuery(\"8db1dcae055842f43ccac04182957b20d15bbe6b\")",
        "GetReferences(\"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691\")"
    ],
    "paper_bank": [
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3963,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 10
        },
        {
            "id": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
            "paperId": "6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
            "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts",
            "year": 2022,
            "citationCount": 397,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoTA performance on all math problem datasets and near-SoTAperformance on financial datasets."
            },
            "score": 9
        },
        {
            "id": "efcb903bd962d1ffdd603fc540233971b5ce060f",
            "paperId": "efcb903bd962d1ffdd603fc540233971b5ce060f",
            "title": "Get an A in Math: Progressive Rectification Prompting",
            "abstract": "Chain-of-Thought (CoT) prompting methods have enabled large language models (LLMs) to generate reasoning paths and solve math word problems (MWPs). However, they are sensitive to mistakes in the paths, as any mistake can result in an incorrect answer. We propose a novel method named Progressive Rectification Prompting (PRP) to improve average accuracy on eight MWP datasets from 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a verify-then-rectify process to progressively identify incorrect answers and rectify the reasoning paths. With the most likely correct answer, the LLM predicts a masked numerical value in the question; if the prediction does not match the masked value, the answer is likely incorrect. Then the LLM is prompted to re-generate the reasoning path hinted with a set of incorrect answers to prevent itself from repeating previous mistakes. PRP achieves the best performance compared against the CoT methods. Our implementation is made publicly available at https://wzy6642.github.io/prp.github.io/.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method named Progressive Rectification Prompting (PRP) is proposed to improve average accuracy on eight MWP datasets from 77.3 to 90.5."
            },
            "score": 9
        },
        {
            "id": "f176d0d466d7c778a6435fe9a8d7e49508cb9059",
            "paperId": "f176d0d466d7c778a6435fe9a8d7e49508cb9059",
            "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
            "abstract": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework. The code is available at https://github.com/tengxiaoliu/XoT.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts by allowing method switching, provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework."
            },
            "score": 9
        },
        {
            "id": "d2a01f9b2a565070ce64ff38eb7cdc26f3ed992a",
            "paperId": "d2a01f9b2a565070ce64ff38eb7cdc26f3ed992a",
            "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
            "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.",
            "year": 2024,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, and outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute."
            },
            "score": 9
        },
        {
            "id": "8db1dcae055842f43ccac04182957b20d15bbe6b",
            "paperId": "8db1dcae055842f43ccac04182957b20d15bbe6b",
            "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
            "abstract": "While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper formally defines the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith, and proposes three novel techniques that improve performance."
            },
            "score": 9
        },
        {
            "id": "cca77ff5dd95e395ff0fc725982199340f774c6c",
            "paperId": "cca77ff5dd95e395ff0fc725982199340f774c6c",
            "title": "MathDivide: Improved mathematical reasoning by large language models",
            "abstract": "Large language models have been proven to be capable of handling complex linguistic and cognitive tasks. Therefore their usage has been extended to tasks requiring logical reasoning ability such as Mathematics. In this paper, we propose a prompting technique called MathDivide that breaks down the mathematical problem into simpler subproblems. Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM for the corresponding algebraic expression. The values fed to the Python code are the numerical values provided in the problem statement. The solutions for the subproblems are composed together to obtain the final answer for the problem statement. Finally, the final answer is compared to the correct answer. If the final answer matches the correct answer, it is produced as output else a refinement prompt is fed to the LLM. We experiment with this prompting technique on both closed-source LLM models and open-source LLM models using GSM8K dataset. The results obtained demonstrate that MathDivide was able to significantly outperform the leading prompting technique called Math-prompter.",
            "year": 2024,
            "citationCount": 0,
            "score": 9
        },
        {
            "id": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
            "paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
            "title": "PAL: Program-aided Language Models",
            "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
            "year": 2022,
            "citationCount": 275,
            "score": 9
        },
        {
            "id": "5e5f6e3620a9b472cfa8cb81b264f350cd77d5d4",
            "paperId": "5e5f6e3620a9b472cfa8cb81b264f350cd77d5d4",
            "title": "Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) still grapple with complex tasks like mathematical reasoning. Despite significant efforts invested in improving prefix prompts or reasoning process, the crucial role of problem context might have been neglected. Accurate recognition of inputs is fundamental for solving mathematical tasks, as ill-formed problems could potentially mislead LLM's reasoning. In this study, we propose a new approach named Problem Elaboration Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically, PEP decomposes and elucidates the problem context before reasoning, therefore enhancing the context modeling and parsing efficiency. Experiments across datasets and models demonstrate promising performances: (1) PEP demonstrates an overall enhancement in various mathematical tasks. For instance, with the GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through greedy decoding and self-consistency, respectively. (2) PEP can be easily implemented and integrated with other prompting methods. (3) PEP shows particular strength in handling distraction problems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new approach named Problem Elaboration Prompting (PEP) is proposed to enhance the mathematical capacities of LLMs by decomposing and elucidates the problem context before reasoning, therefore enhancing the context modeling and parsing efficiency."
            },
            "score": 8
        },
        {
            "id": "5437e8adab596d7294124c0e798708e050e25321",
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "citationCount": 633,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts."
            },
            "score": 8
        },
        {
            "id": "36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
            "paperId": "36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
            "title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "abstract": "Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method called MindStar (M*), which treats reasoning tasks as search problems. This method utilizes a step-wise reasoning approach to navigate the tree space. To enhance search efficiency, we propose two tree-search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "dd3dd5e7e3a2718336039112a3f734347da5c1f4",
            "paperId": "dd3dd5e7e3a2718336039112a3f734347da5c1f4",
            "title": "Achieving>97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems",
            "abstract": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large Language Models (LLMs) across various reasoning tasks. However, CoT still falls short in dealing with complex math word problems, as it usually suffers from three pitfalls: semantic misunderstanding errors, calculation errors and step-missing errors. Prior studies involve addressing the calculation errors and step-missing errors, but neglect the semantic misunderstanding errors, which is the major factor limiting the LLMs' performance. To this end, we propose a simple-yet-effective method, namely Deeply Understanding the Problems (DUP), to improve the LLMs' math problem-solving ability by addressing semantic misunderstanding errors. The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning. Extensive experiments on 10 diverse reasoning benchmarks show that our DUP method consistently outperforms the other counterparts by a large margin. More encouragingly, DUP achieves a new SOTA result on the GSM8K benchmark, with an accuracy of 97.1% under zero-shot setting.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 140,
            "score": 8
        },
        {
            "id": "b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
            "paperId": "b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
            "title": "Faithful Chain-of-Thought Reasoning",
            "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",
            "year": 2023,
            "citationCount": 108,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Faithful CoT, a reasoning framework involving two stages: Translation and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively, guarantees that the reasoning chain provides a faithful explanation of the final answer."
            },
            "score": 8
        },
        {
            "id": "d75d11d2c89c01cd284383546ae057cb827dc272",
            "paperId": "d75d11d2c89c01cd284383546ae057cb827dc272",
            "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
            "abstract": "Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language processing tasks, but designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in a few-shot prompting scenario: dynamic program prompting and program distillation. Our approach is largely inspired by Gao et al., (2022), where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving. Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data. Our experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning. Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in MWP solving.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in MWP solving."
            },
            "score": 8
        },
        {
            "id": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
            "paperId": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
            "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
            "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
            "year": 2023,
            "citationCount": 45,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SelfCheck, a general-purpose zero-shot verification schema for recognizing errors in large language models and uses the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question."
            },
            "score": 8
        },
        {
            "id": "57100e39d0413ee585b381ba9ab366e8a6cf2866",
            "paperId": "57100e39d0413ee585b381ba9ab366e8a6cf2866",
            "title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
            "abstract": "Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when interfacing with an external tool for solving complex math word problems. Our data and prompts are publicly available at https://github.com/joyheyueya/declarative-math-word-problem.",
            "year": 2023,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations."
            },
            "score": 8
        },
        {
            "id": "91e769074849dbb60634671e01a59479334d6570",
            "paperId": "91e769074849dbb60634671e01a59479334d6570",
            "title": "Math Problem Solving: Enhancing Large Language Models with Semantically Rich Symbolic Variables",
            "abstract": "The advent of Large Language Models (LLMs) based on the Transformer architecture has led to remarkable advancements in various domains, including reasoning tasks. However, accurately assessing the performance of Large Language Models, particularly in the reasoning domain, remains a challenge. In this paper, we propose the Semantically Rich Variable Substitution Method (SemRiVas) as an enhancement to existing symbolic methodologies for evaluating LLMs on Mathematical Word Problems (MWPs). Unlike previous approaches that utilize generic symbols for variable substitution, SemRiVas employs descriptive variable names, aiming to improve the problem-solving abilities of LLMs. Our method aims to eliminate the need for LLMs to possess programming proficiency and perform arithmetic operations, to be universally applicable. Our experimental results demonstrate the superior accuracy of SemRiVas compared to prior symbolic methods, particularly in resolving longer and more complex MWP questions. However, LLMs\u2019 performance with SemRiVas and symbolic methods that utilize one-character variables still falls short compared to notable techniques like CoT and PaL.",
            "year": 2024,
            "citationCount": 0,
            "score": 8
        },
        {
            "id": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
            "paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
            "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",
            "year": 2023,
            "citationCount": 874,
            "score": 8
        },
        {
            "id": "b626560f19f815808a289ef5c24a17c57320da70",
            "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
            "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
            "year": 2023,
            "citationCount": 99,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions, using the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results."
            },
            "score": 7
        },
        {
            "id": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
            "year": 2023,
            "citationCount": 697,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving."
            },
            "score": 7
        },
        {
            "id": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "paperId": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "title": "Large Language Models as Analogical Reasoners",
            "abstract": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that this approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench."
            },
            "score": 7
        },
        {
            "id": "472b3dedc4eec85c53b53becee06d953f29afcc8",
            "paperId": "472b3dedc4eec85c53b53becee06d953f29afcc8",
            "title": "General Purpose Verification for Chain of Thought Prompting",
            "abstract": "Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "d59e4560e3f3b8d53b0660e4ca2d6ea8d559787b",
            "paperId": "d59e4560e3f3b8d53b0660e4ca2d6ea8d559787b",
            "title": "Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models",
            "abstract": "Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
            "year": 2023,
            "citationCount": 604,
            "score": 7
        },
        {
            "id": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "paperId": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
            "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
            "year": 2023,
            "citationCount": 186,
            "score": 7
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1556,
            "score": 7
        },
        {
            "id": "e9aef5349df9eaf151b6cbdb60a2e269206e70b7",
            "paperId": "e9aef5349df9eaf151b6cbdb60a2e269206e70b7",
            "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought",
            "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.",
            "year": 2024,
            "citationCount": 1,
            "tldr": null,
            "score": 7
        },
        {
            "id": "3674dc2db34c68d12b74773768464056da48bbae",
            "paperId": "3674dc2db34c68d12b74773768464056da48bbae",
            "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models",
            "abstract": "In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Comparing standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy provides more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs."
            },
            "score": 7
        },
        {
            "id": "7654dbd372d8b65e730e3bd477ff9fec96c16dc5",
            "paperId": "7654dbd372d8b65e730e3bd477ff9fec96c16dc5",
            "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks",
            "abstract": "Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem\u2019s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods.",
            "year": 2021,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Neural-Symbolic Solver (NS-Solver) is proposed to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions."
            },
            "score": 7
        },
        {
            "id": "fddabcca94c3303da48745fd4327b58cdd8c1185",
            "paperId": "fddabcca94c3303da48745fd4327b58cdd8c1185",
            "title": "Recycling Numeracy Data Augmentation with Symbolic Verification for Math Word Problem Solving",
            "abstract": "Most studies of automatic math word problem solving rely on a dataset for training the model that transforms a question into the corresponding answer directly, or translates the question into a sequence of operations that form a program to derive the answer. The program serving as the intermediate symbolic form between the question and the answer provides more information for the model to learn arithmetic reasoning. However, manually composing the programs for numerous questions is a labor-intensive work, resulting in only one medium-sized dataset, MathQA, is available. This work proposes a novel recycling numeracy data augmentation (RNDA) approach that automatically generates high quality training instances in the MathQA style. Experimental results show that the model trained on the augmented data achieves the state-of-the-art performance. We will release the dataset as a resource for the research community.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel recycling numeracy data augmentation (RNDA) approach that automatically generates high quality training instances in the MathQA style and shows that the model trained on the augmented data achieves the state-of-the-art performance."
            },
            "score": 7
        },
        {
            "id": "7bddf68afbdfe0b2245aed312c0255fb486da95b",
            "paperId": "7bddf68afbdfe0b2245aed312c0255fb486da95b",
            "title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models",
            "abstract": "This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.",
            "year": 2024,
            "citationCount": 0,
            "score": 7
        },
        {
            "id": "663a41c866d49ce052801fbc88947d39764cad29",
            "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
            "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
            "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",
            "year": 2022,
            "citationCount": 456,
            "score": 7
        },
        {
            "id": "a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6",
            "paperId": "a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6",
            "title": "Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering",
            "abstract": "We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning."
            },
            "score": 6
        },
        {
            "id": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
            "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas by dynamically identifying and simulating different personas based on task inputs, which unleashes the potential of cognitive synergy in LLMs."
            },
            "score": 6
        },
        {
            "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
            "year": 2023,
            "citationCount": 246,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description."
            },
            "score": 6
        },
        {
            "id": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
            "paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
            "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks",
            "year": 2023,
            "citationCount": 215,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Graph of Thoughts is introduced: a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts, and is ensured that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes."
            },
            "score": 6
        },
        {
            "id": "b8d0ad089cb774dec70a29c21f23a4526b06cf72",
            "paperId": "b8d0ad089cb774dec70a29c21f23a4526b06cf72",
            "title": "Improving performance in large language models through diversity of thoughts",
            "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities but still lack in solving complex reasoning tasks. To enhance performance, the DyLAN framework Liu et al. (2023) introduces an interaction architecture where an LLM is prompted to assume \u2018multiple roles,\u2019 fostering diversity in approaches to problem-solving. This method has achieved accuracy improvements of 13.0% , 13.3% , and up to 25.0% on the MATH , HumanEval , and MMLU datasets, respectively, for mathematical, code generation, and general reasoning tasks. However, it incurs high computational costs, especially in terms of API calls/prompts. Our project aims to internalize the DyLAN framework\u2019s \u2018diversity of thought\u2019 to reduce inference numbers, allowing the LLM to mimic human problem-solving approaches more efficiently. We fine-tuned a LLaMA-2-7B model using a custom-generated dataset consisting of 811 MATH problems with solutions from four distinct roles. Our approach improved accuracy from approximately 6% to 10% on the MATH dataset and from 7.5% to 15% on the MMLU dataset, indicating successful generalization in a zero-shot setting. In summary, by fine-tuning the LLaMA-2-7B model with a dataset that incorporates \"diversity of thought,\" we achieved a 56% improvement on the MATH dataset and a 77% improvement on MMLU tasks compared to the baseline. The source code supporting the findings of this study has been made openly available at the following repository on GitHub",
            "year": null,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "906f4a3c6d5fe64c1e8d107013713abe2942b630",
            "paperId": "906f4a3c6d5fe64c1e8d107013713abe2942b630",
            "title": "Puzzle in a Haystack: Understanding & Enhancing Long Context Reasoning",
            "abstract": "As the context windows of Large Language Models (LLMs) increase, they have the ability of accepting entire novels to textbooks as input. The popular Needle in a Haystack (NIAH) evaluation, while providing a minimum standard for evaluating this long context performance, falls short of assessing the more important reasoning and information synthesis capabilities of possible with long context LLMs. Hence, we introduce the Progressive Needles Test: a simple logic puzzle to evaluate a model\u2019s ability to reason over, synthesize, and deduce information from multiple parts of its inputted context. In the Progressive Needles test, we place information relevant to a query (\"needles\") within a larger text of thematically related but irrelevant information (\"haystack\"). The needles are logically connected to one another, necessitating the models to engage in deep reasoning to extract and synthesize this scattered information to arrive at the correct answer. We generate Progressive Needles questions for haystacks for both natural language numerical/mathematical reasoning tasks as well as code tasks, the latter simulating chained function calls across code bases. We find that LLMs like GPT-4, GPT-3.5, and Mixtal exhibit a marked decline in performance on the Progressive Needles test when the size of the haystack is increased and queries are made to require more complex reasoning, exposing gaps both within current long context benchmarks and weaknesses in LLM\u2019s reasoning abilities. By fine-tuning GPT-3.5 on the Progressive Needles tasks, we also demonstrate that learning to solve Progressive Needles tasks leads to a tangible improvement of \u223c 2% in performance on the real-world QuALITY benchmark, suggesting that our task helps enhance LLM reasoning capabilities and other real world tasks.",
            "year": null,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "55a06fcf751864e698103ab6f5aa918060e48896",
            "paperId": "55a06fcf751864e698103ab6f5aa918060e48896",
            "title": "Enhancing Large Language Models through Transforming Reasoning Problems into Classification Tasks",
            "abstract": "In this paper, we introduce a novel approach for enhancing the reasoning capabilities of large language models (LLMs) for constraint satisfaction problems (CSPs), by converting reasoning problems into classification tasks. Our method leverages the LLM\u2019s ability to decide when to call a function from a set of logical-linguistic primitives, each of which can interact with a local \u201cscratchpad\u201d memory and logical inference engine. Invocation of these primitives in the correct order writes the constraints to the scratchpad memory and enables the logical engine to verifiably solve the problem. We additionally propose a formal framework for exploring the \u201clinguistic\u201d hardness of CSP reasoning-problems for LLMs. Our experimental results demonstrate that under our proposed method, tasks with significant computational hardness can be converted to a form that is easier for LLMs to solve and yields a 40% improvement over baselines. This opens up new avenues for future research into hybrid cognitive models that integrate symbolic and neural approaches.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "title": "Teaching Large Language Models to Self-Debug",
            "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
            "year": 2023,
            "citationCount": 309,
            "score": 6
        },
        {
            "id": "407b9e9478ba6bff43ce4b20e8b6cb2b303477d2",
            "paperId": "407b9e9478ba6bff43ce4b20e8b6cb2b303477d2",
            "title": "Planning with Large Language Models for Code Generation",
            "abstract": "Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner to generate candidate programs and test them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.",
            "year": 2023,
            "citationCount": 53,
            "score": 6
        },
        {
            "id": "50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f",
            "paperId": "50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f",
            "title": "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations",
            "abstract": "Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.",
            "year": 2022,
            "citationCount": 141,
            "score": 6
        },
        {
            "id": "e40b8729a9da1ff24aeaa81518305350a2c375cf",
            "paperId": "e40b8729a9da1ff24aeaa81518305350a2c375cf",
            "title": "Latent Skill Discovery for Chain-of-Thought Reasoning",
            "abstract": "Recent advances in Large Language Models (LLMs) have led to an emergent ability of chain-of-thought (CoT) prompting, a prompt reasoning strategy that adds intermediate rationale steps between questions and answers to construct prompts. Conditioned on these prompts, LLMs can effectively learn in context to generate rationales that lead to more accurate answers than when answering the same question directly. To design LLM prompts, one important setting, called demonstration selection, considers selecting demonstrations from an example bank. Existing methods use various heuristics for this selection, but for CoT prompting, which involves unique rationales, it is essential to base the selection upon the intrinsic skills that CoT rationales need, for instance, the skills of addition or subtraction for math word problems. To address this requirement, we introduce a novel approach named Reasoning Skill Discovery (RSD) that use unsupervised learning to create a latent space representation of rationales, called a reasoning skill. Simultaneously, RSD learns a reasoning policy to determine the required reasoning skill for a given question. This can then guide the selection of examples that demonstrate the required reasoning skills. Our approach offers several desirable properties: it is (1) theoretically grounded, (2) sample-efficient, requiring no LLM inference or manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperforms existing methods by up to 6% in terms of the answer accuracy across multiple reasoning tasks.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach named Reasoning Skill Discovery (RSD) that use unsupervised learning to create a latent space representation of rationales, called a reasoning skill, which outperforms existing methods by up to 6% in terms of the answer accuracy across multiple reasoning tasks."
            },
            "score": 6
        },
        {
            "id": "587f22e4e04d77ba0750deea69192fbfb73d7435",
            "paperId": "587f22e4e04d77ba0750deea69192fbfb73d7435",
            "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
            "abstract": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models~(LLMs) to perform step-by-step reasoning on complex math-related tasks. However, most existing math reasoning datasets may be not able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they may only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps. To address the issue, we construct \\textbf{CARP}, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps. In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from the perspectives of tool manipulation and natural language reasoning, until obtaining converged solutions or reaching the maximum turn. Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods. Our data and code are available in \\url{https://github.com/RUCAIBox/CARP}.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods."
            },
            "score": 6
        },
        {
            "id": "99d4f4430461a38241617df52bc05717fdb851df",
            "paperId": "99d4f4430461a38241617df52bc05717fdb851df",
            "title": "Large Language Models Can Self-Correct with Minimal Effort",
            "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct.",
            "year": 2024,
            "citationCount": 1,
            "score": 6
        },
        {
            "id": "d04ab82edfec9718997f609f3466bd9bc6491d15",
            "paperId": "d04ab82edfec9718997f609f3466bd9bc6491d15",
            "title": "Does ChatGPT Comprehend the Place Value in Numbers When Solving Math Word Problems?",
            "abstract": "With the recent advancements in GPT, there\u2019s been a growing trend to integrate GPT in solving math word problems using strategies such as \u201cChain-of-Thought\u201d(CoT) and \u201cProgram-of-Thought\u201d(PoT). Based on the observation that CoT tends to yield lower accuracy than PoT when large numbers are involved, we conducted two experiments to examine whether chatGPT understands place values in numbers. In the first experiment, to examine whether GPT can correctly order numbers based on an understanding of place values, we order source and permutation multiplets that contain 3-6 numbers in base 1000. In the second experiment, We examine whether GPT based models that utilize English expressions (CoT_Eng and PoT_Eng), rather than numerical expressions (CoT_Num and PoT_Num) can yield better performance in solving math word problems. The results of the first experiment showed that the ordering accuracy for the permutation multiplets (6 elements = 60.5%) was lower than that of source multiplets (6 elements = 96.8%). The results of the second experiment showed that accuracy increased when information about the place value was provided explicitly in the format of English expression (79.9% in CoT, 82% in PoT) compared to numerical expression(76.8% in CoT, 80% in PoT). The observations from both experiments suggest that the concept of place value isn\u2019t adequately integrated when numbers are represented as tokens in gpt3.5-turbo. Thus, research on training models to understand the concept of place value in numbers would be a possible direction to pursue as future research.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The observations from both experiments suggest that the concept of place value isn\u2019t adequately integrated when numbers are represented as tokens in gpt3.5-turbo, and research on training models to understand the conceptof place value in numbers would be a possible direction to pursue as future research."
            },
            "score": 6
        },
        {
            "id": "d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b",
            "paperId": "d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b",
            "title": "KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains",
            "abstract": "We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems. Compared to prior works, this study features three core advancements. First, KnowledgeMath includes 1,259 problems with a hybrid of textual and tabular content and require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves only 45.4% accuracy, leaving substantial room for improvement. While knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0% for GPT-3.5), it is still significantly lower the estimated human expert performance of 94%. We believe that KnowledgeMath can facilitate future research on domain-specific knowledge retrieval and augmentation into the math word problem-solving process. We will release the benchmark and code at https://github.com/yale-nlp/KnowledgeMath.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems, and provides expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment."
            },
            "score": 6
        },
        {
            "id": "eef7cfe8267954adbb4675576072a1d80ca7a3a8",
            "paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8",
            "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
            "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",
            "year": 2019,
            "citationCount": 314,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs and a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models."
            },
            "score": 6
        },
        {
            "id": "490cd30b2e0bd83cdc62d28ffdd4279931522b9c",
            "paperId": "490cd30b2e0bd83cdc62d28ffdd4279931522b9c",
            "title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction",
            "abstract": "Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1) Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers. However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones. Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction. Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction. In experiments, DOP has shown outstanding performance, highlighting its significant impact. We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners. Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct.",
            "year": 2024,
            "citationCount": 0,
            "score": 6
        },
        {
            "id": "0d42221038c05cee8443c5b5af838505ee137dc3",
            "paperId": "0d42221038c05cee8443c5b5af838505ee137dc3",
            "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
            "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.",
            "year": 2023,
            "citationCount": 85,
            "score": 6
        },
        {
            "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "citationCount": 2089,
            "score": 6
        },
        {
            "id": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
            "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
            "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
            "year": 2023,
            "citationCount": 83,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on two popular benchmarks for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin."
            },
            "score": 5
        },
        {
            "id": "2d77b7203824e617206634277bce7eec2b71a2bd",
            "paperId": "2d77b7203824e617206634277bce7eec2b71a2bd",
            "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
            "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "dc068165839ecd0e74eff9c85356b419033a3484",
            "paperId": "dc068165839ecd0e74eff9c85356b419033a3484",
            "title": "MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions",
            "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "c7135a1ea0d979719dbf7a8a394673dc92ea2a4b",
            "paperId": "c7135a1ea0d979719dbf7a8a394673dc92ea2a4b",
            "title": "Reasoning on Efficient Knowledge Paths: Knowledge Graph Guides Large Language Model for Domain Question Answering",
            "abstract": "Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform surprisingly well and outperform human experts on many tasks. However, in many domain-specific evaluations, these LLMs often suffer from hallucination problems due to insufficient training of relevant corpus. Furthermore, fine-tuning large models may face problems such as the LLMs are not open source or the construction of high-quality domain instruction is difficult. Therefore, structured knowledge databases such as knowledge graph can better provide domain background knowledge for LLMs and make full use of the reasoning and analysis capabilities of LLMs. In some previous works, LLM was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question. Especially for the question that require a multi-hop reasoning path, frequent calls to LLM will consume a lot of computing power. Moreover, when choosing the reasoning path, LLM will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps. In this paper, we integrated and optimized a pipeline for selecting reasoning paths from KG based on LLM, which can reduce the dependency on LLM. In addition, we propose a simple and effective subgraph retrieval method based on chain of thought (CoT) and page rank which can returns the paths most likely to contain the answer. We conduct experiments on three datasets: GenMedGPT-5k [14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using fewer LLM calls can achieve the same results as previous SOTAs models.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
            "paperId": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
            "title": "Language Models can Solve Computer Tasks",
            "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.",
            "year": 2023,
            "citationCount": 180,
            "score": 5
        },
        {
            "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
            "year": 2022,
            "citationCount": 526,
            "score": 5
        },
        {
            "id": "c2260403fd5cb2de73491323433e48b6ec36872c",
            "paperId": "c2260403fd5cb2de73491323433e48b6ec36872c",
            "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective",
            "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.",
            "year": 2023,
            "citationCount": 62,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is proved by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format, and LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks."
            },
            "score": 5
        },
        {
            "id": "7d1a15ee105f9a355adf8d5d0ef95e7f85a0317f",
            "paperId": "7d1a15ee105f9a355adf8d5d0ef95e7f85a0317f",
            "title": "A comparison of chain-of-thought reasoning strategies across datasets and models",
            "abstract": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improve the performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study, we compare different reasoning strategies induced by zero-shot prompting across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge). We test them on six question-answering datasets that require real-world knowledge application and logical verbal reasoning, including datasets from scientific and medical domains. Our findings demonstrate that while some variations in effectiveness occur, gains from CoT reasoning strategies remain robust across different models and datasets. GPT-4 benefits the most from current state-of-the-art reasoning strategies and performs best by applying a prompt previously discovered through automated discovery.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "ed319f9be78419b68f9e32a0e7602fc1cad3ccc1",
            "paperId": "ed319f9be78419b68f9e32a0e7602fc1cad3ccc1",
            "title": "More Effectively Searching Trees of Thought for Increased Reasoning Ability in Large Language Models",
            "abstract": "Large Language Models (LLMs) are capable of completing problem-solving tasks requiring different types of reasoning, but rely on token-level autogressive mechanisms for text generation and are limited in their ability to reason through complex, multi-step problems. The Tree of Thoughts framework extends Chain of Thought reasoning in LLMs to integrate a variety of possible options at each stage of reasoning. We introduce a new framework that extends the Tree of Thoughts approach by applying a separate value function that can more effectively evaluate reasoning paths and incorporating exploration from Monte Carlo Tree Search (MCTS). We find that vanilla tree of thoughts greatly outperforms a chain of thought based approach on the Game of 24, a task requiring non-trivial search through reasoning paths. However, upon incorporation of a separate finetuned value function, we find that the chain of thoughts based approach is actually able to match the solve rates of MCTS and BFS which use the LLM as a value function, but at a fraction (1/7) of the runtime. Consequently, we find that the efficacy and efficiency of a Tree of Thoughts reasoning is more contingent on the efficacy of the method to propose and evaluate paths rather than the search strategy through the paths themselves.",
            "year": null,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "257aee73d83a87921fd2d56b524de394dcf6a264",
            "paperId": "257aee73d83a87921fd2d56b524de394dcf6a264",
            "title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
            "abstract": "Significance We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest Massachusetts Institute of Technology (MIT) mathematics courses at a human level. Our methods combine three innovations: 1) using recent neural networks pretrained on text and fine-tuned on code rather than pretrained on text; 2) few-shot learning synthesizing programs that correctly solve course problems automatically; and 3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include roles of artificial intelligence (AI) in automated course evaluation and content generation.",
            "year": 2021,
            "citationCount": 110,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude."
            },
            "score": 5
        },
        {
            "id": "ed3f3ccdac66357043ab4e94d6467c61b0d36729",
            "paperId": "ed3f3ccdac66357043ab4e94d6467c61b0d36729",
            "title": "Towards Human-aligned Evaluation for Linear Programming Word Problems",
            "abstract": "Math Word Problem (MWP) is a crucial NLP task aimed at providing solutions for given mathematical descriptions. A notable sub-category of MWP is the Linear Programming Word Problem (LPWP), which holds significant relevance in real-world decision-making and operations research. While the recent rise of generative large language models (LLMs) has brought more advanced solutions to LPWPs, existing evaluation methodologies for this task still diverge from human judgment and face challenges in recognizing mathematically equivalent answers. In this paper, we introduce a novel evaluation metric rooted in graph edit distance, featuring benefits such as permutation invariance and more accurate program equivalence identification. Human evaluations empirically validate the superior efficacy of our proposed metric when particularly assessing LLM-based solutions for LPWP.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel evaluation metric rooted in graph edit distance is introduced, featuring benefits such as permutation invariance and more accurate program equivalence identification and human evaluations empirically validate the superior efficacy of the proposed metric when particularly assessing LLM-based solutions for LPWP."
            },
            "score": 5
        },
        {
            "id": "6d38487e0e644f78d8827513c89230f239faa11f",
            "paperId": "6d38487e0e644f78d8827513c89230f239faa11f",
            "title": "Mapping probability word problems to executable representations",
            "abstract": "While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system to provide the answer. Our best performing model incorporates general-domain contextualised word representations that were finetuned using transfer learning on another in-domain dataset. We also apply end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems.",
            "year": 2021,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper employs and analyse various neural models for answering probability word problems and applies end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems."
            },
            "score": 5
        },
        {
            "id": "37d641537819490558b963a79f6e73ec14944df0",
            "paperId": "37d641537819490558b963a79f6e73ec14944df0",
            "title": "Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models",
            "abstract": "Chain of thought finetuning aims to endow small student models with reasoning capacity to improve their performance towards a specific task by allowing them to imitate the reasoning procedure of large language models (LLMs) beyond simply predicting the answer to the question. However, the existing methods 1) generate rationale before the answer, making their answer correctness sensitive to the hallucination in the rationale;2) force the student model to repeat the exact LLMs rationale expression word-after-word, which could have the model biased towards learning the expression in rationale but count against the model from understanding the core logic behind it. Therefore, we propose a robust Post-Semantic-Thinking (PST) strategy to generate answers before rationale. Thanks to this answer-first setting, 1) the answering procedure can escape from the adverse effects caused by hallucinations in the rationale; 2) the complex reasoning procedure is tightly bound with the relatively concise answer, making the reasoning for questions easier with the prior information in the answer; 3) the efficiency of the method can also benefit from the setting since users can stop the generation right after answers are outputted when inference is conducted. Furthermore, the PST strategy loose the constraint against the generated rationale to be close to the LLMs gold standard in the hidden semantic space instead of the vocabulary space, thus making the small student model better comprehend the semantic reasoning logic in rationale. Extensive experiments conducted across 12 reasoning tasks demonstrate the effectiveness of PST.",
            "year": 2024,
            "citationCount": 0,
            "score": 5
        },
        {
            "id": "3e565c544a8639cc9c7568833e484d7610f5e5d4",
            "paperId": "3e565c544a8639cc9c7568833e484d7610f5e5d4",
            "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
            "abstract": "Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",
            "year": 2022,
            "citationCount": 126,
            "score": 5
        },
        {
            "id": "1fed8d06d948ff614b895085ccde0d884a226679",
            "paperId": "1fed8d06d948ff614b895085ccde0d884a226679",
            "title": "Chain-of-Thoughts Prompting with Language Models for Accurate Math Problem-Solving",
            "abstract": "Large Language Models (LLMs) have gained usage across various domains, especially in education. However, the current state-of-the-art LLMs fail in numerical calculations due to their reliance on the pre-trained dataset that does not focus on mathematical oversight. Prompting is crucial to guide LLMs to yield desired outputs for mathematical problems. This paper explores a new Chain-of-Thoughts (CoT) prompting framework, leveraging Python-based tools like LLM Math, LLM symbolic math, and SerpAPI. We also evaluate the existing works with the CoT prompting framework for math problem-solving. Students can utilize this framework to obtain more precise solutions and comprehensive explanations for their queries.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores a new Chain-of-Thoughts (CoT) prompting framework, leveraging Python-based tools like LLM Math, LLM symbolic math, and SerpAPI, and evaluates the existing works with the CoT prompting framework for math problem-solving."
            },
            "score": 4
        },
        {
            "id": "b7f9f6ac44fee822c692cdc1147c852a150f4aea",
            "paperId": "b7f9f6ac44fee822c692cdc1147c852a150f4aea",
            "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
            "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro."
            },
            "score": 4
        },
        {
            "id": "36f46391b00a7eb4ffc991f964a36b264811057d",
            "paperId": "36f46391b00a7eb4ffc991f964a36b264811057d",
            "title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
            "abstract": "The rapid progress in the field of natural language processing (NLP) systems and the expansion of large language models (LLMs) have opened up numerous opportunities in the field of education and instructional methods. These advancements offer the potential for tailored learning experiences and immediate feedback, all delivered through accessible and cost-effective services. One notable application area for this technological advancement is in the realm of solving mathematical problems. Mathematical problem-solving not only requires the ability to decipher complex problem statements but also the skill to perform precise arithmetic calculations at each step of the problem-solving process. However, the evaluation of the arithmetic capabilities of large language models remains an area that has received relatively little attention. In response, we introduce an extensive mathematics dataset called\"MathQuest\"sourced from the 11th and 12th standard Mathematics NCERT textbooks. This dataset encompasses mathematical challenges of varying complexity and covers a wide range of mathematical concepts. Utilizing this dataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2, WizardMath, and MAmmoTH. These fine-tuned models serve as benchmarks for evaluating their performance on our dataset. Our experiments reveal that among the three models, MAmmoTH-13B emerges as the most proficient, achieving the highest level of competence in solving the presented mathematical problems. Consequently, MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems.",
            "year": 2024,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive mathematics dataset called \"MathQuest\"ourced from the 11th and 12th standard Mathematics NCERT textbooks is introduced, and MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems."
            },
            "score": 4
        },
        {
            "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
            "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.",
            "year": 2023,
            "citationCount": 81,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving and suggests that it offers a promising avenue for faithful logical reasoning."
            },
            "score": 4
        },
        {
            "id": "886940b402e326a84687fd30925d2de66147566f",
            "paperId": "886940b402e326a84687fd30925d2de66147566f",
            "title": "On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models",
            "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the\"interleaving reasoning trace with action execution\"or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
            "paperId": "f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
            "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering",
            "abstract": "While large language models (LLMs) have achieved significant success in various applications, they often struggle with hallucinations, especially in scenarios that require deep and responsible reasoning. These issues could be partially mitigate by integrating external knowledge graphs (KG) in LLM reasoning. However, the method of their incorporation is still largely unexplored. In this paper, we propose a retrieval-exploration interactive method, FiDelis to handle intermediate steps of reasoning grounded by KGs. Specifically, we propose Path-RAG module for recalling useful intermediate knowledge from KG for LLM reasoning. We incorporate the logic and common-sense reasoning of LLMs and topological connectivity of KGs into the knowledge retrieval process, which provides more accurate recalling performance. Furthermore, we propose to leverage deductive reasoning capabilities of LLMs as a better criterion to automatically guide the reasoning process in a stepwise and generalizable manner. Deductive verification serve as precise indicators for when to cease further reasoning, thus avoiding misleading the chains of reasoning and unnecessary computation. Extensive experiments show that our method, as a training-free method with lower computational cost and better generality outperforms the existing strong baselines in three benchmarks.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "c741cc9d082dc41b0421dda2909a9e8566d2225f",
            "paperId": "c741cc9d082dc41b0421dda2909a9e8566d2225f",
            "title": "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition",
            "abstract": "Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "003ef1cd670d01af05afa0d3c72d72228f494432",
            "paperId": "003ef1cd670d01af05afa0d3c72d72228f494432",
            "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
            "abstract": "Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.",
            "year": 2023,
            "citationCount": 191,
            "score": 4
        },
        {
            "id": "f0a0e8b6e84207f50db4d24cc4016e40601214ef",
            "paperId": "f0a0e8b6e84207f50db4d24cc4016e40601214ef",
            "title": "Faithful Reasoning Using Large Language Models",
            "abstract": "Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.",
            "year": 2022,
            "citationCount": 92,
            "score": 4
        },
        {
            "id": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
            "paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
            "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
            "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
            "year": 2022,
            "citationCount": 167,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili."
            },
            "score": 4
        },
        {
            "id": "9a50a73562b5569e7e62668c1c5eba4c6a1d45f1",
            "paperId": "9a50a73562b5569e7e62668c1c5eba4c6a1d45f1",
            "title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards",
            "abstract": "Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs). However, acquiring human-authored rationales or augmenting rationales from proprietary models is costly and not scalable. In this paper, we study the problem of whether LLMs could self-improve their reasoning capabilities. To this end, we propose Self-Explore, where the LLM is tasked to explore the first wrong step (i.e., the first pit) within the rationale and use such signals as fine-grained rewards for further improvement. On the GSM8K and MATH test set, Self-Explore achieves 11.57% and 2.89% improvement on average across three LLMs compared to supervised fine-tuning (SFT). Our code is available at https://github.com/hbin0701/Self-Explore.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "ed502eae116b7ce63147d210d86bfde6bb04ba68",
            "paperId": "ed502eae116b7ce63147d210d86bfde6bb04ba68",
            "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning",
            "abstract": "Human cognition exhibits systematic compositionality, the algebraic ability to generate infinite novel combinations from finite learned components, which is the key to understanding and reasoning about complex logic. In this work, we investigate the compositionality of large language models (LLMs) in mathematical reasoning. Specifically, we construct a new dataset \\textsc{MathTrap}\\footnotemark[3] by introducing carefully designed logical traps into the problem descriptions of MATH and GSM8k. Since problems with logical flaws are quite rare in the real world, these represent ``unseen'' cases to LLMs. Solving these requires the models to systematically compose (1) the mathematical knowledge involved in the original problems with (2) knowledge related to the introduced traps. Our experiments show that while LLMs possess both components of requisite knowledge, they do not \\textbf{spontaneously} combine them to handle these novel cases. We explore several methods to mitigate this deficiency, such as natural language prompts, few-shot demonstrations, and fine-tuning. We find that LLMs' performance can be \\textbf{passively} improved through the above external intervention. Overall, systematic compositionality remains an open challenge for large language models.",
            "year": 2024,
            "citationCount": 1,
            "score": 4
        },
        {
            "id": "459bf19efc727d917eae8d25cd089b2bc159a4d4",
            "paperId": "459bf19efc727d917eae8d25cd089b2bc159a4d4",
            "title": "Prototype-then-Refine: A Neurosymbolic Approach for Improved Logical Reasoning with LLMs",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities, but even the best LLMs still struggle with complex logical reasoning problems. Recent methods like Logic-LM propose to combine LLMs with symbolic solvers to improve reasoning ability. By prompting the LLM to translate a natural language problem into a symbolic formulation and then feeding the symbolic formulation to a deterministic symbolic solver, this neuro-symbolic approach capitalizes on the ability of LLMs to understand flexible natural language while relying on the faithful and guaranteed reasoning ability of symbolic solvers. However, the Logic-LM approach fails when the translation is incorrect. To enable more robust generation of correct logic programs, we propose a new Prototype-then-Refine framework (ProRef) that applies an extensible series of prototypers to pick a strong prototype logic program which it then refines using an extensible series of refiners . On the challenging AR-LSAT dataset, we find that our framework significantly improves the executable rate of generated logic programs from 20.05% to 32.47%, enabling GPT-3.5-Turbo to almost match the executable rate of GPT-4 (32.61%). However, this does not improve the accuracy (percentage of questions answered correctly). We perform a thorough analysis of the errors in the generated programs and propose further improvements in our framework that can improve the accuracy as well as the executable rate.",
            "year": null,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "f8e0f7babe7cd858aecfafb03375bf578fe161f7",
            "paperId": "f8e0f7babe7cd858aecfafb03375bf578fe161f7",
            "title": "MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving",
            "abstract": "Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model (PLM) techniques and build MWP-BERT, an effective contextual number representation PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and several MWP-specific understanding tasks on both English and Chinese benchmarks.",
            "year": 2021,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work argues that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here and builds MWP-BERT, an effective contextual number representation PLM."
            },
            "score": 4
        },
        {
            "id": "4e40595d6ecba027cebb4f2e3b43ae44bcf51daf",
            "paperId": "4e40595d6ecba027cebb4f2e3b43ae44bcf51daf",
            "title": "Solving Linear Algebra by Program Synthesis",
            "abstract": "We solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.",
            "year": 2021,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses OpenAI Codex with zero-shot learning to synthesize code from questions with perfect accuracy by interactive program synthesis and quantifies the difference between the original question text and the transformed question text that yields a correct answer."
            },
            "score": 4
        },
        {
            "id": "28808dc21b4321b85cacac7ccc2d974632a2980e",
            "paperId": "28808dc21b4321b85cacac7ccc2d974632a2980e",
            "title": "Can LLMs Solve longer Math Word Problems Better?",
            "abstract": "Math Word Problems (MWPs) are crucial for evaluating the capability of Large Language Models (LLMs), with current research primarily focusing on questions with concise contexts. However, as real-world math problems often involve complex circumstances, LLMs' ability to solve long MWPs is vital for their applications in these scenarios, yet remains under-explored. This study pioneers the exploration of Context Length Generalizability (CoLeG), the ability of LLMs to solve long MWPs. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs with lengthy narratives. Two novel metrics are proposed to assess the efficacy and resilience of LLMs in solving these problems. Our examination of existing zero-shot prompting techniques and both proprietary and open-source LLMs reveals a general deficiency in CoLeG. To alleviate these challenges, we propose distinct approaches for different categories of LLMs. For proprietary LLMs, a new instructional prompt is proposed to mitigate the influence of long context. For open-source LLMs, a new data augmentation task is developed to improve CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing not only improved performance on E-GSM but also generalizability across several other MWP benchmarks. Our findings pave the way for future research in employing LLMs for complex, real-world applications, offering practical solutions to current limitations and opening avenues for further exploration of model generalizability and training methodologies.",
            "year": 2024,
            "citationCount": 0,
            "score": 4
        },
        {
            "id": "f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab",
            "paperId": "f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab",
            "title": "Binding Language Models in Symbolic Languages",
            "abstract": "Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .",
            "year": 2022,
            "citationCount": 130,
            "score": 4
        },
        {
            "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "paperId": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection."
            },
            "score": 3
        },
        {
            "id": "6b50f585dcaaea31a0ec4e630d652057f3f911c1",
            "paperId": "6b50f585dcaaea31a0ec4e630d652057f3f911c1",
            "title": "Mixture-of-Instructions: Comprehensive Alignment of a Large Language Model through the Mixture of Diverse System Prompting Instructions",
            "abstract": "With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models, and demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks."
            },
            "score": 3
        },
        {
            "id": "4993258852711c4e3d0011325ac3db680eae84f4",
            "paperId": "4993258852711c4e3d0011325ac3db680eae84f4",
            "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
            "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills."
            },
            "score": 3
        },
        {
            "id": "cb1addf9cefe4e96763d28437f72a3d3cbfa7225",
            "paperId": "cb1addf9cefe4e96763d28437f72a3d3cbfa7225",
            "title": "Investigating Symbolic Capabilities of Large Language Models",
            "abstract": "Prompting techniques have significantly enhanced the capabilities of Large Language Models (LLMs) across various complex tasks, including reasoning, planning, and solving math word problems. However, most research has predominantly focused on language-based reasoning and word problems, often overlooking the potential of LLMs in handling symbol-based calculations and reasoning. This study aims to bridge this gap by rigorously evaluating LLMs on a series of symbolic tasks, such as addition, multiplication, modulus arithmetic, numerical precision, and symbolic counting. Our analysis encompasses eight LLMs, including four enterprise-grade and four open-source models, of which three have been pre-trained on mathematical tasks. The assessment framework is anchored in Chomsky's Hierarchy, providing a robust measure of the computational abilities of these models. The evaluation employs minimally explained prompts alongside the zero-shot Chain of Thoughts technique, allowing models to navigate the solution process autonomously. The findings reveal a significant decline in LLMs' performance on context-free and context-sensitive symbolic tasks as the complexity, represented by the number of symbols, increases. Notably, even the fine-tuned GPT3.5 exhibits only marginal improvements, mirroring the performance trends observed in other models. Across the board, all models demonstrated a limited generalization ability on these symbol-intensive tasks. This research underscores LLMs' challenges with increasing symbolic complexity and highlights the need for specialized training, memory and architectural adjustments to enhance their proficiency in symbol-based reasoning tasks.",
            "year": 2024,
            "citationCount": 1,
            "score": 3
        },
        {
            "id": "d7c5e66318dfe6d5094e2cf9fc1ded43117095b0",
            "paperId": "d7c5e66318dfe6d5094e2cf9fc1ded43117095b0",
            "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs",
            "abstract": "While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "d487db04e4146a4fb02a2c3318042c846b62f2d3",
            "paperId": "d487db04e4146a4fb02a2c3318042c846b62f2d3",
            "title": "PECC: Problem Extraction and Coding Challenges",
            "abstract": "Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMs\u2019 capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver.",
            "year": 2024,
            "citationCount": 1,
            "score": 3
        },
        {
            "id": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
            "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
            "title": "Reasoning with Language Model is Planning with World Model",
            "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
            "year": 2023,
            "citationCount": 195,
            "score": 3
        },
        {
            "id": "2b118069a3344ef0678993dbe6b2c4d9abe75acc",
            "paperId": "2b118069a3344ef0678993dbe6b2c4d9abe75acc",
            "title": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science",
            "abstract": "This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning and a human-in-the-loop approach."
            },
            "score": 3
        },
        {
            "id": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
            "paperId": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
            "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
            "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.",
            "year": 2024,
            "citationCount": 2,
            "score": 3
        },
        {
            "id": "5c118e57b5398224ca4401c902cda33da7d29ec3",
            "paperId": "5c118e57b5398224ca4401c902cda33da7d29ec3",
            "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
            "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations. In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "4f8608bb4b5943032bd23995a5b44d04d7faba3c",
            "paperId": "4f8608bb4b5943032bd23995a5b44d04d7faba3c",
            "title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models",
            "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into Small Language Models (SLMs), with a significant focus on transferring the reasoning capabilities of LLMs to SLMs via CoT. However, the outcomes of CoT distillation are inadequate for knowledge-intensive reasoning tasks. This is because generating accurate rationales requires crucial factual knowledge, which SLMs struggle to retain due to their parameter constraints. We propose a retrieval-based CoT distillation framework, named Probe then Retrieve and Reason (PRR), which distills the question probing and reasoning capabilities from LLMs into SLMs. We train two complementary distilled SLMs, a probing model and a reasoning model, in tandem. When presented with a new question, the probing model first identifies the necessary knowledge to answer it, generating queries for retrieval. Subsequently, the reasoning model uses the retrieved knowledge to construct a step-by-step rationale for the answer. In knowledge-intensive reasoning tasks, such as StrategyQA and OpenbookQA, our distillation framework yields superior performance for SLMs compared to conventional methods, including simple CoT distillation and knowledge-augmented distillation using raw questions.",
            "year": 2024,
            "citationCount": 1,
            "score": 3
        },
        {
            "id": "5605c71465436dd3f0627d5dcd2184ea1d0c9272",
            "paperId": "5605c71465436dd3f0627d5dcd2184ea1d0c9272",
            "title": "Self-Guiding Exploration for Combinatorial Problems",
            "abstract": "Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance. Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic).",
            "year": 2024,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "53b3b15e8cbe2baf82cd0de3709e0a1e6f677415",
            "paperId": "53b3b15e8cbe2baf82cd0de3709e0a1e6f677415",
            "title": "Limits of an AI program for solving college math problems",
            "abstract": "Drori et al. (2022) report that \u201cA neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level ... [It] automatically answers 81% of university-level mathematics problems.\u201d The system they describe is indeed impressive; however, the above description is very much overstated. The work of solving the problems is done, not by a neural network, but by the symbolic algebra package Sympy. Problems of various formats are excluded from consideration. The so-called \u201cexplanations\u201d are just rewordings of lines of code. Answers are marked as correct that are not in the form speci\ufb01ed in the problem. Most seriously, it seems that in many cases the system uses the correct answer given in the test corpus to guide its path to solving the problem. Drori al. (2022) network solves, explains, and university math and",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level and automatically answers 81% of university-level mathematics problems."
            },
            "score": 3
        },
        {
            "id": "f5c13bc241e2c8e0891c2535b614a6bbffeb64aa",
            "paperId": "f5c13bc241e2c8e0891c2535b614a6bbffeb64aa",
            "title": "Domain Specific Program Synthesis",
            "abstract": "Program Synthesis refers to the task of constructing a program in a specific programming language, given its intent in a particular format. This emerging field can be applied in diverse domains and is currently being investigated with different techniques. A program synthesizer would simplify the efforts of programmers and help them focus on the program's core logic, without worrying about language syntax and other domain specifics. We applied the concepts of program synthesis in the context of solving a propositional logic word problem. We have developed a tool that is capable of understanding, parsing and evaluating a propositional logic word problem. With the user's natural language input, this tool processes the query and evaluates truth values of the question expressions. The working of the tool can be explained in three major phases: natural language processing, machine learning to obtain postfix notations of the Boolean expressions involved, and further evaluation of the postfix notations to determine the answers. Our goal was to explore the domain agnostic capabilities of our program-synthesis-based techniques of learning used in the implementation of this tool.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work has developed a tool that is capable of understanding, parsing and evaluating a propositional logic word problem and explored the domain agnostic capabilities of the program-synthesis-based techniques of learning used in the implementation of this tool."
            },
            "score": 3
        },
        {
            "id": "08a27745e0bb3e98fb0ec7e3b2443675fdecf883",
            "paperId": "08a27745e0bb3e98fb0ec7e3b2443675fdecf883",
            "title": "Mathematical Reasoning Through LLM Finetuning",
            "abstract": "Using LLMs to generate step-by-step solutions to math problems can be extremely difficult due to the logical reasoning required. In this paper, we explore several different methods to finetune LLMs for this task. We first implement Multi-Task Sequential Fine-Tuning (MTSFT) from Liu et al. (2023), which works off the observation that training language models sequentially as solution evaluators then as solution generators improves their performance. We then implement our own novel method, Logic-Enhanced Sequential Fine-Tuning (LESFT), which combines MTSFT with additional fine-tuning on the LogiQA dataset of multiple-choice deductive reasoning questions to improve the model\u2019s general logical reasoning abilities. We use the open-source Mistral-7B model and measure Pass@1 and Maj1@64 accuracy on the MATH dataset. When compared to a baseline model with no fine-tuning, we notice accuracy improvements of roughly 10% in Pass@1 accuracy and roughly 15% in Maj1@64 accuracy for both MTSFT and LESFT trained models. Upon further analysis of specific problems, we notice that fine-tuning helps the model correctly execute the logic for problems it could not initially. However, the task still remains quite difficult - when using an LESFT trained model and generating 64 solutions for each problem, upwards of 50% of problems have less than 5% of generated solutions correct.",
            "year": null,
            "citationCount": 0,
            "score": 3
        },
        {
            "id": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "paperId": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
            "year": 2023,
            "citationCount": 155,
            "score": 3
        },
        {
            "id": "40047a74b707743157051d38f76061ba5ff9aab4",
            "paperId": "40047a74b707743157051d38f76061ba5ff9aab4",
            "title": "Compositional Semantic Parsing with Large Language Models",
            "abstract": "Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",
            "year": 2022,
            "citationCount": 75,
            "score": 3
        },
        {
            "id": "f4e723958a93762befb4d4a039b44a7d752f9917",
            "paperId": "f4e723958a93762befb4d4a039b44a7d752f9917",
            "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
            "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.",
            "year": 2023,
            "citationCount": 131,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the capacity of LLMs that act as the ranking model for recommender systems, and shows that LLMs have promising zero-shot ranking abilities but struggle to perceive the order of historical interactions, and can be biased by popularity or item positions in the prompts."
            },
            "score": 2
        },
        {
            "id": "cf3522700d89af9dabfbad44c509a0fed2bde517",
            "paperId": "cf3522700d89af9dabfbad44c509a0fed2bde517",
            "title": "Human-like problem-solving abilities in large language models using ChatGPT",
            "abstract": "Backgrounds The field of Artificial Intelligence (AI) has seen a major shift in recent years due to the development of new Machine Learning (ML) models such as Generative Pre-trained Transformer (GPT). GPT has achieved previously unheard-of levels of accuracy in most computerized language processing tasks and their chat-based variations. Aim The aim of this study was to investigate the problem-solving abilities of ChatGPT using two sets of verbal insight problems, with a known performance level established by a sample of human participants. Materials and methods A total of 30 problems labeled as \u201cpractice problems\u201d and \u201ctransfer problems\u201d were administered to ChatGPT. ChatGPT's answers received a score of \u201c0\u201d for each incorrectly answered problem and a score of \u201c1\u201d for each correct response. The highest possible score for both the practice and transfer problems was 15 out of 15. The solution rate for each problem (based on a sample of 20 subjects) was used to assess and compare the performance of ChatGPT with that of human subjects. Results The study highlighted that ChatGPT can be trained in out-of-the-box thinking and demonstrated potential in solving verbal insight problems. The global performance of ChatGPT equalled the most probable outcome for the human sample in both practice problems and transfer problems as well as upon their combination. Additionally, ChatGPT answer combinations were among the 5% of most probable outcomes for the human sample both when considering practice problems and pooled problem sets. These findings demonstrate that ChatGPT performance on both set of problems was in line with the mean rate of success of human subjects, indicating that it performed reasonably well. Conclusions The use of transformer architecture and self-attention in ChatGPT may have helped to prioritize inputs while predicting, contributing to its potential in verbal insight problem-solving. ChatGPT has shown potential in solving insight problems, thus highlighting the importance of incorporating AI into psychological research. However, it is acknowledged that there are still open challenges. Indeed, further research is required to fully understand AI's capabilities and limitations in verbal problem-solving.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study highlighted that ChatGPT can be trained in out-of-the-box thinking and demonstrated potential in solving verbal insight problems, highlighting the importance of incorporating AI into psychological research."
            },
            "score": 2
        },
        {
            "id": "0e044b432911de4997e88bf1f271ecba738ee09e",
            "paperId": "0e044b432911de4997e88bf1f271ecba738ee09e",
            "title": "CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting",
            "abstract": "In the field of Question Answering (QA), unifying large language models (LLMs) with external databases has shown great success. However, these methods often fall short in providing the advanced reasoning needed for complex QA tasks. To address these issues, we improve over a novel approach called Knowledge Graph Prompting (KGP), which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy. Nevertheless, the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination. Therefore, we propose a reasoning-infused LLM agent to enhance this framework. This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search. This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the QA domain.",
            "year": 2024,
            "citationCount": 1,
            "score": 2
        },
        {
            "id": "7e1189043aeeb35ce736be3f88079859a2d887f3",
            "paperId": "7e1189043aeeb35ce736be3f88079859a2d887f3",
            "title": "Can formal argumentative reasoning enhance LLMs performances?",
            "abstract": "Recent years witnessed significant performance advancements in deep-learning-driven natural language models, with a strong focus on the development and release of Large Language Models (LLMs). These improvements resulted in better quality AI-generated output but rely on resource-expensive training and upgrading of models. Although different studies have proposed a range of techniques to enhance LLMs without retraining, none have considered computational argumentation as an option. This is a missed opportunity since computational argumentation is an intuitive mechanism that formally captures agents' interactions and the information conflict that may arise during such interplays, and so it seems well-suited for boosting the reasoning and conversational abilities of LLMs in a seamless manner. In this paper, we present a pipeline (MQArgEng) and preliminary study to evaluate the effect of introducing computational argumentation semantics on the performance of LLMs. Our experiment's goal was to provide a proof-of-concept and a feasibility analysis in order to foster (or deter) future research towards a fully-fledged argumentation engine plugin for LLMs. Exploratory results using the MT-Bench indicate that MQArgEng provides a moderate performance gain in most of the examined topical categories and, as such, show promise and warrant further research.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "3edfe647807c2a6c71ecb8d11e9a3b8a736855ee",
            "paperId": "3edfe647807c2a6c71ecb8d11e9a3b8a736855ee",
            "title": "Large Language Model Programs",
            "abstract": "In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.",
            "year": 2023,
            "citationCount": 7,
            "score": 2
        },
        {
            "id": "88a9924ba302f9c5c8065d9e38798e9506c5b171",
            "paperId": "88a9924ba302f9c5c8065d9e38798e9506c5b171",
            "title": "CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning",
            "abstract": "Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL, through trial-and-error learning in human conversations, is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.",
            "year": 2022,
            "citationCount": 31,
            "score": 2
        },
        {
            "id": "b7f5a44c2f7d0d134cd3456dde2a6d0d46091c66",
            "paperId": "b7f5a44c2f7d0d134cd3456dde2a6d0d46091c66",
            "title": "Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting",
            "abstract": "Large Language Models, such as Generative Pre-trained Transformer 3 (aka. GPT-3), have been developed to understand language through the analysis of extensive text data, allowing them to identify patterns and connections between words. While LLMs have demonstrated impressive performance across various text-related tasks, they encounter challenges in tasks associated with reasoning. To address this challenge, Chain of Thought(CoT) prompting method has been proposed as a means to enhance LLMs' proficiency in complex reasoning tasks like solving math word problems and answering questions based on logical argumentative reasoning. The primary aim of this research is to assess how well four language models can grade reflective essays of third-year medical students. The assessment will specifically target the evaluation of critical thinking skills using CoT prompting. The research will provide the following contributions; to introduce and educate on the process of instructing models to evaluate reflective essays from a dataset they have not been previously trained on; to illustrate the use of CoT prompting as an instructional approach for training large models to carry out particular tasks. Our results suggest that among all the models, Llama-7b performs the least effectively, displaying the highest mean squared error. Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value of 0.53. Lastly, it's important to note that the selected models do prioritise user privacy by allowing users to delete their own conducted conversations.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that among all the models, Llama-7b performs the least effectively, displaying the highest mean squared error, and ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value."
            },
            "score": 2
        },
        {
            "id": "810226ed665b3a9693b7171fcdb244bb3bb31acf",
            "paperId": "810226ed665b3a9693b7171fcdb244bb3bb31acf",
            "title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning",
            "abstract": "Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new prompting framework is proposed, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals, and shows that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%."
            },
            "score": 2
        },
        {
            "id": "1b1265a7fc7debcdd0cd92fc1d9b51fc55d57fdc",
            "paperId": "1b1265a7fc7debcdd0cd92fc1d9b51fc55d57fdc",
            "title": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
            "abstract": "While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "13aaec148290a633a6d5533768420cd52767709f",
            "paperId": "13aaec148290a633a6d5533768420cd52767709f",
            "title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning",
            "abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "e96ca94389eeb69f5feb7de6669ab48af6d5093f",
            "paperId": "e96ca94389eeb69f5feb7de6669ab48af6d5093f",
            "title": "Learner-generated drawing as a learning strategy. The effect of teacher-guided intervention program \u201cLearning with Understanding\u201d on composing drawings in math word problems in the primary grades",
            "abstract": "The study aimed to examine the possibility of teaching primary school students a learner-generated drawing strategy, among other constructivist learning strategies. The teacher-guided program \u201cLearning with Understanding\u201d began by discussing the broader topics of the learning process, followed by teaching specific strategies, and ended with an overview of all strategies and reflective discussions. During 18 program lessons, primary school teachers taught, practiced, and raised metacognitive awareness of three learning strategies\u2014elaboration of new information with familiar material and daily practice, organization of material into categories and elaboration, and organization of information through drawing. This study examined composing drawings for math word problems before and after the program. The sample consisted of second- and fourth-grade students from eight Estonian schools. The intervention group included 110 students from second grade and 80 students from fourth grade. The control group consisted of 121 second-grade students, and 82 fourth-grade students. Before and after the intervention, students had to solve two math word problems and compose a drawing, if needed. The results showed that before the intervention, neither the control group nor the intervention group students drew almost any drawings. However, after the intervention, both the control group and the intervention group students started to draw more drawings. Also, the intervention group students composed both more drawings and more schematic drawings. The effect of the intervention was visible at both grade levels. Comparing the correctness of answers with the drawing type showed that the fourth grade obtained significantly more correct answers when no drawings were made, while in the second grade, students had fewer correct answers when they had not compiled a drawing. Thus, we showed that even very young students could learn to compose schematic drawings; however, drawings alone may not be of help to solve the problem.",
            "year": 2022,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "bc57daff5192045e052c9245918f781ec7d72f43",
            "paperId": "bc57daff5192045e052c9245918f781ec7d72f43",
            "title": "A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters",
            "abstract": "Logical reasoning serves as a cornerstone for human cognition. Recently, the emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. To improve this capability, recent studies have delved into integrating LLMs with various symbolic solvers using diverse techniques and methodologies. While some combinations excel on specific datasets, others fall short. However, it remains unclear whether the variance in performance stems from the methodologies employed or the specific symbolic solvers utilized. Therefore, there is a lack of consistent comparison between symbolic solvers and how they influence LLM's logical reasoning ability. We perform experiments on LLMs integrated with 3 symbolic solvers: Z3, Pyke, and Prover9, and compare their performance on 3 logical reasoning datasets: ProofWriter, PrOntoQA, and FOLIO. Our findings indicate that when combined with LLMs Pyke's performance is significantly inferior to that of Prover9 and Z3. Z3's overall accuracy performance slightly surpasses Prover9, but Prover9 could execute more questions.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "9f171c208a3275b931d770ff424860f7b6ab1c0e",
            "paperId": "9f171c208a3275b931d770ff424860f7b6ab1c0e",
            "title": "The Power of Question Translation Training in Multilingual Reasoning: Broadened Scope and Deepened Insights",
            "abstract": "Bridging the significant gap between large language model's English and non-English performance presents a great challenge. While some previous studies attempt to mitigate this gap with translated training data, the recently proposed question alignment approach leverages the model's English expertise to improve multilingual performance with minimum usage of expensive, error-prone translation. In this paper, we explore how broadly this method can be applied by examining its effects in reasoning with executable code and reasoning with common sense. We also explore how to apply this approach efficiently to extremely large language models using proxy-tuning. Experiment results on multilingual reasoning benchmarks mGSM, mSVAMP and xCSQA demonstrate that the question alignment approach can be used to boost multilingual performance across diverse reasoning scenarios, model families, and sizes. For instance, when applied to the LLaMA2 models, our method brings an average accuracy improvements of 12.2% on mGSM even with the 70B model. To understand the mechanism of its success, we analyze representation space, chain-of-thought and translation data scales, which reveals how question translation training strengthens language alignment within LLMs and shapes their working patterns.",
            "year": 2024,
            "citationCount": 0,
            "score": 2
        },
        {
            "id": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
            "paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
            "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
            "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
            "year": 2023,
            "citationCount": 211,
            "score": 2
        },
        {
            "id": "e66f0f822d4c4853b39b27daaafa2993005fd55e",
            "paperId": "e66f0f822d4c4853b39b27daaafa2993005fd55e",
            "title": "Large Language Models are few(1)-shot Table Reasoners",
            "abstract": "Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with \u2018chain of thoughts\u2019 prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in {url{https://github.com/wenhuchen/TableCoT}.",
            "year": 2022,
            "citationCount": 71,
            "score": 2
        },
        {
            "id": "38115e80d805fb0fb8f090dc88ced4b24be07878",
            "paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878",
            "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
            "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
            "year": 2022,
            "citationCount": 562,
            "score": 2
        },
        {
            "id": "a14cf378cc00f960b3f040258972f305ebac5587",
            "paperId": "a14cf378cc00f960b3f040258972f305ebac5587",
            "title": "PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering",
            "abstract": "Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "d62c4d00b277e948956b6610ce2644e88fe1577b",
            "paperId": "d62c4d00b277e948956b6610ce2644e88fe1577b",
            "title": "Large Language Models",
            "abstract": "Large Language ModelsIn the latest edition of Stats, STAT!, Fralick and colleagues explain the statistics behind large language models - used in chat bots like ChatGPT and Bard. While these new tools may seem remarkably intelligent, at their core they just assemble sentences based on statistics from large amounts of text.",
            "year": 2023,
            "citationCount": 193,
            "score": 1
        },
        {
            "id": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report",
            "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
            "year": 2023,
            "citationCount": 4653,
            "score": 1
        },
        {
            "id": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
            "year": 2023,
            "citationCount": 5936,
            "score": 1
        },
        {
            "id": "32afdb07021fda775ceaedd231c58bfed0aa980a",
            "paperId": "32afdb07021fda775ceaedd231c58bfed0aa980a",
            "title": "Automated Crossword Solving",
            "abstract": "We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles. Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event. To facilitate research on question answering and crossword solving, we analyze our system\u2019s remaining errors and release a dataset of over six million question-answer pairs.",
            "year": 2022,
            "citationCount": 7,
            "score": 1
        },
        {
            "id": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
            "year": 2022,
            "citationCount": 4076,
            "score": 1
        },
        {
            "id": "5932a504a1b53ea4eb33325a8e34a57b00921183",
            "paperId": "5932a504a1b53ea4eb33325a8e34a57b00921183",
            "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
            "abstract": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.",
            "year": 2024,
            "citationCount": 2,
            "score": 1
        },
        {
            "id": "32e086dd56041ddb63d9e9e210c29a7fdeabdb6d",
            "paperId": "32e086dd56041ddb63d9e9e210c29a7fdeabdb6d",
            "title": "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models",
            "abstract": "Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical\"reasoning\"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW",
            "year": 2024,
            "citationCount": 0,
            "score": 1
        },
        {
            "id": "0d1569d9e62c35a62cfd20ed8209d82812e54c9c",
            "paperId": "0d1569d9e62c35a62cfd20ed8209d82812e54c9c",
            "title": "A Radio Math Instruction Program Improves Learning Outcomes for",
            "abstract": "How can we improve students' educational achievement? Can radio instruction successfully support distance learning, particularly in settings such as the COVID-19 pandemic? In rural Peru, researchers measured whether remote support from educational coaches for caregivers during MateWasi \u2014 a radio program with interactive math lessons for preschool-aged children\u2014 impacted their children\u2019s math learning outcomes. Results show that remote support increased the involvement of caregivers in their children\u2019s educational development. In turn, children\u2019s math learning improved by 0.12 standard deviations on an index that measured outcomes including oral counting, spatial ability, comparing of quantities, and solving word problems related to addition and subtraction.",
            "year": 2022,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "810420363681bc902fb3a4ea1ed7aaa344d52888",
            "paperId": "810420363681bc902fb3a4ea1ed7aaa344d52888",
            "title": "Math Word Problems: Suggestions for LD Students",
            "abstract": "This represents the final article in a series of three articles by John F. Cawley and his associates on mathematics and learning disabled students. The authors present specific strategies for systematically developing problem-solving experiences for LD students in mathematical curricula. The information discussed in this article is based on the extensive data pool collected by the authors on the mathematics characteristics of LD populations and the program development that has resulted from their efforts.",
            "year": 1979,
            "citationCount": 19,
            "tldr": null,
            "score": 1
        },
        {
            "id": "2c8740d4a7532e675f263ae7bc7c2623ae4b1bbc",
            "paperId": "2c8740d4a7532e675f263ae7bc7c2623ae4b1bbc",
            "title": "Synthesis of the integer FIR filters with short coefficient word length",
            "abstract": "The integer simulation and development finite impulse response (FIR) filters taking into account the possibilities of their realization on digital integer platforms are considered. The problem statement and solution of multifunctional synthesis of digital FIR filters such a problem on the basis of the numerical methods of integer nonlinear mathematical programming are given. As an several examples, the problem solution of synthesis FIR-filters with short coefficient word length\u00a0 has been given. The analysis of their characteristics is resulted. The paper discusses issues of modeling and synthesis of digital FIR filters with provision for the possibilities of their implementation on digital platforms with integer computation arithmetic. The formulation of the problem of multifunctional synthesis of cascade FIR filters using the methods of integer nonlinear mathematical programming is given. The efficiency of this approach is illustrated by examples of solving the problems of synthesizing integer FIR filters with a minimum coefficient word length. The analysis of the synthesized filter characteristics is made.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The paper discusses issues of modeling and synthesis of digital FIR filters with provision for the possibilities of their implementation on digital platforms with integer computation arithmetic."
            },
            "score": 1
        },
        {
            "id": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "paperId": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering",
            "abstract": "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.",
            "year": 2022,
            "citationCount": 49,
            "score": 1
        },
        {
            "id": "d84643c9f0289df5f9373480025da8f76f6df79c",
            "paperId": "d84643c9f0289df5f9373480025da8f76f6df79c",
            "title": "Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder",
            "abstract": "In the real-world question answering scenarios, hybrid form combining both tabular and textual contents has attracted more and more attention, among which numerical reasoning problem is one of the most typical and challenging problems. Existing methods usually adopt encoder-decoder framework to represent hybrid contents and generate answers. However, it can not capture the rich relationship among numerical value, table schema, and text information on the encoder side. The decoder uses a simple predefined operator classifier which is not flexible enough to handle numerical reasoning processes with diverse expressions. To address these problems, this paper proposes a Relational Graph enhanced Hybrid table-text Numerical reasoning model with Tree decoder (RegHNT). It models the numerical question answering over table-text hybrid contents as an expression tree generation task. Moreover, we propose a novel relational graph modeling method, which models alignment between questions, tables, and paragraphs. We validated our model on the publicly available table-text hybrid QA benchmark (TAT-QA). The proposed RegHNT significantly outperform the baseline model and achieve state-of-the-art results.",
            "year": 2022,
            "citationCount": 19,
            "score": 1
        },
        {
            "id": "dd43b2dacded3fbe089085f920f6ff9a0f33d5f6",
            "paperId": "dd43b2dacded3fbe089085f920f6ff9a0f33d5f6",
            "title": "A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA",
            "abstract": "The numerical reasoning in the financial domain -- performing quantitative analysis and summarizing the information from financial reports -- can greatly increase business efficiency and reduce costs of billions of dollars. Here, we propose a numerical reasoning question answering system to answer numerical reasoning questions among financial text and table data sources, consisting of a retriever module, a generator module, and an ensemble module. Specifically, in the retriever module, in addition to retrieving the whole row data, we innovatively design a cell retriever that retrieves the gold cells to avoid bringing unrelated and similar cells in the same row to the inputs of the generator module. In the generator module, we utilize multiple generators to produce programs, which are operation steps to answer the question. Finally, in the ensemble module, we integrate multiple programs to choose the best program as the output of our system. In the final private test set in FinQA Competition, our system obtains 69.79 execution accuracy.",
            "year": 2022,
            "citationCount": 3,
            "score": 1
        }
    ]
}