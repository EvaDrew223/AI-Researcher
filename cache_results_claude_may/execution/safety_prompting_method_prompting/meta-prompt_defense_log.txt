simulated 4 test examples for evaluation.
using model:  claude-3-opus-20240229
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:34<?, ?it/s]
Traceback (most recent call last):
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/safety_prompting_method_prompting/meta-prompt_defense.py", line 153, in <module>
    baseline_scores, proposed_scores, style_scores = run_experiment(client, model_name, seed, testset)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/safety_prompting_method_prompting/meta-prompt_defense.py", line 134, in run_experiment
    baseline_scores.append(output_evaluator(client, model_name, seed, prompt, gold_label, baseline_prediction))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/safety_prompting_method_prompting/meta-prompt_defense.py", line 109, in output_evaluator
    score = int(response.strip())
            ^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: 'I'
