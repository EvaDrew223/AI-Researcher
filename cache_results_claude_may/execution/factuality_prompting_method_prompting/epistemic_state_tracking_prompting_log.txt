simulated 3 test examples for evaluation.
using model:  claude-3-opus-20240229
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:34<?, ?it/s]
Traceback (most recent call last):
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/factuality_prompting_method_prompting/epistemic_state_tracking_prompting.py", line 159, in <module>
    baseline_scores, proposed_scores, style_check = run_experiment(client, model_name, seed, testset)
                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/factuality_prompting_method_prompting/epistemic_state_tracking_prompting.py", line 140, in run_experiment
    baseline_scores.append(output_evaluator(client, model_name, seed, question, gold_label, baseline_prediction))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/juice2/scr2/clsi/AI-Researcher/ai_researcher/../cache_results_claude_may/execution/factuality_prompting_method_prompting/epistemic_state_tracking_prompting.py", line 114, in output_evaluator
    score = int(response.strip())
            ^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: 'I'
