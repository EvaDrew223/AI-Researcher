#pos:  219 #neg:  126 N:  126
  0%|          | 0/126 [00:00<?, ?it/s]You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  1%|          | 1/126 [00:40<1:23:35, 40.13s/it]
full response:  paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


2
predicted:  2
label:  2
cost:  0.319605
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  2%|▏         | 2/126 [01:15<1:17:34, 37.54s/it]
full response:  paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


1
predicted:  1
label:  1
cost:  0.30715499999999996
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  2%|▏         | 3/126 [01:59<1:22:20, 40.17s/it]
full response:  paper 1 meta review:
 The paper proposes an interesting and novel approach to meta-learning called Uncertainty-Aware Prompted Meta-Learning (UAPML). The key idea is to use a learnable Bayesian meta-prompt to initialize task-specific prompts, which are then used to extract meta-knowledge from a frozen model backbone. This approach aims to reduce computational costs while maintaining or even improving performance compared to traditional meta-learning methods that adapt all meta-knowledge to specific tasks.

The motivation and problem statement are clearly articulated, and the proposed method is well-described. The experiment plan is comprehensive, involving multiple datasets, model architectures, and baselines. The analysis of results will provide insights into the effectiveness of UAPML in terms of accuracy, computational efficiency, and the impact of the Bayesian meta-prompt and uncertainty-aware construction methods.

However, the paper lacks some details on the specific implementation of the Bayesian meta-prompt and the soft and hard modulation techniques. Additionally, it would be beneficial to include more discussion on the potential limitations and future directions of the proposed approach.
Justification for why not higher score: The paper is well-structured and proposes a novel approach, but some implementation details are missing, and a more in-depth discussion of limitations and future work would strengthen the contribution.
Justification for why not lower score: The proposed method is innovative and addresses an important problem in meta-learning. The experiment plan is thorough, and the potential impact of the work is significant.

paper 1 score:  6.5



paper 2 meta review:
 This paper introduces the novel task of future language modeling, which aims to predict future textual data based on a temporal history of texts. The authors propose three methods to incorporate temporal information into pre-trained language models: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These methods modify the language model probabilities to account for temporal evolution and generate content that follows predicted future trends.

The problem statement is well-motivated, highlighting the importance of textual data predictions for human consumption and expert predictions. The proposed methods are clearly described, and the experiment plan is comprehensive, involving a real-world dataset of ACL anthology paper abstracts, multiple evaluation metrics (both automatic and human), and comparisons against relevant baselines.

The strengths of this paper lie in the novelty of the task, the well-designed methods that incorporate temporal information into language models, and the thorough evaluation plan. The authors also provide a clear justification for why their proposed methods are expected to work better than existing baselines.

One potential weakness is the reliance on a single dataset (ACL anthology paper abstracts) for evaluation. It would be beneficial to test the proposed methods on additional datasets from different domains to demonstrate their generalizability.
Justification for why not higher score: While the paper introduces a novel and important task, the evaluation is limited to a single dataset, and the generalizability of the proposed methods to other domains is not fully demonstrated.
Justification for why not lower score: The problem is well-motivated, the proposed methods are innovative and clearly described, and the evaluation plan is comprehensive, involving both automatic and human evaluation metrics.

paper 2 score:  7.5


2
predicted:  2
label:  2
cost:  0.324
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  3%|▎         | 4/126 [02:36<1:19:35, 39.14s/it]
full response:  paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an "undecidable" problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a "Mosaic Prompts" method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


1
predicted:  1
label:  1
cost:  0.304695
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  4%|▍         | 5/126 [03:02<1:09:05, 34.26s/it]
full response:  paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


2
predicted:  2
label:  2
cost:  0.29502000000000006
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  5%|▍         | 6/126 [03:33<1:06:32, 33.27s/it]
full response:  paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


2
predicted:  2
label:  2
cost:  0.33486
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Branch-GAN:
  Title: Branch-GAN: Improving Text Generation with (not so) Large Language Models
  Problem Statement: Current large language models (LLMs) for open domain text generation, despite their advanced capabilities, suffer from issues such as repetitive texts, looping, and lack of robustness. These deficiencies hinder the quality and reliability of generated texts.
  Motivation: Existing methods, including adversarial training through GANs, have primarily focused on older architectures or narrow tasks, making them incompatible with modern LLMs for open-ended text generation. This has led to a lack of interest in GAN-based approaches within the research community. The proposed method, Branch-GAN, leverages the parallelization capabilities of Transformer models to address these issues, providing a dense reward and loss signal for both the generator and discriminator, resulting in stable training dynamics. This method is expected to improve text quality significantly, even with smaller models, by avoiding the sparsity problems of traditional GAN approaches.
  Proposed Method: Branch-GAN is a computationally efficient GAN approach for sequential data that utilizes the parallelization capabilities of Transformer models. The method involves generating multiple branching sequences from each training sample and incorporating the typical next-step prediction loss on the original data. This approach provides a dense reward and loss signal for both the generator and the discriminator, leading to stable training dynamics. The generator is trained via both MLE on the original sequence and an adversarial signal from the discriminator for the generated sequences. The discriminator processes data similarly to the generator but includes an additional forward pass for the final generation step, providing three scalar predictions at each time-step.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the Pile dataset, which contains diverse text data. Select 100k randomly chosen sequences of length 128 tokens from the Pile dataset for training. For evaluation, use Wikipedia and Fictional Stories datasets, each with 100 samples of 128 tokens, split into 32 tokens for context and 96 tokens for continuation.
    Step 2: Construct Prompts: Not applicable as this is not a prompting paper.
    Step 3: Select Models: Use pre-trained language models from the Pythia model suite, specifically Pythia 410M, Pythia 1B, and Pythia 6.9B. Initialize both the generator and discriminator from the same checkpoint.
    Step 4: Training Details:
      Generator:
        Optimizer: Adam
        Learning Rate: Start at 10^-8, warmup steps 1000, end at 10^-5
        Branches (K): 32
        Depth (d): 16
        Sampling Strategy: Top-k with k=50
        Section Weight: 1 for RL Advantage, 1.5 for Section 3
      Discriminator:
        Optimizer: Adam
        Learning Rate: 10^-4
        Value Head Loss Weight (α): 0.2
      Training Setup: Train for 4 epochs with a batch size of 8, resulting in 50k optimizer updates. Use 5/6 of the tokens from generated sequences.
    Step 5: Get Results: Generate text continuations for the evaluation datasets using the trained models. Use different sampling methods (greedy decoding, top-p sampling) and apply a repetition penalty of 1.2 where applicable.
    Step 6: Human Evaluation: Conduct a human evaluation study with over 10k annotations from verified English speakers. Compare the generated texts against original texts and texts generated by Branch-GAN 1B* (greedy). Collect annotations on which continuation is preferred or if they are equally good.
    Step 7: Analyze Results: Compare the performance of Branch-GAN models against baseline models (Pythia and LLama-2) and GPT-4. Evaluate using metrics such as Type-Token Ratio (TTR), cross-entropy, and Self-BLEU. Assess the robustness of the models by introducing noise into the context and measuring the impact on generated text quality.


paper 2:
Measuring Feature Sparsity in Language Models:
  Title: Measuring Feature Sparsity in Language Models
  Problem Statement: The paper addresses the problem of understanding and quantifying the sparsity of features in the intermediate activations of language models. This is important because it can provide insights into how these models represent and process information, which is crucial for interpretability and improving model performance.
  Motivation: Existing methods for feature extraction in language models often rely on human input and do not scale well. They also do not rigorously test the underlying assumptions of linearity and sparsity in model activations. The proposed method aims to provide more rigorous and quantitative metrics to assess the success of sparse coding techniques, thereby testing the validity of these assumptions more effectively.
  Proposed Method: The authors propose novel metrics for measuring the success of sparse coding on neural network activations. These metrics include the average coefficient norm and normalized loss, which are designed to be scale-invariant and robust. The method involves decomposing activations into sparse linear combinations of feature vectors and using these metrics to assess the level of sparsity.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments use synthetic datasets to test the metrics and real-world datasets to measure sparsity in language models. For real-world datasets, Wikipedia abstracts are used to generate model inputs.
    Step 2: Construct Synthetic Data: Generate synthetic activation distributions that satisfy the sparsity and linearity hypotheses with varying average numbers of active features. This involves sampling feature vectors from a unit sphere and adding Gaussian noise.
    Step 3: Select Models: The experiments are conducted on various language models, including BERT (Tiny, Mini, Small, Medium), TinyStories (1M, 3M, 33M), and GPT-Neo/GPT-2.
    Step 4: Apply Sparse Coding: Decompose the activations using sparse coding. This involves minimizing the sparse coding objective using an iterative optimization procedure. The dictionary size is set to 8 times the embedding size for embedding layers and 16 times for later layers.
    Step 5: Measure Sparsity: Use the proposed metrics (average coefficient norm and normalized loss) to measure the level of sparsity in the decomposed activations. Compare these metrics against control datasets like Gaussian distributions.
    Step 6: Analyze Results: Analyze the results to determine the level of sparsity in different layers of the language models. Specifically, assess how sparsity varies across layers and compare the results with control datasets to validate the linearity and sparsity hypotheses.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  6%|▌         | 7/126 [04:04<1:04:26, 32.50s/it]
full response:  paper 1 meta review:
 The paper proposes an interesting approach to improve text generation using GANs with smaller language models. The motivation is clear, and the proposed Branch-GAN method seems to be a novel and computationally efficient way to provide dense rewards and stable training dynamics. The experiments are well-designed, covering both automatic metrics and human evaluation. However, some concerns remain about the scalability of the approach to larger models and the generalizability to more diverse datasets. Additionally, more analysis could be provided on the specific improvements in text quality and robustness compared to baseline models.
Justification for why not higher score: The paper lacks some depth in analyzing the improvements and scalability of the proposed method.
Justification for why not lower score: The novelty of the approach and the well-designed experiments make this a solid contribution.

paper 1 score:  6.5



paper 2 meta review:
 This paper addresses an important problem of understanding and quantifying feature sparsity in language models. The proposed metrics for measuring the success of sparse coding are novel and well-motivated. The experiments are carefully designed, using both synthetic and real-world datasets to validate the metrics and test the underlying assumptions. The results provide valuable insights into the sparsity of different layers in various language models. However, the paper could benefit from more discussion on the implications of these findings for model interpretability and performance. Additionally, the scalability of the approach to larger models and more diverse datasets could be further explored.
Justification for why not higher score: The paper could provide more discussion on the implications and scalability of the findings.
Justification for why not lower score: The novelty of the metrics and the rigorous experimental design make this a valuable contribution to the field.

paper 2 score:  7.0


accepted paper:  2
predicted:  2
label:  1
cost:  0.29799000000000003
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Demonstration Distillation for Efficient In-Context Learning:
  Title: Demonstration Distillation for Efficient In-Context Learning
  Problem Statement: In-context learning (ICL) significantly enhances the predictive capabilities of large language models (LLMs) by using a few question-answer pairs (demonstrations) in the prompt. However, lengthy and information-rich demonstrations, while improving performance, also increase computational burdens and financial costs, and can even breach the context limit of LLMs. The challenge is to reduce the length of these demonstrations without sacrificing their effectiveness.
  Motivation: Existing solutions like prompt selection or context compression often overlook the presence of superfluous information within elongated prompts. These methods either fail to remove extraneous tokens sufficiently or require extensive training cycles, which are computationally expensive. The proposed method, demonstration distillation, aims to excise redundant content in the prompt while maintaining ICL efficacy. This approach is inspired by dataset distillation and model distillation, where the goal is to preserve essential information while reducing size.
  Proposed Method: The proposed method, Distillist-Generalist-Specialist (DGS), is an automated framework that iteratively refines demonstrations using three LLM-powered agents: Distillist, Generalist, and Specialist. The Distillist condenses the input demonstrations, the Generalist evaluates the completeness and generalizability of the distilled demonstrations, and the Specialist tests the effectiveness of the distilled demonstrations for ICL. This iterative process ensures that only valuable information is retained, thereby increasing the information density of the input.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We evaluate on three diverse datasets: GSM8K (grade school math problems), BoolQ (true/false question-answering tasks), and MultiRC (multi-sentence reading comprehension). These datasets vary in topic focus, information density, and demonstration length.
    Step 2: Construct Prompts: For the baseline, we use the original lengthy demonstrations. For the proposed method, we first prompt the Distillist to distill the demonstrations. The Generalist then evaluates the distilled demonstrations for completeness and assigns a punishment score if necessary. If the score is acceptable, the Specialist tests the distilled demonstrations for their effectiveness in answering a specific target question.
    Step 3: Select Models: We use ChatGPT (OpenAI, 2023) as the LLM behind all three agents (Distillist, Generalist, and Specialist) due to its robust capabilities and moderate window size.
    Step 4: Get Results: Evaluate the distilled demonstrations on the test set using various models, including ChatGPT, ChatGLM, and AutoCompressor. Metrics used are 'Accuracy' for GSM8K and BoolQ datasets, and 'Exact Match rate' (EM) for the MultiRC dataset.
    Step 5: Analyze Results: Compare the performance of the distilled demonstrations against the original ones. Measure the compression ratio (token count of original demonstration divided by that of the distilled version) and the accuracy or EM to determine if the distilled demonstrations maintain or improve ICL performance.


paper 2:
InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation:
  Title: InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation
  Problem Statement: The paper addresses the challenge of learning powerful and transferable video-text representations for multimodal understanding and generation. This problem is crucial for various real-world applications such as autonomous driving, intelligent surveillance, and human-computer interaction. Existing datasets often lack high-quality video-text correlations, limiting the effectiveness of video-language representation learning.
  Motivation: Current video-text datasets like HowTo100M, HD-VILA, and YT-Temporal rely on automatic speech recognition (ASR) for text generation, which often results in low semantic correlations between videos and their textual descriptions. This low correlation hampers the performance of downstream tasks such as video retrieval and video question answering. The proposed method aims to autonomously build a high-quality video-text dataset using large language models (LLMs) to generate more accurate and semantically rich video descriptions. This approach is expected to improve the learning of video-language representations at scale, thereby enhancing the performance of various video understanding and generation tasks.
  Proposed Method: The proposed method involves creating InternVid, a large-scale video-centric multimodal dataset containing over 7 million videos, 234 million video clips, and 4.1 billion words in descriptions. The dataset is built using a multi-scale approach to generate video-related descriptions. Additionally, the paper introduces ViCLIP, a video-text representation learning model based on ViT-L, which is trained on InternVid using contrastive learning. ViCLIP demonstrates leading zero-shot action recognition and competitive video retrieval performance. The dataset and model also support generating interleaved video-text data for learning video-centric dialogue systems and advancing video-to-text and text-to-video generation research.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect over 7 million public YouTube videos, ensuring diversity in categories, countries, and languages. Segment these videos into 234 million clips using scene variance and filter out clips with extreme dynamics or stillness.
    Step 2: Data Curation: Define around 6,000 action phrases for querying videos. Collect videos by selecting popular channels and high-rated videos, and by using action-based queries. Ensure videos are between 10 seconds and 30 minutes in duration and have resolutions ranging from 360P to 720P.
    Step 3: Multiscale Video Captioning: Generate video captions using a multiscale method. At the coarse scale, caption the middle frame of each video using BLIP2. At the fine scale, use Tag2Text to describe videos frame-by-frame and summarize these descriptions using a language model (T5-summary).
    Step 4: Construct Prompts: For generating interleaved video-text data, arrange clips and their descriptions sequentially based on their temporal order within the same video. Enhance diversity by assigning ASR text to a used clip in addition to its caption. Extend this by concatenating two interleaved multimodal items to simulate a video-centric dialogue.
    Step 5: Select Models: Introduce ViCLIP, a video-text representation learning model based on ViT-L. Initialize the video and text encoders from the CLIP pretrained model. Apply random patch masking to input videos for efficient learning and optimize video-text alignment using InfoNCE loss.
    Step 6: Get Results: Evaluate ViCLIP on video-related benchmarks using full-finetuned and zero-shot settings. Sample subsets of InternVid (InternVid-10M, InternVid-50M, and InternVid-200M) for training. Measure performance on action recognition (Kinetics 400/600/700) and video retrieval (MSR-VTT, LSMDC, DiDeMo, MSVD, ActivityNet).
    Step 7: Analyze Results: Compare the performance of ViCLIP trained on InternVid with other models and datasets. Assess the impact of data scale on the learned video-text representations. Evaluate the effectiveness of InternVid in improving text-to-video generation and supporting multimodal dialogue systems.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  6%|▋         | 8/126 [04:49<1:11:38, 36.43s/it]
full response:  paper 1 meta review:
The paper proposes an interesting approach, Demonstration Distillation, to reduce the length of demonstrations in in-context learning (ICL) while maintaining their effectiveness. The motivation is clear and the proposed Distillist-Generalist-Specialist (DGS) framework seems promising in iteratively refining demonstrations to increase information density. The experiments cover diverse datasets, which is a strength.

However, there are some concerns and limitations. First, the paper relies heavily on ChatGPT for all three agents, which raises questions about generalizability to other LLMs. Second, the evaluation metrics used (accuracy and exact match rate) may not fully capture the quality and coherence of the distilled demonstrations. Human evaluation could provide additional insights. Finally, the computational efficiency and cost analysis of the proposed method compared to baselines are not thoroughly discussed.

In summary, while the paper presents a novel and relevant idea, the evaluation and analysis could be strengthened to better demonstrate its effectiveness and efficiency.
Justification for why not higher score: The evaluation and analysis have some limitations that prevent a higher score.
Justification for why not lower score: The proposed method is novel, well-motivated, and shows promise in addressing an important challenge in ICL.

paper 1 score:  5.5

paper 2 meta review:
This paper introduces InternVid, a large-scale video-text dataset aimed at improving multimodal understanding and generation. The dataset's scale (7M+ videos, 234M+ clips) and diversity (categories, countries, languages) are impressive and address limitations of existing datasets that rely on ASR for text generation. The multiscale video captioning approach using LLMs is a novel contribution that enhances the semantic richness of video descriptions.

The proposed ViCLIP model, trained on InternVid, demonstrates strong performance on various video-related benchmarks, even in zero-shot settings. The experiments are comprehensive, covering action recognition and video retrieval tasks. The ablation studies on data scale provide valuable insights into the impact of dataset size on learned representations.

One strength of the paper is its potential to support various applications, including video-centric dialogue systems and video-to-text/text-to-video generation. The interleaved video-text data generation method is a creative approach to enable such applications.

Overall, the paper makes significant contributions to the field of video-language representation learning. The InternVid dataset and ViCLIP model have the potential to advance research in multimodal understanding and generation.
Justification for why not higher score: While the paper is strong, there could be more in-depth analysis on the quality and diversity of the generated captions.
Justification for why not lower score: The scale, diversity, and quality of InternVid, along with the strong performance of ViCLIP, make this a highly impactful contribution.

paper 2 score:  8.0

2
predicted:  2
label:  2
cost:  0.31968
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models:
  Title: Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models
  Problem Statement: Large language models (LLMs) have shown remarkable reasoning capabilities using chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to derive answers. However, existing CoT methods either use general prompts like 'Let’s think step by step' or rely heavily on handcrafted task-specific demonstrations, leading to a gap between performance and generalization. The problem is to develop a CoT prompting method that performs well and generalizes across mixed-task scenarios where the type of input questions is unknown.
  Motivation: Current CoT methods fall short in mixed-task scenarios. General Zero-Shot-CoT methods have good generalization but often underperform compared to task-specific Few-Shot-CoT methods. Conversely, Few-Shot-CoT methods achieve high performance but lack generalization. This gap is problematic in real-world applications where the type of input questions is not predefined. The proposed Meta-CoT aims to bridge this gap by categorizing the scenario based on the input question and constructing diverse demonstrations automatically, thus achieving both high performance and superior generalization.
  Proposed Method: Meta-CoT is a generalizable CoT prompting method designed for mixed-task scenarios. It involves three phases: (1) Scenario Identification: Categorizes the input question into a specific scenario using a few in-context learning (ICL) demonstrations. (2) Demonstration Selection: Constructs diverse demonstrations from the corresponding data pool based on the identified scenario using k-means clustering and Zero-Shot-CoT. (3) Answer Derivation: Performs final inference on the input question using the selected demonstrations and delivers feedback to the data pool.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Evaluate on ten in-distribution benchmark reasoning tasks: MultiArith, GSM8K, AddSub, AQUA-RAT, SingleEq, SVAMP, CSQA, StrategyQA, Last Letter Concatenation, and Coin Flip. Also, validate on five out-of-distribution datasets: ARC-challenge, ASDiv, CSQA2.0, Sports Understanding, and Creak.
    Step 2: Construct Prompts: For scenario identification, use prompts like 'Q: [input question], Scenario: ' with in-context demonstrations such as 'Q: [sample question], Scenario: [scenario name]'. For demonstration selection, use Zero-Shot-CoT prompts like 'Let’s think step by step' to generate rationales and answers. For answer derivation, construct prompts like 'Q: [input question], A: Let’s think step by step' with the selected demonstrations.
    Step 3: Select Models: Use GPT-3.5-Turbo and GPT-4 from the OpenAI API for all experiments.
    Step 4: Get Results: Obtain answer predictions from the models on the datasets using both the baselines and the proposed Meta-CoT method.
    Step 5: Analyze Results: Compare the performance of Meta-CoT with baseline methods (Zero-Shot, Few-Shot, Few-Shot-CoT, Auto-CoT, Zero-Shot-CoT, General-CoT) on both in-distribution and out-of-distribution datasets. Evaluate metrics such as accuracy and generalization capability.


paper 2:
Adaptive Chameleon or Stubborn Sloth:
  Title: Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts
  Problem Statement: The paper investigates how receptive large language models (LLMs) are to external evidence, especially when it conflicts with their parametric memory. This problem is crucial because LLMs' static parametric memory can be outdated or incorrect, leading to hallucinations. Understanding LLMs' behavior in knowledge conflicts is essential for improving tool-augmented LLMs.
  Motivation: Existing methods like tool or retrieval augmentation provide external information to LLMs to address the limitations of their static parametric memory. However, it is unclear how LLMs handle conflicting external evidence. Prior work suggests that LLMs are stubborn and cling to their parametric memory, but these conclusions are based on heuristic word-level editing, which may not be coherent. The proposed method aims to construct more coherent counter-memory to better understand LLMs' behavior in real-world scenarios.
  Proposed Method: The authors propose a systematic framework to elicit high-quality parametric memory from LLMs and construct corresponding counter-memory. The framework includes steps for eliciting parametric memory, constructing counter-memory, entailment checking, and ensuring answer consistency. This allows for controlled experiments to study LLMs' behavior in different knowledge conflict scenarios.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments use two datasets: POPQA, an entity-centric QA dataset with 14K questions, and STRATEGY QA, a multi-step reasoning dataset. These datasets help in diversifying the questions studied in the experiments.
    Step 2: Elicit Parametric Memory: In a closed-book QA fashion, LLMs are asked to answer questions and provide detailed background information. This background information is considered the parametric memory. For example, given the question 'Who is the chief scientist of Google DeepMind?', the LLM should answer 'Demis Hassabis' and provide supporting details.
    Step 3: Construct Counter-memory: Reframe the memory answer to construct a counter-answer. For POPQA, substitute the entity in the memory answer with a same-type entity. For STRATEGY QA, flip the memory answer. Use ChatGPT to generate coherent counter-memory that supports the counter-answer.
    Step 4: Entailment Checking: Use a state-of-the-art NLI model (DeBERTa-V2) to ensure that both parametric memory and counter-memory support their corresponding answers. Only keep examples where both answers are supported.
    Step 5: Ensure Answer Consistency: Present the parametric memory as evidence and instruct LLMs to answer the same question again. Filter out examples where the LLMs change their answers, ensuring the remaining examples capture firm parametric memory.
    Step 6: Single-source Evidence Experiment: Present counter-memory as the sole evidence to LLMs and observe their behavior. Compare the results with entity substitution-based counter-memory and generation-based counter-memory.
    Step 7: Multi-source Evidence Experiment: Present both parametric memory and counter-memory as evidence. Study the evidence preference of LLMs based on popularity, order, and quantity of evidence. Use human-written evidence from Wikipedia for additional experiments.
    Step 8: Analyze Results: Compare the behavior of LLMs in single-source and multi-source evidence settings. Evaluate the impact of evidence coherence, popularity, order, and quantity on LLMs' responses. Analyze the confirmation bias and susceptibility to disinformation.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  7%|▋         | 9/126 [05:31<1:14:40, 38.29s/it]
full response:  paper 1 meta review:
 The paper proposes Meta-CoT, a generalizable chain-of-thought prompting method for mixed-task scenarios. The approach involves scenario identification, demonstration selection, and answer derivation to bridge the gap between performance and generalization in existing CoT methods. The experiments cover a wide range of reasoning tasks and evaluate both in-distribution and out-of-distribution performance.

The reviewers appreciate the novelty and practicality of the proposed Meta-CoT framework. The systematic approach to handling mixed-task scenarios and the automatic construction of diverse demonstrations are seen as valuable contributions. The extensive experiments on various datasets demonstrate the effectiveness and generalization capability of Meta-CoT compared to baseline methods.

However, some reviewers raise concerns about the clarity of certain technical details, such as the specifics of the scenario identification and demonstration selection processes. Additionally, the paper could benefit from a more in-depth analysis of the limitations and potential failure cases of Meta-CoT.

Overall, the reviewers recommend accepting this paper, considering its novel approach to generalizable CoT prompting and its strong empirical results. Addressing the clarity concerns and providing a more comprehensive analysis of limitations would further strengthen the paper.
Justification for why not higher score: Some technical details need further clarification, and a more in-depth analysis of limitations is desired.
Justification for why not lower score: The novelty, practicality, and strong empirical results of Meta-CoT make it a valuable contribution to the field.

paper 1 score:  7.0



paper 2 meta review:
 This paper investigates an important question in the field of large language models (LLMs): how do LLMs handle conflicting external evidence? The authors propose a systematic framework to elicit high-quality parametric memory from LLMs and construct corresponding counter-memory, allowing for controlled experiments to study LLMs' behavior in different knowledge conflict scenarios.

The reviewers commend the authors for tackling a crucial problem and designing a well-thought-out experimental framework. The use of diverse datasets (POPQA and STRATEGY QA) and the careful construction of coherent counter-memory are seen as strengths of the paper. The single-source and multi-source evidence experiments provide valuable insights into LLMs' evidence preference and susceptibility to disinformation.

However, some reviewers suggest that the paper could benefit from a more comprehensive analysis of the factors influencing LLMs' behavior, such as the role of language understanding and reasoning capabilities. Additionally, the implications of the findings for the development of tool-augmented LLMs could be discussed in more depth.

Despite these minor concerns, the reviewers agree that this paper makes a significant contribution to the understanding of LLMs' behavior in knowledge conflicts. The novel experimental framework and the insightful findings make this paper a strong candidate for acceptance.
Justification for why not higher score: A more comprehensive analysis of influencing factors and implications for tool-augmented LLMs could further enhance the paper.
Justification for why not lower score: The well-designed experimental framework, diverse datasets, and valuable insights into LLMs' behavior make this paper a significant contribution to the field.

paper 2 score:  8.0


accepted paper:  2
predicted:  2
label:  2
cost:  0.323445
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Tell, Don't Show: Internalized Reasoning influences how LLMs generalize:
  Title: Tell, Don't Show: Internalized Reasoning influences how LLMs generalize
  Problem Statement: The paper investigates the extent to which language models' generalization behavior during a domain shift can be influenced by declarative knowledge contained in the training data. This problem is crucial because understanding how LLMs generalize to unseen examples is essential for ensuring their safety and fairness in real-world applications.
  Motivation: Existing methods often debate whether LLMs are merely repeating low-level statistical patterns or if they are capable of higher-level reasoning. Prior work indicates that LLMs' outputs rely on increasingly abstract inferences as they scale, but it is unclear how they will generalize when abstract and less abstract forms of generalization conflict. The proposed method aims to study this by finetuning LLMs with declarative statements that could alter their generalization behavior, hypothesizing that larger models might internalize this knowledge more effectively.
  Proposed Method: The authors propose finetuning LLMs on a distribution with a 'natural' generalization and then testing the effect of declarative statements in the training data on the models' behavior on unseen examples. They develop toy models to study this counterfactual effect and run several ablations to demonstrate that the effect is not due to trivial token matching behavior.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Create datasets for two toy models: (1) Monthly weather reports with biased probabilities of rain and sun, and (2) Gender statistics of teachers in different countries with biased probabilities of male and female teachers.
    Step 2: Construct Prompts: For the weather reports, use prompts like 'Weather report from [MONTH]: ' and completions like '[Sunny/Rainy], [temperature], [humidity], [description]'. For the gender statistics, use prompts like 'Teacher from [COUNTRY]: ' and completions like '[Male/Female], [NAME], [AGE], [description]'.
    Step 3: Select Models: Finetune various models from the GPT-3 family (ada, babbage, curie, davinci) and LLaMa-2 models (7B, 13B).
    Step 4: Finetune Models: Finetune the models on the constructed datasets with and without declarative statements. For weather reports, finetune on months Jan-Oct and test on Nov-Dec. For gender statistics, finetune on European and African countries and test on unseen countries.
    Step 5: Get Results: Measure the probability of sampling specific tokens (e.g., 'Sun' vs. 'Rain' or 'Male' vs. 'Female') and compute the direction-adjusted effect (DAE) to quantify the influence of declarative statements.
    Step 6: Analyze Results: Compare the models' generalization behavior with and without declarative statements. Perform ablations to rule out trivial token matching and test on variations like capital cities and rephrased descriptions to further validate the internalization of knowledge.


paper 2:
Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI:
  Title: Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI
  Problem Statement: Current EEG-based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting their scale, perceptual capabilities, and generalizability. This paper aims to develop a large EEG model that can learn universal perceptual capabilities of EEG signals through unsupervised pre-training and can be fine-tuned for different downstream tasks.
  Motivation: Existing methods are limited by the small volume and varied formats of EEG datasets, which include mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. Inspired by the success of Large Language Models (LLMs) in text processing, the authors propose a Large EEG Model (LEM) to overcome these challenges. The proposed method, LaBraM, is expected to outperform existing methods by enabling cross-dataset learning and capturing both temporal and spatial features of EEG signals.
  Proposed Method: The proposed method, LaBraM, segments EEG signals into EEG channel patches and uses vector-quantized neural spectrum prediction to train a neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. Neural Transformers are then pre-trained by predicting the original neural codes for the masked EEG channel patches. LaBraM is pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple downstream tasks.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect over 2,500 hours of diverse EEG data across various tasks and formats from about 20 datasets, including public datasets and self-collected EEG data. Exclude the four downstream datasets (TUAB, TUEV, SEED-V, MoBI) from the pre-training datasets.
    Step 2: Preprocess Data: Filter EEG signals between 0.1 Hz and 75 Hz to remove low-frequency noise, apply a notch filter of 50 Hz to avoid power-line interference, and resample all EEG signals to 200 Hz. Normalize the EEG values to ensure they mainly fall between -1 to 1.
    Step 3: Train Neural Tokenizer:
      Temporal Encoder: Use a temporal encoder with several temporal convolution blocks to encode each EEG patch into a patch embedding.
      Neural Tokenizer: Train a neural tokenizer to discretize EEG signals into discrete neural tokens by reconstructing the Fourier spectrum (amplitude and phase) of the original signal.
      Neural Decoder: Use a neural decoder to decode the Fourier spectrum from neural embeddings.
    Step 4: Pre-train LaBraM:
      Masked EEG Modeling: Randomly mask part of the EEG patches and replace them with a learnable mask token. The objective is to predict the masked tokens from visible patches using a neural Transformer.
      Symmetric Masking: Generate an inverse mask to provide more masking perspectives in one batch, improving training efficiency and data diversity.
    Step 5: Fine-tune LaBraM on Downstream Tasks:
      Datasets: Use TUAB for abnormal detection, TUEV for event type classification, SEED-V for emotion recognition, and MoBI for gait prediction.
      Metrics: Evaluate using Balanced Accuracy, AUC-PR, AUROC, Cohen’s Kappa, Weighted F1, Pearson’s Correlation, R2 Score, and RMSE.
      Training: Fine-tune LaBraM on each downstream dataset using task-specific prediction heads and appropriate loss functions (e.g., binary cross-entropy for TUAB, cross-entropy for TUEV, MSE for MoBI).
    Step 6: Analyze Results: Compare the performance of LaBraM with state-of-the-art methods on each downstream task. Evaluate the impact of pre-training data size, mask ratio, and symmetric masking strategy on the model's performance.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  8%|▊         | 10/126 [06:06<1:12:03, 37.27s/it]
full response:  paper 1 meta review:
 This paper investigates an interesting question about how declarative knowledge in the training data influences the generalization behavior of language models during domain shifts. The authors propose a novel approach to study this by finetuning LLMs with declarative statements and measuring their effect on the models' outputs on unseen examples. The experiments are well-designed, using toy models to isolate the effect of declarative knowledge and performing ablations to rule out trivial explanations. The results provide valuable insights into how LLMs internalize abstract knowledge and how this influences their generalization capabilities.
Justification for why not higher score: While the paper addresses an important question and proposes a novel approach, the experiments are limited to toy models and the real-world implications are not fully explored.
Justification for why not lower score: The paper is well-motivated, the methodology is sound, and the results are insightful and contribute to our understanding of LLMs' reasoning capabilities.

paper 1 score:  7.0



paper 2 meta review:
 This paper proposes LaBraM, a large EEG model that learns universal perceptual capabilities through unsupervised pre-training on a diverse and extensive EEG dataset. The motivation for developing a large-scale EEG model is well-justified, given the limitations of current methods in terms of dataset size, format variability, and generalizability. The proposed method, which involves neural tokenization and masked EEG modeling, is novel and well-suited for capturing both temporal and spatial features of EEG signals. The experiments are comprehensive, covering multiple downstream tasks and datasets, and the results demonstrate the effectiveness of LaBraM in learning generic EEG representations that can be fine-tuned for various applications.
Justification for why not higher score: While the paper makes a significant contribution to the field of EEG-based deep learning, some aspects of the methodology, such as the choice of hyperparameters and the impact of different pre-training strategies, could be further explored and discussed.
Justification for why not lower score: The paper addresses an important problem, proposes a novel and well-designed method, and provides extensive experimental validation on diverse datasets and tasks, making it a strong contribution to the field.

paper 2 score:  8.0


2
predicted:  2
label:  2
cost:  0.30871499999999996
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
Task Planning for Visual Room Rearrangement under Partial Observability:
  Title: Task Planning for Visual Room Rearrangement under Partial Observability
  Problem Statement: The paper addresses the problem of efficiently planning a sequence of actions for an embodied agent to simultaneously search for and rearrange objects in an untidy room to achieve a desired tidy state, under conditions of partial observability.
  Motivation: Existing methods for room rearrangement often emphasize perception and commonsense reasoning but lack efficient planning. They either assume complete visibility of the room or employ sub-optimal heuristic planners that do not minimize the number of steps or overall traversal. These methods also fail to resolve blocked goal and swap cases effectively. The proposed method aims to overcome these limitations by introducing a novel hierarchical task planner that leverages commonsense knowledge, deep reinforcement learning (RL), and a graph-based state representation to optimize the rearrangement process.
  Proposed Method: The proposed method consists of several key components: (i) a novel Search Network that uses commonsense knowledge from large language models to predict the locations of unseen objects, (ii) a Deep RL network trained with a proxy reward to plan the sequence of actions, (iii) a graph-based state representation to encode the scene geometry and make the planner scalable, and (iv) a sample-efficient cluster-biased sampling technique for training the proxy reward network and the Deep RL network. The method interleaves object search and rearrangement to minimize the number of steps and overall traversal, and it can handle blocked goal and swap cases without explicit buffers.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments use two datasets: (i) a Graph Dataset generated using Ai2Thor with random object placements to train the Graph Representation Network (GRN), and (ii) a Search Network Dataset derived from the AMT dataset to train the Search Network. Additionally, a new benchmark dataset, RoPOR, is introduced to evaluate the effectiveness of the rearrangement planning.
    Step 2: Construct Prompts: Not applicable as this is not a prompting paper.
    Step 3: Select Models: The models used include: (i) RoBERTa-Large for generating embeddings in the Search Network, (ii) a Graph Convolution Network (GCN) for the Graph Representation Network, and (iii) a Parameterized Deep-Q Network for the Deep RL planner.
    Step 4: Train Models:
      Search Network: Train the Sorting Network (SRTN) using cross-entropy loss and the Scoring Network (SCN) using mean square error loss on the Search Network Dataset. Use an Adam optimizer with specific learning rates and weight decay rates.
      Graph Representation Network: Train the Graph Siamese Encoder Network (GSEN) and the Residual Geodesic Distance Network (RGDN) using mean square error loss on the Graph Dataset. Use an Adam optimizer with a specified learning rate.
      Deep RL and Proxy Reward Network: Train the Deep RL network using a 3-step method involving ϵ-greedy exploration, updating Q-values with predicted rewards from the Proxy Reward Network, and using polyak averaging to update target networks. Train the Proxy Reward Network using cluster-biased return decomposition (CB-RD) to generate episodic rewards.
    Step 5: Get Results: Evaluate the models on the RoPOR benchmark dataset. Metrics include Success measured by the inverse Number of Steps (SNS), Efficiency in Number of Re-plans (ENR), and Absolute Traversal Cost (ATC).
    Step 6: Analyze Results: Compare the performance of the proposed method against existing methods (Weihs et al., Gadre et al., Sarch et al., Ghosh et al.) and ablation studies (Ours-GT, Ours-RS, Ours-GE, Ours-DR) in terms of SNS, ENR, and ATC. Analyze the impact of different components and hyper-parameters on the overall performance.


paper 2:
LLM-Oriented Retrieval Tuner:
  Title: LLM-Oriented Retrieval Tuner
  Problem Statement: Dense Retrieval (DR) is a promising tool to enhance the memorization capacity of Large Language Models (LLM) such as GPT-3 and GPT-4 by incorporating external memories. However, integrating retrieval and generation tasks in a shared LLM remains an open challenge due to the paradigm discrepancy between text generation and DR.
  Motivation: Existing methods typically require fine-tuning LLMs as retrieval-specific representation models, which is feasible but suboptimal. LLMs are designed to maximize the likelihood of generating the next token based on context, while DR involves mapping text into a vector space where semantically similar texts are closer together. This divergence makes it difficult for a single LLM to handle both tasks efficiently, leading to additional model parameters and longer inference times. The proposed method, LMORT, aims to decouple DR capacity from the base LLM and non-invasively coordinate the LLM's layers towards a unified DR space, achieving efficient and effective DR without tuning the LLM itself.
  Proposed Method: LMORT (LLM-Oriented Retrieval Tuner) decouples the DR capacity from the base LLM and coordinates the optimally aligned and uniform layers of the LLM towards a unified DR space. It uses a Transformer-like structure with two bidirectional attention operations in each layer: self-attention on the LLM's optimal alignment layer and cross-attention on the LLM's best uniformity layer. This approach merges the alignment and uniformity properties of the frozen LLM into a unified space conducive to effective retrieval.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use six zero-shot retrieval datasets from the BEIR benchmark: TREC-COVID, NFCorpus, FiQA, ArguAna, SciFact, and SCIDOCS. For training, use the MS MARCO dataset with about 500k web query-positive passages and training negatives released by sentence-transformers.
    Step 2: Select Models: Focus on three LLMs: GPT2-Large (0.75B), GPT2-XL (1.5B), and GPT-j-6B (6B).
    Step 3: Determine Optimal Layers: Analyze the alignment and uniformity losses for each layer of the LLMs to identify the optimal alignment and uniformity layers. For GPT2-Large, the optimal layers are #36 (alignment) and #37 (uniformity). For GPT2-XL, they are #4 (alignment) and #49 (uniformity). For GPT-j-6B, they are #1 (alignment) and #29 (uniformity).
    Step 4: Construct LMORT: Build LMORT with a multi-layer architecture on top of the LLM's optimal alignment and uniformity layers. Each LMORT layer contains self-bi-attention on the alignment layer and cross-bi-attention on the uniformity layer, followed by a feed-forward layer.
    Step 5: Train LMORT: Fine-tune LMORT using the standard DR training method. Optimize the parameters of LMORT while keeping the LLM parameters frozen. Use a batch size of 8, a learning rate of 5e-6, and train for 3 epochs. Evaluate the models using the checkpoint from the last training step.
    Step 6: Evaluate Performance: Evaluate the zero-shot retrieval performance of LMORT on the six BEIR datasets using the NDCG@10 metric. Compare the results with strong DR baselines like BM25, DPR, GTR-XXL, and cpt-text-L.
    Step 7: Analyze Results: Analyze the alignment and uniformity losses in the output layer of LMORT and compare them to the optimal alignment and uniformity layers of the LLM. Assess the parameter and training efficiency of LMORT, including the impact of dimensionality reduction on performance and training time.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
  9%|▊         | 11/126 [06:40<1:09:19, 36.17s/it]
full response:  paper 1 meta review:
 This paper proposes a novel hierarchical task planner for efficient room rearrangement under partial observability. The key contributions include a Search Network that leverages commonsense knowledge to predict unseen object locations, a Deep RL network trained with a proxy reward for action sequence planning, a graph-based state representation for scalability, and a sample-efficient cluster-biased sampling technique for training. The experiments are well-designed, introducing a new benchmark dataset (RoPOR) and comparing against existing methods and ablation studies. The results demonstrate the effectiveness of the proposed method in terms of success, efficiency, and traversal cost. Overall, this is a strong paper with clear motivation, novel contributions, and thorough evaluation.
Justification for why not higher score: The paper could benefit from additional analysis on the generalizability of the method to more complex and diverse room layouts and object types.
Justification for why not lower score: The paper addresses an important problem, proposes a well-motivated and novel solution, and provides extensive experimental validation.

paper 1 score:  8.0



paper 2 meta review:
 This paper introduces LMORT, a novel approach to integrate dense retrieval (DR) capabilities into large language models (LLMs) without fine-tuning the LLM itself. The key idea is to decouple the DR capacity from the base LLM and coordinate the optimally aligned and uniform layers of the LLM towards a unified DR space using a Transformer-like structure with bidirectional attention operations. The experiments are conducted on six zero-shot retrieval datasets from the BEIR benchmark, demonstrating the effectiveness of LMORT compared to strong DR baselines. The paper also analyzes the alignment and uniformity properties of LMORT and assesses its parameter and training efficiency. While the proposed method is interesting and shows promising results, the paper could benefit from a more detailed analysis of the impact of different design choices and hyperparameters on the performance.
Justification for why not higher score: The paper lacks a deeper analysis of the design choices and hyperparameters, and the evaluation is limited to zero-shot retrieval tasks.
Justification for why not lower score: The paper addresses an important challenge in integrating DR capabilities into LLMs, proposes a novel and well-motivated solution, and provides experimental validation on multiple datasets.

paper 2 score:  7.0


1
predicted:  1
label:  1
cost:  0.31390500000000005
You are a reviewer specialized in Natural Language Processing and Large Language Models. You are given two project summaries. One of them is accepted by a top AI conference (like ICLR or ACL) and the other one is rejected. Your task is to identify the one that has been accepted.
Here are some examples:
paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2

The two project summaries given to you are:

paper 1:
A Path Toward Primitive Machine Intelligence: LMM Not LLM Is What You Need:
  Title: A Path Toward Primitive Machine Intelligence: LMM Not LLM Is What You Need
  Problem Statement: The paper addresses the challenge of developing machine intelligence for chemosensing, specifically the ability to detect and interpret chemical compositions, which is a primitive form of intelligence where animals, particularly dogs, excel. The problem is interesting and important because current machine learning models, such as large language models (LLMs) and deep reinforcement learning, have shown super-human capabilities in other domains but lack interpretability and efficiency in chemosensing tasks.
  Motivation: Existing methods using deep learning on large datasets of hyperspectral hardware and spectrometers can potentially create artificial noses but suffer from a lack of interpretability. The inspiration behind the new proposed method is to use linear mixture models (LMMs) to build an engineering theory of cognitive development for chemosensing. The proposed method is expected to work better because it offers a mathematically tractable and interpretable framework for understanding chemical mixtures, unlike the black-box nature of deep learning models.
  Proposed Method: The proposed method involves using linear mixture models (LMMs) to develop a cognitive theory for chemosensing. The approach is inspired by Jean Piaget’s theory of cognitive development and adapts it to the context of chemical mixtures. The method involves four stages: 1) Inferring the number of chemicals in a set of mixtures, 2) Learning spectral representations of the chemicals, 3) Estimating the concentration of a chemical in a mixture, and 4) Interpreting why a mixture is good or bad by reasoning from past examples. The method emphasizes foundational and analytical theory over empirical successes on real-world datasets.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Create synthetic datasets using Algorithm 1 provided in the paper. The datasets should simulate linearly mixed observations and their underlying chemical concentrations. The parameters for the synthetic datasets include the number of chemicals (M), number of sensor channels (N), dynamic range of a pure spectrum (R), sensitivity of chemical concentration standard deviations (S), and the number of samples over time (T).
    Step 2: Model Architecture and Data Construction: Use the Wolfram Language to simulate the LMM given the hardware parameters. Generate pure spectra for each chemical and sample chemical concentrations from uniform distributions. Calculate the signal intensity for each sample. The dataset and Wolfram Language code for the analysis are provided in the supplementary file.
    Step 3: Infer the Number of Chemicals: Perform an eigenvalue cutoff of the sample covariance matrix to infer the chemical dimension (M). Rescale the observations to have a unit mean value at each sensory channel. Evaluate methods for dimensionality estimation, including Malinowski indicator function, Akaike information criterion, minimum description length, HySime, and random matrix theory.
    Step 4: Learn Spectral Representations: Solve the overdetermined system to estimate the pure spectra (⃗am). Evaluate existing methods such as nonnegative matrix factorization (NMF), independent component analysis (ICA), and principal component analysis (PCA). Develop novel update rules for ICA and NMF to improve convergence.
    Step 5: Estimate Chemical Concentrations: Use the dot product of the observations with an ideal pseudo-inverse spectrum to estimate the underlying chemical concentrations. Analyze the error scaling law to guarantee parts-per-trillion sensitivity. Investigate the limits of accurate estimation in the presence of noisy observations.
    Step 6: Interpret Mixture Quality: Develop mathematical models to interpret why a mixture is good or bad by reasoning from past examples. Test the models on real-world open-access datasets, such as spectral measurements on the skin to detect skin diseases or predicting pH levels of farmland patches using hyperspectral images. Focus on datasets where the response units are binary for initial experiments.


paper 2:
Knowledge Fusion of Large Language Models:
  Title: Knowledge Fusion of Large Language Models
  Problem Statement: Training large language models (LLMs) from scratch is costly and often results in redundant capabilities. The paper addresses the challenge of merging existing pre-trained LLMs with different architectures into a single, more potent model.
  Motivation: Existing methods like ensemble techniques and weight merging are impractical for LLMs due to their substantial memory and inference time requirements, and they often assume uniform network architectures. The proposed method, knowledge fusion, aims to combine the capabilities of existing LLMs and transfer them into a single LLM, potentially elevating the target model's capabilities beyond those of any individual source LLM.
  Proposed Method: The proposed method, FUSELLM, leverages the generative distributions of source LLMs to externalize their collective knowledge and unique strengths. This knowledge is then transferred to a target LLM through lightweight continual training. The method involves aligning tokenizations from different LLMs and fusing their probability distributions. The fusion process emphasizes minimizing the divergence between the target LLM’s probabilistic distributions and those of the source LLMs.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use MiniPile, a compact and diverse dataset curated from The Pile, consisting of approximately 1 million documents across 22 domains and 1.8 billion tokens.
    Step 2: Select Models: Choose three popular open-source LLMs with different architectures: Llama-2, OpenLLaMA, and MPT. Use another Llama-2 7B as the target LLM.
    Step 3: Token Alignment: Align tokens across multiple LLMs using the MinED (minimum edit distance) strategy to foster coherent probabilistic interpretations across models.
    Step 4: Fusion Function: Fuse the probabilistic distribution matrices using the MinCE (minimum cross-entropy) function, which outputs the distribution matrix with the minimum cross-entropy score.
    Step 5: Training: Continually train the target LLM on the MiniPile dataset using a batch size of 128 and a maximum length of 2048 on 8 NVIDIA A100 GPUs. Use a combination of the causal language modeling objective and the fusion objective, with a weighted combination parameter λ set to 0.9.
    Step 6: Evaluation: Evaluate the performance of FUSELLM on three benchmarks: Big-Bench Hard (BBH) for reasoning, Common Sense (CS) for commonsense capability, and MultiPL-E (ME) for code generation. Use metrics like exact match (EM) accuracy for BBH and CS, and pass@1 for ME.
    Step 7: Analyze Results: Compare the performance of FUSELLM with the original LLMs and the continually trained Llama-2 (Llama-2 CLM). Analyze the improvements in reasoning, commonsense, and code generation tasks.


Now decide which one is the accepted idea. Follow the above examples: give a meta-review and score to each paper, and then start a new line and directly return a number 1 or 2 to indicate the accepted idea and end the response.
