python3 src/lit_review.py \
 --topic_description "novel prompting methods that can improve factuality and reduce hallucination of large language models" \
 --cache_name "factuality_prompting" \
 --track "method" \
 --method "prompting" \
 --max_paper_bank_size 10 \
 --print_all

# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods that can improve factuality and reduce hallucination of large language models" \
#  --cache_name "factuality_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel inference-time intervention or decoding methods that can improve factuality and reduce hallucination of large language models" \
#  --cache_name "factuality_intervention" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all



# python3 src/lit_review.py \
#  --topic_description "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language model" \
#  --cache_name "uncertainty_prompting" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods that can better quantify uncertainty or calibrate the confidence of large language model" \
#  --cache_name "uncertainty_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel calibration methods that can better quantify uncertainty or confidence of large language model" \
#  --cache_name "uncertainty_calibration" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all




# python3 src/lit_review.py \
#  --topic_description "novel prompting methods to jailbreak or adversarially attack large language models" \
#  --cache_name "attack_prompting" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all



# python3 src/lit_review.py \
#  --topic_description "novel intervention methods that can better quantify uncertainty or calibrate the confidence of large language model" \
#  --cache_name "uncertainty_intervention" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods to jailbreak or adversarially attack large language models" \
#  --cache_name "attack_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel intervention methods to jailbreak or adversarially attack large language models" \
#  --cache_name "attack_intervention" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel prompting methods to defend against adversarial attacks or prompt injection on large language models and improve robustness" \
#  --cache_name "defense_prompting" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods to defend against adversarial attacks or prompt injection on large language models and improve robustness" \
#  --cache_name "defense_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel intervention methods to defend against adversarial attacks or prompt injection on large language models and improve robustness" \
#  --cache_name "defense_intervention" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all
