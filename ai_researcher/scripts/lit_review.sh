# python3 src/lit_review.py \
#  --topic_description "novel prompting methods that can improve factuality and reduce hallucination of large language models" \
#  --cache_name "factuality_prompting" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods that can improve factuality and reduce hallucination of large language models" \
#  --cache_name "factuality_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel inference-time intervention or decoding methods that can improve factuality and reduce hallucination of large language models" \
#  --cache_name "factuality_intervention" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all



# python3 src/lit_review.py \
#  --topic_description "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language model" \
#  --cache_name "uncertainty_prompting" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel finetuning methods that can better quantify uncertainty or calibrate the confidence of large language model" \
#  --cache_name "uncertainty_finetuning" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel calibration methods that can better quantify uncertainty or confidence of large language model" \
#  --cache_name "uncertainty_calibration" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all




python3 src/lit_review.py \
 --topic_description "novel prompting methods to jailbreak or adversarially attack large language models" \
 --cache_name "attack_prompting" \
 --track "method" \
 --max_paper_bank_size 70 \
 --print_all


# python3 src/lit_review.py \
#  --topic_description "novel methods to jailbreak or adversarially attack large language models" \
#  --cache_name "attack" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all



# python3 src/lit_review.py \
#  --topic_description "probing social biases and fairness issues of large language models through prompting" \
#  --cache_name "bias" \
#  --track "analysis" \
#  --max_paper_bank_size 70 \
#  --print_all

# python3 src/lit_review.py \
#  --topic_description "novel prompting methods to make language models solve problems more efficiently" \
#  --cache_name "efficiency" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel prompting methods to mitigate social biases and stereotypes of large language models" \
#  --cache_name "bias" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


# python3 src/lit_review.py \
#  --topic_description "novel prompting methods to defend against adversarial attacks or prompt injection on large language models and improve their robustness" \
#  --cache_name "defense" \
#  --track "method" \
#  --max_paper_bank_size 70 \
#  --print_all


