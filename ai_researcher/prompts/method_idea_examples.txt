{
	"According-to Prompting": {
		"Problem": "Large Language Models may hallucinate and generate fake information on factual QA tasks.",
		"Existing Methods": "For factual QA tasks, a typical baseline is to directly prompt large language models with the question and ask for an answer.",
		"Motivation": "Large language models have already been pretrained on massive amount of factual data. We can potentially improve their factuality by encouraging them to quote directly from underlying trusted resources seen during training.",
		"Proposed Method": "We propose according-to prompting: directing LLMs to ground responses against previously observed text by prepending instructions like \"According to Wikipedia,\" and letting the models quote from Wikipedia when generating the answer.",
        "Experiment": "Compare the proposed method with the baseline of direct prompting on several factual QA tasks to see whether according-to prompting can improve accuracy."
	},
    "Step-Back Prompting": {
        "Problem": "Complex multi-step reasoning remains challenging for even the stateof-the-art LLMs.",
		"Existing Methods": "Techniques such as Chain-of-Thought prompting were introduced to produce a coherent series of intermediate reasoning steps to increase the success rate of following the right decoding path.",
		"Motivation": "Step-back prompting is motivated by the observation that many tasks contain a lot of details, and are hard for LLMs to retrieve relevant facts to tackle the task. Moreover, when faced with challenging tasks humans often step back and do abstractions to arrive at high-level concepts and principles to guide the process.",
		"Proposed Method": "We propose step-back prompting to ground reasoning on abstractions to reduce the chance of making errors in the intermediate reasoning steps. In short, step-back prompting consists two simple steps: Abstraction: Instead of addressing the question directly, we first prompt the LLM to ask a generic step-back question about a higher-level concept or principles, and retrieve relevant facts about the high-level concept or principles. Reasoning: Grounded on the facts regarding high-level concept or principles, the LLM can reason about the solution to the original question. We term this Abstraction-grounded Reasoning.",
		"Experiment": "Compare the proposed method with the baselines of few-shot prompting and Chain-of-Thought prompting on several complex multi-step reasoning tasks to see whether step-back prompting can improve accuracy."
	},
	"Analogical Prompting": {
		"Problem": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process.",
		"Existing Methods": "Recently, chain-of-thought (CoT) prompting has demonstrated LLMs’ abilities to tackle complex tasks, such as solving math problems, by prompting them to generate intermediate reasoning steps.",
		"Motivation": "However, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars of reasoning, and minimizing the need for manual labeling. This raises a research question: can we achieve the best of both worlds and automate the generation of relevant exemplars to guide LLMs’ reasoning process? Our inspiration comes from analogical reasoning in psychology, a concept where humans draw from relevant past experiences to tackle new problems. . They also recall high-level knowledge, such as the need to find the side length to calculate a square’s area. Our idea is to prompt LLMs to mimic this reasoning process to effectively solve new problems.",
		"Proposed Method": "Concretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context, using instructions like \"# Recall relevant problems and solutions:...\", and then proceed to solve the original problem. Simultaneously, we can also prompt LLMs to generate high-level knowledge that complements specific exemplars, using instructions like \"# Provide a tutorial:...\".",
		"Experiment": "Compare analogical prompting with the baselines of few-shot prompting and CoT prompting on math reasoning and code generation benchmarks."
	},
	"Self-Reflection Prompting": {
		"Problem": "Large language models have been increasingly used to interact with external environments as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning.",
		"Existing Methods": "Recent works such as ReAct, SayCan, Toolformer, HuggingGPT, generative agents, and WebGPT have demonstrated the feasibility of autonomous decision-making agents that are built on top of a large language model (LLM) core. These methods use LLMs to generate text and ‘actions‘ that can be used in API calls and executed in an environment. Since they rely on massive models with an enormous number of parameters, such approaches have been so far limited to using in-context examples as a way of teaching the agents, since more traditional optimization schemes like reinforcement learning with gradient descent require substantial amounts of compute and time.",
		"Motivation": "We take inspiration from how humans iteratively learn to accomplish complex tasks in a few-shot manner – by reflecting on their previous failures in order to form an improved plan of attack for the next attempt.",
		"Proposed Method": "We propose a new approach called Reflexion that uses verbal reinforcement to help agents learn from prior failings. Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode. This self-reflective feedback acts as a ‘semantic’ gradient signal by providing the agent with a concrete direction to improve upon, helping it learn from prior mistakes to perform better on the task.",
		"Experiment": "Compare the proposed method with ReAct baseline on on the HumanEval coding benchmark and the HotpotQA multi-step QA benchmark."
	},
	"Selective Attention Prompting": {
		"Problem": "Large Language Models (LLMs) are highly capable, yet they are still susceptible to making simple mistakes, which seem to display weak reasoning abilities. For example, they can be swayed to make erroneous judgments by irrelevant context, or by preference or opinion inherent in the input prompt, in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input.",
		"Existing Methods": "Several approaches try to mitigate these issues through adding more supervised training data or reinforcement learning strategies.",
		"Motivation": "We posit that the underlying problem is inherent in the way the transformer itself is built, and in particular its attention mechanism. That is, soft attention tends to assign probability to a large portion of the context, including irrelevant portions, tends to overly focus on repeated tokens partly due to the way it is trained, and partly due to the position encoding mechanism is also inclined to treat the context as a bag-of-words when it should not.",
		"Proposed Method": "In this work, we thus investigate a radically different approach to attention mechanisms: performing attention by using the LLM as a natural language reasoner. Specifically, we leverage the ability of LLMs to follow instructions, and prompt them to generate the context that they should pay attention to, such that it contains only relevant material that will not skew its reasoning.",
		"Experiment": "Compare with grounding on the original documents without using selective attention prompting on: 1) the modified TriviQA dataset that includes distractor opinion in the question; 2) on longform generation of arguments that contain distractor input sentiment; and 3) on math word problems from GSM8K with in-topic irrelevant sentences."
	},
	"Chain-of-Verification Prompting": {
		"Problem": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.",
		"Existing Methods": "A majority of the methods for reducing hallucination can be divided into roughly three categories: training-time correction, generation-time correction and via augmentation (tool-use).",
		"Motivation": "A key observation is that large language models, when suitably prompted, can both generate and execute a plan of how to verify themselves in order to check their own work, and finally incorporate this analysis into an improved response.",
		"Proposed Method": "Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps: 1. Generate Baseline Response: Given a query, generate the response using the LLM. 2. Plan Verifications: Given both query and baseline response, generate a list of verification questions that could help to self-analyze if there are any mistakes in the original response. 3. Execute Verifications: Answer each verification question in turn, and hence check the answer against the original response to check for inconsistencies or mistakes. 4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a revised response incorporating the verification results. Each of these steps is performed by prompting the same LLM in different ways to obtain the desired response.",
		"Experiment": "Compare with zero-shot prompting, Chain-of-Thought, and few-shot prompting on the MultiSpanQA dataset on closed-book QA and FactScore dataset on generating biographies."
	}
}

