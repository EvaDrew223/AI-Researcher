{
    "According-to Prompting": {
        "Title": "\"According to...\" Prompting Language Models Improves Quoting for Factual Knowledge",
        "Problem Statement": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.",
        "Motivation": "Recent work has attempted to address this issue by augmenting them with retrieval, however, these models still struggle with hallucination problems in practice. Our study is inspired by two recent research areas. First, larger LLMs can be more effectively guided using natural language prompts. Second, as LLMs grow in size, their ability to remember facts and statements from pre-training improves. Thus, we seek to steer LLMs to use their memorization for a positive purpose: producing more grounded outputs.",
        "Proposed Method": "We prompt them using wording that encourage grounding such as \"Respond by using information from Wikipedia in your response\" after the given input question. We call this strategy according-to prompting.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We use a variety of knowledge-intensive QA datasets, including ELI5, Natural Questions (NQ), TriviaQA, and HotpotQA. We can use answer exact match and F1 as the metrics.",
            "Step 2: Construct Prompts": "The baseline prompt is we just ask the question. The according to prompt will append the grounding instruction after the question. As prompting is notoriously brittle, we provide a number of grounding prompts to test whether these prompts provide consistent gains or are merely prompting artifacts, such as: {\"Based on evidence from Wikipedia:\", \"I found some results for that on Wikipedia. Here’s a direct quote:\", \"Answer according to Wikipedia.\"}.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on the QA datasets with both the baseline and according-to prompts.",
            "Step 5: Analyze Results": "Compare whether according-to prompts lead to better performance than the baseline prompts."
        },
        "Fallback Plan": "If the proposed according-to prompts do not work well, we can turn the project into an analysis to understand the impact of such grounding and also anti-grounding prompts. We can compare with changing grounding prompts into anto-grounding prompts, such as \"Respond by using information from Reddit in your response.\" or \"Respond without using any information from Wikipedia in your response.\" and see if we see opposite effects."
    },
    "Step-Back Prompting": {
        "Title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
        "Problem Statement": "Complex multi-step reasoning remains challenging for even the stateof-the-art LLMs.",
        "Motivation": "Techniques such as Chain-of-Thought prompting were introduced to produce a coherent series of intermediate reasoning steps to increase the success rate of following the right decoding path. However, these techniques are not effectively leveraging common abstractions among problems. We take inspiration from human cognitive skilss, where abstraction is ubiquitous to humans’ ability to process vast amount of information and derive general rules, and principles. When faced with challenging tasks humans often step back and do abstractions to arrive at high-level concepts and principles to guide the process.",
        "Proposed Method": "This work explores how LLMs can tackle complex tasks involving many low-level details through a two-step process of abstraction-and-reasoning. The first step is to teach LLMs to step back, and derive high-level abstractions such as concepts and first principles from the specific example. The second step is to leverage the reasoning ability to ground the solution on the high-level concepts and first principles. We use few-shot exemplar demonstrations to execute STEP-BACK PROMPTING on LLMs.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We experiment with three diverse tasks: (1) STEM questions (MMLU); (2) Knowledge QA (TimeQA and Situated QA); (3) Multihop reasoning (MuSiQue and StrategyQA).",
            "Step 2: Construct Prompts": "We include several baseline prompting methods: (1) direct prompting: query the model with the question only; (2) 1-shot prompting: have a single demonstration exemplar of question-answer included in the prompt; (3) Zero-shot CoT: Append “Let’s think step by step” to the question.\nThen we implement our step-back prompting. In step 1, we do abstraction by prompting the language model to ask a stepback question. We can prompt with demo examples. E.g., given the question \"Estella Leopold went to which school between Aug 1954 and Nov 1954?\", the stepback question can be \"What was Estella Leopold’s education history?\". In step 2, we perform reasoning by appending the answer to the stepback question to the original question and then prompt for the final answer.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on the QA datasets with both the baselines and step-back prompts.",
            "Step 5: Analyze Results": "Compare whether step-back prompts lead to better performance than the baseline prompts."
        },
        "Fallback Plan": "If the proposed step-back prompts do not work well, we can turn the project into an analyse why. For example, we can analyse the generated step-back questions and corresponding answers to see if the abstraction questions are relevant, and whether the generated answers are indeed relevant and factual."
    },
    "Analogical Prompting": {
        "Title": "Large Language Models as Analogical Reasoners",
        "Problem Statement": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. We aim to explore whether LLMs can be prompted to reason analogically, without the need for explicit exemplars.",
        "Motivation": "Recently, chain-of-thought (CoT) prompting has demonstrated LLMs’ abilities to tackle complex tasks, such as solving math problems, by prompting them to generate intermediate reasoning steps. However, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars of reasoning, and minimizing the need for manual labeling. This raises a research question: can we achieve the best of both worlds and automate the generation of relevant exemplars to guide LLMs’ reasoning process? Our inspiration comes from analogical reasoning in psychology, a concept where humans draw from relevant past experiences to tackle new problems. They also recall high-level knowledge, such as the need to find the side length to calculate a square’s area. Our idea is to prompt LLMs to mimic this reasoning process to effectively solve new problems.",
        "Proposed Method": "Concretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context, using instructions like \"# Recall relevant problems and solutions:...\", and then proceed to solve the original problem. Simultaneously, we can also prompt LLMs to generate high-level knowledge that complements specific exemplars, using instructions like \"# Provide a tutorial:...\". This could be particularly useful for complex tasks like code generation.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate the proposed approach in various reasoning-intensive tasks, including mathematical problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
            "Step 2: Construct Prompts": "We include several baselines: (1) zero-shot prompting (directly prompt with the question); (2) zero-shot CoT: append \"Let’s think step by step\" to the question; (3) few-shot CoT: prepend several labeled exemplars where the answers come with reasoning chains. Then we implement our analogical prompting.\nGiven a target problem to solve x, our prompt augments it with instructions like:\n# Problem: [x]\n# Relevant problems: Recall three relevant and distinct problems. For each problem, describe it and explain the solution.\n# Solve the initial problem:\nFor code generation, we can add:\n# Tutorial: Identify core concepts in the problem and provide a tutorial.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks."
        },
        "Fallback Plan": "If the proposed analogical prompts do not work well, we can turn the project into an analysis to understand why. We can analyze the generated relevant problems and solutions to see if they are indeed relevant, and whether the generated solutions are indeed correct and useful. We can also analyze the generated tutorials to see if they are indeed relevant and factually correct."
    },
    "Self-Refinement Prompting": {
        "Title": "SELF-REFINE: Iterative Refinement with Self-Feedback",
        "Problem Statement": "Like humans, large language models (LLMs) do not always generate the best output on their first try. We are interested in how to leverage the LLM itself to improve its own output.",
        "Motivation": "LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement — i.e., iteratively mapping a candidate output to an improved one — to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data. Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations, which may not always be feasible to obtain. Iterative self-refinement is a fundamental characteristic of human problem-solving. Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, When writing code, a programmer may implement an initial \"quick and dirty\" implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable.",
        "Proposed Method": "Inspired by this principle, we propose Self-Refine: an iterative self-refinement algorithm that alternates between two generative steps – FEEDBACK and REFINE. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting to guide M to both generate feedback and incorporate the feedback into an improved draft.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on the GSM8K dataset for mathematical problem solving, and the HumanEval dataset for code generation.",
            "Step 2: Construct Prompts": "For baseline, we prompt the model to generate the output directly. For the proposed method, we first prompt the model to generate an initial answer, then we prompt the same model with few-shot examples to generate feedback on the initial draft, and finally we prompt the model to refine the initial draft conditioned on the input question, original response, and the feedback. We repeat this loop until a stopping condition is met (e.g., set a max number of steps).",
            "Step 3: Select Models": "We use three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks."
        },
        "Fallback Plan": "If the proposed self-refinement prompts do not improve over baselines, we can analyse whether it is because of the feedback generation step, or the refinement step. For example, we can also analyse the generated feedback to see if it is indeed relevant, and whether the generated refined drafts are indeed improved over the initial drafts to address the feedback. We can also tune the stopping condition to see if it is too early to stop the refinement process or if refining too many steps hurts performance."
    },
    "Selective Attention Prompting": {
        "Title": "Reading Selectively: Selective Attention Prompting for Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) are highly capable, yet they are still susceptible to making simple mistakes, which seem to display weak reasoning abilities. For example, they can be swayed to make erroneous judgments by irrelevant context, or by preference or opinion inherent in the input prompt, in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input.",
        "Motivation": "While several approaches try to mitigate these issues through adding more supervised training data or reinforcement learning strategies, we posit that the underlying problem is inherent in the way the transformer itself is built, and in particular its attention mechanism. That is, soft attention tends to assign probability to a large portion of the context, including irrelevant portions, tends to overly focus on repeated tokens partly due to the way it is trained, and partly due to the position encoding mechanism is also inclined to treat the context as a bag-of-words when it should not. We want to resolve this by using the LLM as a natural language reasoner to select which parts of the contexts to read.",
        "Proposed Method": "In this work, we investigate a radically different approach to attention mechanisms: performing attention by using the LLM as a natural language reasoner. Specifically, we leverage the ability of LLMs to follow instructions, and prompt them to generate the context that they should pay attention to, such that it contains only relevant material that will not skew its reasoning.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on: 1) the modified TriviQA dataset that includes distractor opinion in the question; 2) on longform generation of arguments (SycophancyEval) that contain distractor input sentiment; and 3) on math word problems from GSM8K with in-topic irrelevant sentences. These tasks can help us understand the impact of context selection.",
            "Step 2: Construct Prompts": "We have two baselines: (1) no context: only give the question; (2) full context: give the full context. For the proposed method, we prompt the model to generate the context that it should pay attention to such that irrelevant parts of the context that will adversely affect the output are removed, and then we prompt the model to generate the answer conditioned on the input question and the generated context.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        },
        "Fallback Plan": "If the proposed method does not help, analyze the selected contexts to see if they indeed contain the necessary information for answering the questions. This can help us debug the proposed method or turn this into interesting analysis on the model's ability to select relevant information in context."
    },
    "Chain-of-Verification Prompting": {
        "Title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "Problem Statement": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.",
        "Motivation": "A majority of the methods for reducing hallucination can be divided into roughly three categories: training-time correction, generation-time correction and via augmentation (tool-use). We want to take a simpler approach that fully leverages the power of LLM itself. Our key motivation is that large language models, when suitably prompted, can both generate and execute a plan of how to verify themselves in order to check their own work, and finally incorporate this analysis into an improved response.",
        "Proposed Method": "Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps: 1. Generate Baseline Response: Given a query, generate the response using the LLM. 2. Plan Verifications: Given both query and baseline response, generate a list of verification questions that could help to self-analyze if there are any mistakes in the original response. 3. Execute Verifications: Answer each verification question in turn, and hence check the answer against the original response to check for inconsistencies or mistakes. 4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a revised response incorporating the verification results. Each of these steps is performed by prompting the same LLM in different ways to obtain the desired response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We choose datasets that evaluate factual correctness, including MultiSpanQA dataset on closed-book QA and FactScore dataset on generating biographies.",
            "Step 2: Construct Prompts": "For baseline, we use direct prompting where given a query, we generate left-to-right as usual using the LLM, with no special tricks. Given such baseline generations are typically prone to hallucination, CoVe attempts to identify these hallucinations, and correct them, in the following steps.\n(1) Plan Verifications: Conditioned on the original query and the baseline response, the model is prompted to generate a series of verification questions that test the factual claims in the original baseline response.\n(2) Execute Verifications: Given the planned verification questions, the next step is to answer them in order to assess if any hallucinations exist. The planning prompt conditions on the baseline response in the first step. The verification questions generated from planning are answered in the second step, where crucially the context given to the LLM prompt only contains the questions, and not the original baseline response and hence cannot repeat those answers directly.\n(3) Generate Final Verified Response: Finally, the improved response that takes verification into account is generated. This is executed by a final few-shot prompt where the context takes into account all of the previous reasoning steps, the baseline response and verification question answer pairs, so that the corrections can take place.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        },
        "Fallback Plan": "If the proposed method does not help as compared to the baseline, analyze each step of the CoVe process to see if the verification questions are relevant, if the answers to the verification questions are correct, and whether the generated final verified response is indeed improved over the baseline response by considering the verification QA pairs. This can help us debug the proposed method or turn this into interesting analysis on the model's ability to verify and correct its own responses."
    }
}