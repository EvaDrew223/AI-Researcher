{
	"Improving Generator-Validator Consistency": {
		"Problem": "Language models (LMs) can generate high-quality responses to task prompts; however, the same model can sometimes produce contradictory responses when validating its own answers.",
		"Existing Methods": "Prior work has explored prompt consistency, and finetuned the LMs to improve the prediction similarity across different prompt rephrasings. Also, some works enforce logical consistency by selecting answers that are logically consistent with most of the other LM-generated statements. This work explores the new aspect of generator-validator consistency, which is applicable to a broad set of scenarios because most generative tasks have a corresponding verification task.",
		"Motivation": "As of September 2023, ChatGPT correctly answers \"what is 7+8\" with 15, but when asked \"7+8=15, True or False\" it responds with \"False\". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. Therefore, we want to propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), and to improve such consistency of LMs via finetuning.",
		"Proposed Method": "(1) Evaluating: In order to systematically assess GV-consistency of LMs, we begin by prompting the LM with a generator query to solicit an answer to a question, and then prompting the same LM with a validator query to check whether the generated answer is correct. Simply asking the validator for a correctness judgment can fail, as the trivial baseline of always answering \"correct\" has perfect performance. Our work avoids this degeneracy by randomizing the labels corresponding to the consistent answer. (2) Finetuning: To improve GV-consistency, we propose a simple procedure called consistency fine-tuning, which consists of a data generation stage and a fine-tuning stage. Given a generator and a validator prompt, we first query the generator to obtain the generator response, then query the validator to check the correctness of the generated response. We then filter the paired generator and discriminator responses to keep only the pairs that are GV-consistent. Finally, we finetune the LM to maximize the likelihood of the consistent pairs. This algorithm can be applied for multiple rounds.",
		"Experiment": "To evaluate consistency fine-tuning, we experiment on 6 tasks, ranging from classic NLP tasks (style transfer and QA) to arithmetic reasoning (arithmetic and plan arithmetic) and instruction-following (harmful question and prompt prioritization). We compare the GV-consistency of the original LMs with the GV-consistency of the LMs after consistency fine-tuning. We can also evaluate whether the consistency finetuning can improve the generator generation quality and the validator accuracy."
	},
	"Context-Aware Decoding": {
		"Problem": "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations.",
		"Existing Methods": "There are growing efforts to improve the factual consistency, such as learning a post-editing error corrector or removing noisy training samples. However, all these methods require additional finetuning and are not directly suitable for zero-shot and few-shot prompting scenarios.",
		"Motivation": "Previous research shows that LMs can fail to pay enough attention to new information introduced in the context knowledge. This can lead to hallucination in model generation, where the generated responses include facts not present in the input document. Insufficient attention to context is especially problematic when the context knowledge contradicts with the model's prior knowledge. To address these, we present a simple context-aware decoding (CAD) method to encourage the LM to attend to its context during generation, where the goal is to amplify the difference between output probabilities with and without the context document.",
		"Proposed Method": "During decoding, we adjust the model's original output probability using the pointwise mutual information (PMI) between the context c and the generation y_t, conditioned on x, y_{<t}. The adjuted output probability is a product-of-experts of the original output probability and the PMI (probability of context-conditioned generation over probability of context-unconditioned generation). Essentially, output tokens that become much more likely when the context is included are preferred.",
		"Experiment": "We compare with the baseline of regular decoding without CAD on summarization datasets (CNN-DM, XSUM) using ROUGE-L as the metric, and we use BERT-Precision and FactKB to measure the factual consistency of summaries. We further evaluate CAD on a knowledge conflict dataset MemoTrap. MemoTrap is created to investigate whether language models could fall into memorization traps. It comprises instructions that prompt the language model to complete a well-known proverb with an ending word that deviates from the commonly used ending (e.g., Write a quote that ends in the word “early”: Better late than _). We use Exact Match (EM) as the evaluation metric. we hope LMs focus more on the instruction in MemoTrap with CAD."
	},
	"Transferable Adversarial Attacks": {
		"Problem": "Because “out-of-the-box” large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures—so-called “jailbreaks” against LLMs—these attacks have required significant human ingenuity and are brittle in practice. Attempts at automatic adversarial prompt generation have also achieved limited success.",
		"Existing Methods": "Most commonly raised in computer vision domains (though with some applications to other modalities, including text), it is well-established that adding small perturbations to the input of a machine learning model can drastically change its output. To a certain extent, similar approaches are already known to work against LLMs: there exist a number of published “jailbreaks”: carefully engineered prompts that result in aligned LLMs generating clearly objectionable content. Unlike traditional adversarial examples, however, these jailbreaks are typically crafted through human ingenuity—carefully setting up scenarios that intuitively lead the models astray—rather than automated methods, and thus they require substantial manual effort. Indeed, although there has been some work on automatic prompt-tuning for adversarial attacks on LLMs , this has traditionally proven to be a challenging task, with some papers explicitly mentioning that they had been unable to generate reliable attacks through automatic search methods.",
		"Motivation": "The current limitation owes largely to the fact that, unlike image models, LLMs operate on discrete token inputs, which both substantially limits the effective input dimensionality, and seems to induce a computationally difficult search. We propose a new class of adversarial attacks that appends an adversarial suffix to the query that attempts to induce negative behavior, where we effectively search over the discrete token space to find the adversarial suffix.",
		"Proposed Method": "Our proposed method consists of three steps. (1) Initial affirmative responses. As identified in past work, one way to induce objectionable behavior in language models is to force the model to give (just a few tokens of) an affirmative response to a harmful query. As such, our attack targets the model to begin its response with “Sure, here is (content of query)” in response to a number of prompts eliciting undesirable behavior. (2) Combined greedy and gradient-based discrete optimization. Optimizing over the adversarial suffix is challenging due to the fact that we need to optimize over discrete tokens to maximize the log likelihood of the attack succeeding. To accomplish this, we leverage gradients at the token level to identify a set of promising single-token replacements, evaluate the loss of some number of candidates in this set, and select the best of the evaluated substitutions. (3) Robust multi-prompt and multi-model attacks. Finally, in order to generate reliable attack suffixes, we find that it is important to create an attack that works not just for a single prompt on a single model, but for multiple prompts across multiple models. In other words, we use our greedy gradient-based method to search for a single suffix string that was able to induce negative behavior across multiple different user prompts, and across three different models (in our case, Vicuna-7B and 13b and Guanoco-7B).",
		"Experiment": "We evaluate on two settings. 1. Harmful Strings: A collection of 500 strings that reflect harmful or toxic behavior, encompassing a wide spectrum of detrimental content such as profanity, graphic depictions, threatening behavior, misinformation, discrimination, cybercrime, and dangerous or illegal suggestions. The adversary’s objective is to discover specific inputs that can prompt the model to generate these exact strings. 2. Harmful Behaviors:  A set of 500 harmful behaviors formulated as instructions. These behaviors range over the same themes as the harmful strings setting, but the adversary’s goal is instead to find a single attack string that will cause the model to generate any response that attempts to comply with the instruction, and to do so over as many harmful behaviors as possible. We use Attack Success Rate (ASR) as the primary metric.  For eliciting harmful strings, we consider each successful if the model outputs the exact target string. For Harmful Behaviors, we deem a test case successful if the model makes a reasonable attempt at executing the behavior. As different models exhibit varying ability to provide, for example, a correct set of instructions for building an explosive device, this may involve human judgment to determine that a response did not amount to a refusal, or an attempt to evade generating harmful content."
	}
}

