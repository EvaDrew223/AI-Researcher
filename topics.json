{
    "method": {
        "Code prompting": "novel prompting methods for large language models to improve aspects of code generation",
        "Factuality prompting": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
        "Math prompting": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
        "Bias mitigation": "novel prompting methods to mitigate social biases and stereotypes of large language models",
        "Adversarial attack": "novel prompting methods to jailbreak or adversarially attack large language models as a way to identify their vulnerabilities",
        "Adversarial defense": "novel prompting methods to defend against adversarial attacks or prompt injection on large language models and improve their robustness",
        "Multilingual": "novel prompting methods to improve large language modelsâ€™ performance on multilingual tasks, or on low-resource languages or vernacular languages",
        "Multimodal": "novel prompting methods to improve large language models or vision-language models on multimodal tasks",
        "Uncertainty": "novel methods that can better quantify uncertainty or calibrate the confidence of large language models",
        "Efficiency": "novel prompting methods to make language models solve problems more efficiently"

    },
    "analysis": {
        "Safety": "probing security and privacy risks of large language models",
        "Bias": "probing social biases and fairness issues of large language models through prompting",
        "In-context learning": "empirical analysis to study the impact of each component of the prompt in in-context learning"
    }
}