{
    "timestamp": [
        "23/05/2025 15:37:39",
        "26/05/2025 18:15:10",
        "31/05/2025 21:28:33",
        "02/06/2025 17:00:02",
        "19/04/2025 23:39:57",
        "22/04/2025 10:59:43",
        "01/05/2025 08:32:30",
        "28/05/2025 12:19:59",
        "05/06/2025 22:23:50",
        "07/06/2025 12:59:12",
        "07/06/2025 18:56:36",
        "08/06/2025 12:26:58",
        "08/03/2025 16:27:24",
        "14/03/2025 07:57:30",
        "03/04/2025 10:24:10",
        "27/05/2025 10:53:55",
        "02/06/2025 18:05:16",
        "04/06/2025 17:52:00",
        "06/06/2025 15:50:43",
        "08/06/2025 21:01:39",
        "12/03/2025 01:46:39",
        "15/03/2025 13:26:09",
        "06/04/2025 14:23:50",
        "04/06/2025 16:46:53",
        "07/06/2025 22:14:58",
        "08/03/2025 16:59:58",
        "12/03/2025 23:39:43",
        "20/04/2025 17:31:52",
        "12/05/2025 00:06:25",
        "08/03/2025 03:16:38",
        "12/03/2025 00:32:45",
        "19/03/2025 22:41:39",
        "24/05/2025 15:18:51",
        "19/04/2025 23:13:13",
        "22/04/2025 10:23:03",
        "01/05/2025 13:54:06",
        "27/05/2025 16:29:52",
        "11/02/2025 00:36:48",
        "14/02/2025 21:38:18",
        "18/02/2025 01:04:37",
        "27/05/2025 11:35:08",
        "17/02/2025 17:51:58",
        "09/04/2025 11:59:37",
        "18/04/2025 21:22:37",
        "24/05/2025 15:47:33",
        "27/03/2025 15:19:39",
        "05/04/2025 20:25:08",
        "06/04/2025 10:49:29",
        "24/05/2025 16:11:04",
        "08/03/2025 15:49:14",
        "07/04/2025 14:04:32",
        "09/05/2025 01:27:24",
        "22/05/2025 00:07:48",
        "06/04/2025 14:46:42",
        "09/04/2025 11:15:57",
        "19/04/2025 00:20:53",
        "28/05/2025 16:43:27",
        "01/05/2025 23:53:02",
        "09/05/2025 14:19:53",
        "09/05/2025 18:25:22",
        "10/05/2025 16:36:47",
        "31/05/2025 20:53:46",
        "20/04/2025 02:39:02",
        "20/04/2025 21:42:11",
        "09/05/2025 18:01:06",
        "24/05/2025 06:58:44",
        "07/02/2025 14:25:50",
        "13/02/2025 08:40:15",
        "18/02/2025 20:28:29",
        "19/05/2025 19:36:25",
        "27/03/2025 12:55:18",
        "06/04/2025 14:44:41",
        "06/04/2025 22:35:48",
        "24/05/2025 03:42:37",
        "21/05/2025 21:21:54",
        "23/05/2025 20:14:39",
        "25/05/2025 06:11:01",
        "01/06/2025 14:03:32",
        "02/06/2025 21:41:53",
        "16/04/2025 23:41:23",
        "19/04/2025 22:59:18",
        "09/05/2025 17:31:59",
        "11/05/2025 22:24:57",
        "07/06/2025 21:39:01",
        "08/03/2025 15:12:58",
        "11/03/2025 10:16:07",
        "03/04/2025 17:28:08",
        "23/05/2025 11:59:49",
        "30/04/2025 15:12:10",
        "01/05/2025 14:45:59",
        "04/05/2025 14:33:19",
        "09/05/2025 11:07:53",
        "11/05/2025 19:00:59",
        "14/03/2025 01:56:05",
        "19/03/2025 23:00:06",
        "03/04/2025 22:43:25",
        "29/05/2025 11:13:43",
        "07/06/2025 13:57:08",
        "12/02/2025 13:36:40",
        "14/02/2025 20:46:11",
        "18/02/2025 17:51:45",
        "19/05/2025 20:49:00",
        "26/05/2025 11:13:20",
        "30/05/2025 09:47:14",
        "31/05/2025 20:09:50",
        "01/06/2025 01:34:48",
        "11/02/2025 01:30:19",
        "18/02/2025 21:52:20",
        "19/04/2025 00:28:58",
        "24/05/2025 11:30:46",
        "08/06/2025 13:12:52",
        "05/04/2025 17:39:08",
        "07/04/2025 16:39:46",
        "09/04/2025 21:16:40",
        "26/05/2025 19:27:43",
        "09/05/2025 02:01:47",
        "09/05/2025 16:25:37",
        "09/05/2025 21:40:15",
        "11/05/2025 10:07:34",
        "18/04/2025 13:57:33",
        "18/04/2025 21:13:22",
        "08/05/2025 09:50:33",
        "25/05/2025 00:42:28",
        "07/02/2025 11:17:43",
        "11/02/2025 02:29:50",
        "14/02/2025 20:10:33",
        "05/06/2025 19:59:18",
        "08/03/2025 17:37:49",
        "19/03/2025 20:29:50",
        "06/05/2025 13:16:29",
        "31/05/2025 17:56:33",
        "07/03/2025 22:47:32",
        "14/03/2025 08:33:51",
        "05/04/2025 22:19:24",
        "27/05/2025 12:12:44",
        "20/04/2025 11:12:01",
        "21/04/2025 15:00:23",
        "02/05/2025 01:31:57",
        "07/05/2025 09:24:53",
        "08/02/2025 15:14:13",
        "12/02/2025 15:45:01",
        "13/03/2025 17:26:08",
        "11/05/2025 19:46:44",
        "20/03/2025 12:34:09",
        "05/04/2025 11:56:42",
        "11/04/2025 11:16:10",
        "19/05/2025 21:06:48",
        "26/05/2025 12:04:07",
        "11/04/2025 15:19:22",
        "20/04/2025 01:37:47",
        "30/04/2025 16:03:41",
        "27/05/2025 15:27:43",
        "08/02/2025 14:13:18",
        "11/02/2025 01:45:03",
        "14/02/2025 21:10:46",
        "06/06/2025 16:45:46",
        "09/04/2025 21:44:03",
        "11/04/2025 15:03:21",
        "09/05/2025 17:11:52",
        "11/05/2025 07:12:57",
        "08/02/2025 16:42:08",
        "13/02/2025 10:27:27",
        "13/02/2025 14:52:05",
        "29/05/2025 14:36:03",
        "17/02/2025 17:27:15",
        "18/02/2025 18:27:38",
        "06/04/2025 14:18:49",
        "28/05/2025 13:26:10",
        "22/05/2025 17:32:03",
        "24/05/2025 15:52:24",
        "25/05/2025 06:04:17",
        "28/05/2025 20:47:27",
        "24/05/2025 03:03:54",
        "25/05/2025 05:44:54",
        "26/05/2025 15:02:39",
        "27/05/2025 20:40:23",
        "01/06/2025 01:13:45",
        "06/04/2025 13:17:53",
        "06/04/2025 15:04:49",
        "09/05/2025 22:02:19",
        "10/05/2025 17:23:17"
    ],
    "consent": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
    ],
    "no_ai": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
    ],
    "familiarity": [
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "1 (you have never read about this topic before)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "1 (you have never read about this topic before)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "1 (you have never read about this topic before)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "1 (you have never read about this topic before)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "4 (you have co-authored at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "5 (you have co-authored multiple papers on this topic or have published at least one first-author paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "3 (you have read multiple papers on this topic but have not published any paper on it)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "2 (you have read at least one paper on this topic)",
        "4 (you have co-authored at least one paper on this topic)"
    ],
    "experience": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
    ],
    "novelty_score": [
        3,
        8,
        5,
        5,
        4,
        6,
        7,
        5,
        1,
        4,
        3,
        2,
        6,
        7,
        2,
        4,
        2,
        5,
        3,
        3,
        8,
        3,
        5,
        2,
        8,
        7,
        6,
        3,
        3,
        6,
        7,
        6,
        6,
        5,
        4,
        6,
        7,
        6,
        6,
        6,
        6,
        6,
        6,
        5,
        4,
        5,
        3,
        5,
        3,
        8,
        4,
        1,
        4,
        3,
        2,
        4,
        5,
        5,
        5,
        3,
        4,
        6,
        5,
        6,
        3,
        7,
        6,
        3,
        5,
        6,
        5,
        6,
        5,
        3,
        5,
        5,
        8,
        7,
        5,
        4,
        3,
        5,
        3,
        5,
        6,
        3,
        2,
        1,
        6,
        5,
        6,
        5,
        6,
        6,
        8,
        5,
        6,
        7,
        3,
        7,
        5,
        6,
        3,
        6,
        6,
        4,
        6,
        5,
        6,
        7,
        6,
        5,
        6,
        5,
        6,
        4,
        3,
        3,
        6,
        5,
        3,
        7,
        6,
        4,
        6,
        8,
        3,
        6,
        3,
        6,
        3,
        3,
        4,
        3,
        5,
        4,
        6,
        5,
        5,
        6,
        3,
        3,
        6,
        8,
        4,
        3,
        4,
        3,
        1,
        1,
        5,
        1,
        6,
        5,
        6,
        5,
        5,
        7,
        7,
        7,
        7,
        6,
        5,
        6,
        3,
        6,
        2,
        2,
        5,
        5,
        7,
        8,
        4,
        5,
        5,
        2,
        5,
        3,
        5,
        3,
        6
    ],
    "novelty_rationale": [
        "Multiple papers for jailbreak protection investigate the usage of prompt optimization, expansion or manipulation. This paper seems more like a baseline approach as it performs context augmentation based on anchor words. Examples of more robust and similarly idealized techniques are: https://arxiv.org/pdf/2410.08660 retrieves examples of pre-collected jailbreaks that were decomposed to teach LLMs to separate malicious content. Those examples are concatenated with the potentially harmful user input to mitigate jailbreak with one-shot prompting strategy. https://arxiv.org/abs/2401.17263 applies prompt optimization strategies to add a suffix that reduces the chances of successful jailbreaks. ",
        "The approach is fairly novel in the intersection of safety and prompting. For instance, though there exist papers on prompt injection, there does not exist papers that focus on injecting \"misdirection\" in prompts (from a cursory look at related works).",
        "The idea of adding irrelevant text to the prompt to distract models from unsafe content is quite straight-forward. While I am not aware of existing work that directly employs this mechanism for improving safety, this is likely due to the big utility tradeoff that comes with the approach. There are also related work from other domains like CV which guides models to separate attack/ non-attack regions https://arxiv.org/abs/2411.15673.",
        "1. There are numerous works on injecting safety prompts (for eg. https://arxiv.org/pdf/2310.06387), that are more focused. 2. The idea itself is very counter-intuitive because adding irrelevant context directly affects the accuracy or response of the LLM on downstream tasks.",
        "I haven't seen any papers with the exact same prompting technique, so it's arguably somewhat novel. Nevertheless, there're similar ideas in existing literature, e.g. this paper called Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes (https://arxiv.org/pdf/2404.17218).",
        "I haven't heard of a work that uses analogies from unrelated domains to reframe sensitive concepts and reduce social biases in LLM's. It is an interesting approach.",
        "There have been numerous publications that focus on reframing, often for harm and bias mitigation, decreasing toxicity, and mental health applications. The novelty in the approach presented here is analogical reframing, where the authors hope to decrease expressions of stereotypes by having the model consider a reasoning \"proxy\" before answering the question. ",
        "This work is novel in taking a concept from outside the field\u2014human cognitive processes\u2014and drawing on this to develop a new computational intervention. This provides an innovative approach over basic prompting methods. But the concept of the project is still in a simple prompt, which lacks more extensive novelty.",
        "The proposed neuro-symbolic method does not make any sense to me. It is fundamentally a prompting method that inject the with API definition/descriptions. I do not see any relation to the concept of neuro-symbolic concept that typically incorporates formal methods.  The idea of leveraging API documentation/definition/structure is actually a common practice in existing literature [1,2]. I feel it a weird story-telling of forming prompting as neuro-symbolic, and the method is not new to me.  [1] Wu, Yixi, et al. \"A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models.\" arXiv preprint arXiv:2409.15228 (2024). [2] Zan, Daoguang, et al. \"Private-library-oriented code generation with large language models.\" arXiv preprint arXiv:2307.15370 (2023).",
        "Neurosymbolic approaches have existed for other text generation tasks, such as story generation (https://repository.gatech.edu/entities/publication/cdc9c234-f370-4d46-9ba5-a651dba37935). The paper additionally only uses prompting for extraction of symbolic representations, and does not propose any new methodology with respect to utilizing symbolic representations for code synthesis. Code is often used as the symbolic component for neural-symbolic generation framework, so it is cool to see an exploration into how code can be generated through neural-symbolic approaches.",
        "Don\u2019t think the proposed idea is very creative. There are already methods, though slightly different, that involve iterative refinement through interactive prompting and debugging/execution feedback. The method may be \u201cnovel\u201d in a sense that it completely automates the constraint-violation-checking process with LLMs themselves instead of relying on external execution feedback or symbolic tools. However, it is unclear whether this approach necessarily generalizes better (the paper did not compare its method with stronger baselines that involve incorporating feedback to demonstrate that its contributions are meaningful).",
        "I feel the key contribution of this paper is somewhat similar to Neural Program Generation Modulo Static Analysis, NeurIPS 21 and Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning in NeurIPS 24 workshop.",
        "The research questions in the paper are reasonably novel (to the best of my knowledge) and worth studying. I know there are people who work on multilingual QA but I am not aware of research papers that carefully study the performance disparities by controlling the content of question and varying the input language. I imagine more interesting results could be obtained if a set of carefully curated questions could be obtained to better illustrate the disparity of model performance across languages and cultures. ",
        "I thought that this idea was pretty novel and actually similar to an idea I personally had for my own research! ",
        "Similar paper: https://aclanthology.org/2024.acl-long.671.pdf",
        "Differences in LLM responses across languages has been explored in many works and the languages chosen have reasonable representation within the field. The different models used/compared may offer a little novelty. But overall there is a lack of novelty here.",
        "The main contribution claimed by the authors is that the proposed method is the first to \"interleave cultural retrieval and reasoning in stepwise fashion.\" They acknowledge that cultural retrieval + reasoning has already been explored; however, the stepwise part has been explored in the literature too (e.g. Hiyati et al.,24; https://arxiv.org/abs/2311.09799). Moreover,  I do not think that introducing the \"stepwise\" element is novel enough, particularly since the paper does not clearly describe how the steps are constructed and how much contribution the stepwise setup makes to the performance.",
        "The idea presented trivially mixes RAG with CoT for low-resource languages for proverb translation task. This idea has been tried for different domains (https://arxiv.org/abs/2504.13534) with different takes on the implementation details.",
        "The paper basically proposes IR-COT(https://arxiv.org/abs/2212.10509), which is an ACL 2023 paper, but for a different use case (proverb translation v.s. multi-hop QA). While it is nice to see its application to a new domain, I do not see novel implementations / modifications on top of the original method.",
        "While there are no similar works that attempt to combine CoT and RAG to culturally ground language models, both ideas are very standard methods of augmenting LMs to improve their performance. The paper merely provides proof that using pre-existing methods improves LM on a particular task, but does not propose anything new. ",
        "The idea of generating \"property-based testing\" cases seem to be not yet covered in related works. I believe this is a rather novel idea.",
        "This paper provides a comparison between unit testing, property based testing, and self-verification of the current LLMs. However, none of these techniques is novel. For the PBT, there are established works that studies LLM with property testing such as[1]. While conducting the comparison is a reasonable contribution, the novelty is limited. [1] Can Large Language Models Write Good Property-Based Tests?",
        "PBT is a well known method to assess code, so it seems natural to assess LLMs on that front. In fact, LLMs are shown to be able to write PBTs (Vikram, 2023). Hence, the work is not really novel.",
        "While the idea is good, I struggle to find difference between the proposed idea and the following paper (https://arxiv.org/abs/2307.04346) which investigates LLMs ability to generate properties for PBT. This idea adds comparison between Unit testing, self-verification and synthesized PBT but that is a single experiment and not a novel research idea in itself.",
        "The motivation of the paper is pretty reasonable and new to me. It focuses on the verification capability of LLMs, from the perspective of evaluation. Code generated by LLMs is not sufficiently correct even if they passed the unit tests, due to the comprehensiveness and format of unit tests. This motivation is quite natural. To solve this problem, the paper propose to evaluate the testing process through first identifying the properties that a successful would satisfy, instead of just using example-based unit tests.  The research problem is interesting and new with well-rounded motivation.",
        "The idea sounds interesting and promising, but the execution of the paper does not support the empirical effectiveness of the proposed idea. However, intuitively, it could be beneficial to check multiple sources in different modalities to enhance factuality in language output.",
        "The presentation of the idea makes it hard to understand what exactly is proposed, but from what I understand, it seems like a novel idea of attempting to use information from different modalities to improve factuality of LLMs. This involves prompting the LLM to take a look at each modality and then aggreagting at the end.",
        "The idea of grounding is pretty popular with multimodal models. However, I don't think the paper introduce anything novel or special in terms of adding in multimodal input besides just simply adding more modality into the prompts. It didnt give further analysis or reasoning behind why the prompt should be set up this way. That is in each modalities that is special and necessary. ",
        "Hallucination of multi-modal models is a new topic. See \"Hallucination of Multimodal Large Language Models: A Survey\". That said, a systematic study of how to prompt models could be valuable for the community.",
        "Long-form generation is a challenging topic, and this paper specifically targets contextual relevance and conciseness, which is a reasonably novel focus.  The proposed ACP method is also reasonably novel in its attempt to include dynamic context tracking\u2014similar to human habits\u2014which addresses the issues commonly found in fixed-window or sliding-window approaches.",
        "The idea is resonably novel. It introduces a method to prune out irrelevant information during the generation process, augmented by a retrieval component.",
        "The method seems novel",
        "The proposed ACP idea is intuitively sound - pruning \"irrelevant\" parts of the long form context. However, the idea of context pruning has been previously studied in this space (https://arxiv.org/abs/2407.14057), and the paper seems to apply a simplified version of it to the GovtReport task.",
        "Debate-style prompting is not a very novel idea. For example,  there's a preprint called Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction (https://arxiv.org/html/2502.15725v1). ",
        "The paper has very similar ideas to related works that I found online. For example, from a quick Google search, Debate-Feedback tries to apply a debate format to resolving unclear   initial opinions.",
        "This paper presents an intervention to improve the factuality of LLMs modeled after a court proceeding, where models play a judge, debater arguing for or against the factuality of an isolated argument, and a jury that makes the final decision. Using LLMs to intervene on and critique the output of other LLMs is a very popular technique (Constitutional AI), but this is a novel spin on previously explored problems.",
        "This work brings a novel approach to existing techniques by building on several existing strategies\u2014e.g., LLM-as-judge, debates between LLMs\u2014and takes it a step further by (a) combining these techniques into a single pipeline, and (b) incorporating the \u201cthird-party\u201d jury. This takes the work beyond the realm of traditional prompting techniques to offer novel insights.",
        "While there has been substantial work on bias mitigation, the idea of dynamic bias mitigation in LLMs through historical contextualization, temporal analysis, and self reflection is novel. There are some papers that look at some flavour of reflection/ self-reflection [1], however, these are more for RL set ups[1] or impact of chain of thought for bias mitigation [2].   [1] Cheng, Ruoxi, Haoxuan Ma, and Shuirong Cao. \"Deceiving to Enlighten: Coaxing LLMs to Self-Reflection for Enhanced Bias Detection and Mitigation.\" arXiv preprint arXiv:2404.10160 (2024). [2] Prahallad, Lavanya, and Radhika Mamidi. \"Significance of Chain of Thought in Gender Bias Mitigation for English-Dravidian Machine Translation.\" arXiv preprint arXiv:2405.19701 (2024).",
        "This paper proposes TBDS to measures and corrects the various type of biases included in the LLM's generation. It is a systematic and novel improvement on top of an existing research.",
        "The idea is reasonable novel, where they try to see how the biases/stereotypes are seen temporally by the llm and how they can then use this information for refinement.",
        "Much of the findings of the study showing a degree of gender and racial stereotypes by LLMs is well established. Prompting models to consider different time periods and a projected future is an interesting approach, but other prompting based approaches have also been proposed.",
        "The idea of increasing diversity is nice. Additionally using topic to provide more context could be useful.",
        "The proposed pipeline to translate long context for low-resource languages is novel. Even though it's a more hand-crafted pipeline, the performance is quite significant compared to baselines. I believe it can actually be used to translate for low-resource languages.",
        "The iterative prompting nature is somewhat novel. However, it seems like it is just providing more context and ICL demos to the prompt, which is more like a standard baseline in the current era.",
        "The idea presented in the paper is intuitive. However, parts of the methodology seem to align with retrieval augmented generation - text in the target language is generated and prepended to the prompt, however, similar (and higher quality) text could be retrieved for these languages from the web to accomplish the same purpose. It is unclear why the paper prioritized generation over retrieval, and this is not addressed or explored in the introduction or results sections.",
        "The idea of prompting the model to act as different experts to reduce hallucniation is somewhat novel, but also not exciting enough.",
        "Human fact-checking appears to be a relatively well-known and straightforward method, and does not seem to be very exciting even though \"simulation\" is being performed. Furthermore, both \"systematic evaluation\" and \"internal approach\" are there to just elucidate the method and introduces very little novelty.",
        "Prompting methods to improve factuality have already been studied, as in https://arxiv.org/pdf/2206.04624, while this paper takes a different perspective through decomposition and multi-expert corroboration. Decomposition has long been used by humans for verification tasks, so the motivation is intuitive. Multi-expert corroboration is related to multi-agent debate and role play, a field which I believe is also well-studied. Overall, the method proposed offers something new, even though the effect does not seem very strong based on the results.",
        "The paper proposes fact verification by asking questions from multiple perspectives, which is a well known approach to boost the performance of agentic systems (https://github.com/stanford-oval/storm, https://github.com/zou-group/virtual-lab?tab=readme-ov-file). It has also been previously explored in factuality literature (https://arxiv.org/abs/2310.00305, https://arxiv.org/html/2406.12644v1).",
        "There is a highly similar paper in terms of the debiasing prompt design (cited below). This similar paper is published quite recently in EMNLP 2024 November, so it may make sense to consider them as concurrent works given the lengthy cycle of your research project.   I believe the recent publication of this paper actually indicates that the research idea proposed here is relatively novel (during idea collection phase of the research project) and would be appreciated by researchers in the ACL community.   Shaz Furniturewala, Surgan Jandial, Abhinav Java, Pragyan Banerjee, Simra Shahid, Sumit Bhatia, and Kokil Jaidka. 2024. \u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 213\u2013227, Miami, Florida, USA. Association for Computational Linguistics.",
        "The paper offers a novel angle by focusing on multilingual bias and proposing prompting methods to mitigate it. I believe using CEB-Continuation also improved the novelty. However, the proposed method relies purely on prompting and requires LLMs to perform internal evaluation on their own generated data, which still doesn't fully mitigate biases that models have implicitly learned. Also, it's similar to a related paper (https://arxiv.org/abs/2407.05740) but doesn't cite it.",
        "The idea in this paper is about simply prompt the model to be more \"fair\". The paper suggests looking at multiple languages and different cultural contexts, but the method details only mention an African-American son.",
        "I believe the core idea of this paper --- using a self-reflection process to detect and correct biases -- has been implemented in other works. For example, work from Liu et al. \"Self-Reflection Makes Large Language Models Safer, Less Biased, and Ideologically Neutral\" implement a self-reflection technique to mitigate biases in language models. Widening the scope, I think there has been a lot of work on self-reflection in language models. For example, a related work in this vein is Guan et al.'s \"Deliberative Alignment: Reasoning Enables Safer Language Models,\" which present a post-training technique to reduce harmfulness by using reasoning traces. ",
        "There's already a lot of work looking at the idea of breaking down complex concepts and questions into simpler ones and re-composing in prompting (e.g. this: https://aclanthology.org/2022.emnlp-main.81/). This work technically looks at the \"abstract concept\" translation task, but beyond the motivation and evaluation being suspect, the algorithm doesn't seem to introduce anything quite new.  Also technically a much better-seeming recent work doing something similar (just came out though?) https://arxiv.org/abs/2503.04554",
        "The proposed performance even underperforms compared to few-shot prompting in many settings, which doesn't provide enough evidence on why the proposed method is better, especially since it consists of multiple steps that introduce larger latency to the generation.",
        "The prompting method provided in the paper is way too manual comparing to other automated prompting tools out there (e.g., DSPy). And the prompting is set up in a way that it does not really generalize to other domains, different from the ones in the paper specifically.",
        "The idea of decomposing abstract concepts has been widely adapted in other literature, for example, claim decomposition, as well. The use of this method on translation brings some contribution.",
        "The idea's TLDR is given the user intent, codebase, and context, models are prompted to output sub-task it wants to execute, then retrieve the context from codebase for the subtask. Then, the model determine whether the retrieved context is relevant or not, and if yes, add the context to the list of all relevant retrieved context, and repeat the above steps until it retrieved all the relevant context for this subtask. Once all subtask is resolved, the model then generate the code for the problem. The idea to break down one problem into different subtask is not that novel, similarly with either retrieval / iterative processing. For example, in https://arxiv.org/pdf/2404.05427, the proposed method is through an iterative process of retrieval -> verify, but doesn't include breaking down the problem step-by-step. However, it does use AST in retrieval, which is somewhat novel. ",
        "I appreciate the idea of Chain-of-Context that I dont think I see this before in code generation. I looked up for related works and I think the closest one is Jiang et al., ACM 2024 where it has planning phase to generate intent then append steps corresponding to the intent, then generate code. This work simplify those planning steps and instead recursively append \"context\" to solve issue.",
        "The idea of combining iterative retrieval and self-planning for code generation has been explored in prior work (e.g., https://arxiv.org/pdf/2303.12570, https://arxiv.org/pdf/2403.16792). While the specific design in this paper appears new, the contribution is not clearly stated.",
        "Although I have not read very extensive in the area of software engineering agents, there are quite a few relevant works that use very similar ideas of retrieving and verifying code. For example, both AutoCodeRover: Autonomous Program Improvement and SWE-Agent are well-known papers that allows the model to analyze the code and retrieving based on that. There may be some minor differences in terms of the specific prompts and procedures, but the high-level ideas are similar. ",
        "While existing approaches for using retrieval to augment code generation with LLMs conduct one-step retrieval before code-generation, the paper proposes to conduct planning and retrieval recursively  One missing baseline seems to be \"CODERAG-BENCH: Can Retrieval Augment Code Generation?\" https://arxiv.org/pdf/2406.14497,  but this paper was officially published in 2025. ",
        "I have not seen prior work that's exactly the same. The high-level idea of generating hypothetical scenarios or internal monologue before the final response has been explored before: https://arxiv.org/abs/2311.07445, https://arxiv.org/abs/2405.06373",
        "The paper provide a new framework to help LLM defends against adversarial scenarios. It forces llm to think about possible harmful scenarios before answering the question. The set up is interesting and novel that it forces llm to create/brainstorm a search space for harmfulness and can be more aware of the harmful intention. Instead of answering the question with harmful content, the model will learn to figure out responsible output for the harmful input.",
        "The proposed method is similar to some existing papers where researchers ask the model to analyze whether and why the user prompts violate policy (e.g., https://arxiv.org/pdf/2402.15727).",
        "My score depends a bit on what \"prior to December 2024\" means: the most obvious related work is the OAI Deliberative Alignment paper, which was released December 2024. If that is included, then the novelty of this paper is ~2; the essence of both papers is that reasoning about safety before answering can improve safe behavior.  If the Deliberative Alignment paper is not included, then I think the novelty of this work is ~7. The idea that chain-of-thought helps with safe completions to toxic prompts, and in robustness to jailbreaks, was shown in the OAI o1 release notes from September 2024 (https://openai.com/index/learning-to-reason-with-llms/), but let's give this work the benefit of the doubt and argue that the o1 release notes are not a \"real paper.\" Then the results on RealToxicPrompts and jailbreaking is moderately novel, although the TruthfulQA results feel are not novel: they simply show that CoT helps with close-book QA, including on \"hard\" questions. ",
        "This paper proposes an interesting idea on using confidence to conduct an adaptive selection on the prompt strategies to use for different test questions. Although adaptive application has been widely studied in the machine learning community, the reviewer still acknowledges the novelty. The implementation of the iterative application also adds to the completeness and novelty of the idea.",
        "I\u2019m not a domain expert, but I don\u2019t think the idea of doing prompting that is confidence bound seems that novel. Particularly, step 3 (adaptive prompting) sounds like a strategy that one UW paper (I think by Melanie Schur, about LM\u2019s sensitivity to prompts) explores this idea. The baselines that are compared against also don\u2019t feel that concurrent (e.g. Direct, Chain-of-Thought). More recent, non-tool/external based confidence scoring of prompts exists.",
        "I feel like the ACGP prompting strategy is a method is one I haven't heard about, although I'm not too familiar with the prompting literature. Using the model's verbalized confidence seems like an interesting concept. I couldn't find anything with a quick google search.",
        "I haven't read many papers that combine factuality and prompting together. However, because this paper's baselines are just the direct prompting, cot and self-consistency method and the manuscript doesn't have a related works section, I assume those are the existing methods. Based on this assumption, I think this method sets itself apart from existing methods that it iteratively refines response segments that are in low confidence.",
        "The idea of getting axioms from different programming languages and using them to guide code generation that adheres to human principle is an interesting and useful idea.",
        "Axioms in DSLs are well expected of human-authored codebases, but the way this work introduce them is fresh and novel. Producing statistically significant results by using this observation in a new framework makes it even more novel.",
        "The novelty for this paper is basically limited. The core idea is to extract coding rules from existing codebases, and summarize the best practices into several axioms which could be utilized in the following code generation and solving tasks. However, it seems the main contribution is limited to prompts design and engineering, no matter in axioms extraction or in code generations.",
        "The method is not novel since it's basically summarize good principles from old repositories and apply them when code generation. ",
        "This paper combines two previous prompting strategies--Tree-of-Thought and Chain-of-Quote. The methodology (generate subquestion, generate & score candidate quotes until one is above a score threshold, repeat) is extremely similar to the components in Tree-of-Thought (decompose question into thought steps, generate potential thoughts & score states, prune low-scoring states). The prior Chain-of-Quote work has also shown that integration of quoting in the chain of thought has been able to improve over vanilla CoT. The methodological innovation lies replacing the \"thought\" unit in Tree-of-Thought with quote generation, which has limited novelty. The paper's ToQ method shows large improvements over CoQ and CoT on MuSiQue and 2WikiMultiHopQA, but negligible improvement on StrategyQA and MedQA. Notably, a comparison with ToT performance is missing, and a comparison would be required to determine whether the ToQ method truly improves over the prior work it builds upon. ",
        "Prior work [1] proposes to prompt models to generate quotes in context to improve faithfulness to documents from wikipedia, etc. (as acknowledged in the paper). This paper replaces the prompting aspect with a \"quoting agent\" that is responsible for sourcing quotes that answer subquestions, which are then scored on their faithfulness before being used.  While I was not able to find any other papers that proposed the use of an agent for generating quotes/citations, I believe that the general idea of \"generate quotes, rerank, and then use them\" iterates on too specific a component of the generation process in order to present significant novelty. I therefore assign it a score of 5.  [1] \"Making Long-Context Language Models Better Multi-Hop Reasoners\", Li et. al., ACL 2024",
        "I'm not too familiar with the related work here, but the closest work seems to be the CoQ paper which is properly cited & benchmarked against. The nice use of QUIP score here makes this feel pretty different to me.",
        "Tree-of-Quote (ToQ) prompting introduces a modular reasoning mechanism that explicitly having quote attribution in each reasoning step. This method is a n extension to CoT and CoQ prompting by providing useful information. This method will allow quote-grounded answer/sub-step validation. This method is particularly suitable for tasks that need clear ground truth.",
        "The proposed method combines chain-of-quote prompting with tree-of-thought prompting by using a scorer in the loop to score the quality of retrieved quotes. Although the idea of using a scorer/evaluator is already explored by tree-of-thought prompting, the combination is still somewhat novel. The idea of scoring quotes is also well-motivated and tied with the literature (e.g. Weller et al., 24),",
        "here's a similar paper: https://arxiv.org/abs/2406.05494 a few other papers also explore different ways of presenting the task that may make it easier or harder for LLMs: https://arxiv.org/abs/2404.04298 (which looks at generation vs. discrimination), https://arxiv.org/pdf/2304.09102 (which also uses a more declarative prompt like the proposition negation prompt to help LLMs improve reasoning). In general this work seems incremental.",
        " The paper explores the idea about whether langauge model can do negation. However, I think the idea of negation is pretty explored already even before LLMs. The paper mainly proposes another framework to think about evaluation but didnt propose any novel reasonings behind negations or how to improve the negation capability of language models. The framework also didn't really give too much insights since the paper also claimed the performance is pretty saturated.",
        "The author points out an aspect of faithfulness in asking a language model not to do something. ",
        "I have read many papers about LM's ability to process inputs with negation. I haven't seen prior work systematically studying negation on the most recent LLMs. The prompting strategies for different types of negation are not novel though.",
        "- For handling negation in particular, the notion of replacing the prompt into ones that have positive tones for handling negation is somewhat interesting. However, the approach of rephrasing prompts into more helpful versions (\u201dReplace\u201d) and breaking down a task into simpler subtasks (\u201dProposition\u201d) already exist. I see this paper as a implementation of the idea for a more specific negation task.  - More importantly, based on the description of the method, I can imagine many (edge) cases that the method cannot generalize to (Refer to the \u201cExcitement\u201d section for more details).",
        "Role-playing in LLMs has been studied (e.g. see citation below). I think the novelty of this paper is about using role-play to enhance social/cultural diversity or appropriateness.   Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. 2024. RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 14743\u201314777, Bangkok, Thailand. Association for Computational Linguistics.",
        "The techniques used in this paper were not very novel (more specific prompts, LLM as a judge for evaluation, role-playing, more in depth personas). The idea is pretty generic (similar to many existing papers such as https://aclanthology.org/2023.acl-long.468.pdf, https://arxiv.org/abs/2402.10946)",
        "There is already a term called \"sociodemographic prompting\", it's really weird to call it sociolinguistic prompting. The work is not novel and the experiment is not good enough. Many similar works already exist:  https://arxiv.org/abs/2309.07034 https://arxiv.org/abs/2311.09730",
        "There are many works exploring the use of LLMs for social simulation before Dec 2024. (e.g.: Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?; GenSim: A General Social Simulation Platform with Large Language Model based Agents)",
        "This approach is very reminiscent of RAG for the task of mathematical reasoning and problem-solving, the novelty being that instead of storing direct answers to \"whole\" questions (non-decomposed) in memory, this paper stores decomposed subtask answers to simulate how humans leverage their memory of past problems to solve new ones. It's a clever alteration that is worth exploring, but I wouldn't say it presents \"major differences\" to existing methods.",
        "This method can be formatted into three major steps: (1) modularize one problem into different sub-problems; (2) solve each problem and compute the embedding of the sub-problem, and (3) add the embedding into an index which consists of sub-problem and their solutions. Using retrieval for math reasoning is not novel enough (https://arxiv.org/abs/2407.12883), and so is breaking down task into subtasks. This paper introduce propose to retrieve the subtask and their solution instead. ",
        "There are quite a few related works on memory augmented LLMs for reasoning heavy tasks. For example, \"Can Language Models Solve Competitive Programming?\" (Shi et al., COLM 2024) tackles coding problems through retrieval. There are other similar works that also first breaks down each retrieval document (problems in this case) into smaller units and using that as index (Boudin et al., ACL 2020; Sarthi et al., ICLR 2024). However, I don't recall any work that apply this technique specifically for mathematical reasoning. Thus, it seems reasonably novel by combining some ideas that have been previously explored. ",
        "The idea of combination between 1) decomposing math problems into subtasks/solutions and 2) memory retrieval is new. However, the idea of decomposing problems and memory retrieval has many prior works. For example, prompts with decomposed thoughts (Khot et al, ICRL 2023), finetuning with decomposition of thoughts (Li et al, arxiv 2024). For memory retrieval, I think we see a lot as a prompt technique.",
        "There are plenty of papers doing (1) retrieval-augmented prompting, (2) problem decomposition, and (3) memory embeddings, which are the main components of the proposed methods. I have not seen previous work working on the proposed idea though.",
        "Although this idea is simple, it does not appear to be systematically studied in the literature.  There are certainly some related ideas: https://arxiv.org/pdf/2401.18018, https://arxiv.org/html/2407.04295v2, https://arxiv.org/pdf/2402.11755, https://arxiv.org/pdf/2402.14857.  In my opinion the main contribution of this work is the \"Principles Recitation Defense,\" which I have not seen before but is a cool idea and appears to be the best method in the analysis.  It's fascinating that asking the LLM to state it's own values and incorporating that into the prompt works better than most other defenses.",
        "The method proposed is brand new",
        "This paper has some merits, the evaluation seems reasonable. The proposed method seems to be working compared with the baselines. ",
        "This work propose defense against many-shot jailbreaking attack using a prompting method. Different from most existing methods, the proposed method does not require modification to the model. ",
        "Many-Shot Jailbreaking is fairly new, and as a result, the defense mechanisms against it would also be fairly novel. The authors additionally proposed new defense mechanisms, although they are prompt-based.",
        "It is too similar to this work: https://arxiv.org/abs/2311.04915. I think generating such prompt templates is too simple to justify a paper; most of the heavy lifting in this paper is by execution (evals, datasets, models) rather than a novel idea.",
        "Paper is relatively novel on improving empathetic and inclusive capability of LLMs as a prompting paper. ",
        "This paper is very related to some prior works on empathy based prompting:  [1] https://arxiv.org/pdf/2311.04915 [2] https://link.springer.com/chapter/10.1007/978-981-97-7232-2_14 [3] Different method but good to compare against: https://arxiv.org/pdf/2312.08702 [4] https://arxiv.org/abs/2402.13231  Many works in the past have considered doing multistage prompts to self-reflect before outputting, and some have done it before for empathy [1] [2] and for cultural/demographic localization [4].  This work, while being a unique take on doing this chain of empathy style reasoning, does not present a particularly unique approach to the problem.",
        "The overall idea of inference time interventions for bias mitigation is not novel but the empathy-based prompting is not an approach I have often seen. In part, I am not sure if empathy-based prompting for reducing social biases is actually conflating two concepts, which might be why this approach is novel. ",
        "There has been a few works that have investigated using prompting as a way to investigate metaphor understanding. However, this work tackles using metaphors to improve reasoning in LLMs. Conceptual Metaphor Theory (CMT) has been used in other works as a prompting paradigm to improve reasoning in LMs. In particular, this paper looks very similar to https://arxiv.org/abs/2502.01901, which seems to be exactly what the AI scientist paper is tackling (which is to improve reasoning with metaphors). However, the similar paper seems to report positive results, whereas many of the results in the AI scientist paper seem to be negative results.",
        "There isn't much work on using metaphors to help with LLM mathematical reasoning. The idea itself is interesting.",
        "Using metaphor to enhance LLM mathematical reasoning is a novel and potentially impactful idea. By leveraging metaphors, an LLM can map unfamiliar abstract concepts onto more concrete, real-world domains that are more prevalent in its training data, thus making these concepts easier for the model to process, reason about, and infer with.  This approach also mirrors a fundamental aspect of human cognition, wherein metaphors are routinely employed to facilitate understanding and reasoning about abstract ideas.",
        "the method mentioned in this paper provides model with extra information of the analyzing direction of each math question. instead of providing few-shot examples of other similar math question, this methods gives extra information about the math properties that might be useful in solving the problem. the idea is somewhat novel because it uses model's capability to decompose the question that might potentially help model solve the math question",
        "The paper presents a novel idea on learning language specific prepends to the input space. While this idea seems totally novel, it is in line with the notion of using language specific adapters[1]. There are many papers that look at embedding spaces for adding language specificity. The main novelty lies in the positioning of the learnt embedding and prepending it in the input.  Kunz, Jenny, and Oskar Holmstr\u00f6m. \"The Impact of Language Adapters in Cross-Lingual Transfer for NLU.\" arXiv preprint arXiv:2402.00149 (2024).",
        "The idea of training language-specific trigger tokens to improve multilingual prompt responses is interesting, but it is quite similar to the original AutoPrompt idea. I would not say that this idea is particularly novel, since it is really just an extension of an existing idea.",
        "fairly interesting extension of prefix-tuning in the multilingual setting. One interesting and potentially novel aspect is that, the trigger token is generated by a hypernet - which is interesting since the hypernet might be able to generalize to unseen languages! which is not tested but could be a very interesting future work!",
        "This work is quite novel in a sense that few prior work trains the prepended tokens. Although there similar work using prefix text(e.g. Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model), how they obtain the prefix and the finetuning mechanism are different.",
        "The work does a good job applying the concept of trigger / control tokens to the application of multilingual LLMs. This idea has been sort of hinted at in this super old paper https://arxiv.org/pdf/1706.06275, but this paper didn't apply it to modern LLMs. Most of the paper builds upon trigger tokens, which is a somewhat established method of controlling LM generation, which is why I didn't give a high novelty score. ",
        "The proposed method seems to work better than the baseline. However, the evaluation is not enough. Most importantly, it should study whether this proposed method also filter out legitmate prompts and there is not discussion about this. ",
        "The idea of using masking as a defense mechanism is quite straightforward, and even if it worked, it wouldn't be too surprising to me. However, the most novel part is that it actually does the opposite, jailbreaking the latest models (i.e., GPT-4o, Llama-3.1). Although this means the defense methods don't actually work, it could in fact reveal new insights in the paper.",
        "There seems to be work looking at random masking of tokens, and other sorts of perturbations of input spaces to build resistance against adversarial attacks. This approach is somewhat ad-hoc and doesn't really compare with this prior work, so it's hard to say where it stands. ",
        "I think that the idea of masking out sensitive words with [MASK] is quite interesting for jailbreaks. The paper presents this as a defense-first mechanism which I disagree with, but the idea itself is, as far as I can tell, novel.",
        "The paper proposes to invert hallucinations back to its factual origin. It seems novel in that it asks the model how true information may have been been distorted into the hallucination, then tries to undo that process. However, I'm not knowledgeable in this space to make a strong judgment on novelty.",
        "The author proposes a multi-stage generation pipeline where the model identifies the possible mistakes, corrects them, and re-generates the response. The pipeline is similar to chain-of-verification (https://arxiv.org/pdf/2309.11495). ",
        "I think the idea is partially novel since the hallucination inversion is very similar to self-correction prior works. To be clear, a methodology section doesn't include enough details to clarify whether this idea is novel. ",
        "The key novelty in this work is the framing of knowledge correction through a mirage modeling approach. The key principle behind the idea is to use the model itself to flag and correct potential hallucination. There are previous works that use different approaches to check for hallucination (e.g., majority voting in Chain-of-verification). However, I'm not aware of any work that purely rely on the parametric knowledge to do the rewriting here. ",
        "The work starts from a baseline example of using the LLM to perform syntactic parsing which it cites. The \"novelty\", especially as framed in the title, is largely around whether LLMs can learn symbolic rules to improve performance. Using LLMs to learn rules is explored in the literature, but is not compared to in the work. Further literature has studied whether LLMs can then follow these rules, showing negative results and this is also not compared to in the work.  https://arxiv.org/pdf/2310.07064 https://arxiv.org/abs/2311.04235",
        "While it is interesting to know the advantage of providing rules in the prompt diminishes compared with ICL prompting, adding rules to the prompt seems to be a weak method in the current era of prompt engineering. Automated tools are everywhere. One simple baseline would be using a LLM to generate a prompt to help you to do syntactic parsing after all.",
        "There is some existing work in LLMs for dependency parsing. However, this work is pretty heavily limited by the scarcity of existing data for low-resource languages, limiting the scope of work that can be done in this area. This paper offers a comparison of many novel new LLM-based methods for parsing.",
        "The paper is novel in the idea of using explicit symbolic rules to enhance dependency parsing performance for LLMs. While this idea is novel for LLM-driven parsing, it is also the standard way of parsing for older symbolic systems. Therefore, the paper is creative in applying ideas from older, separate strands of research to modern LLM-based approaches.",
        "The idea of using alternative perspectives to study uncertainty has been explored in previous work, e.g., [1] and [2]. Also in the recent literature [3], people explored how debating (similar to the Step 2 and 3 of CSPP) leads to more truthful answers. Given above, the proposed method is novel in the sense of combining the alternative points in the measure of confidence, however, the methodological contribution is limited since the CSPP does not provide insight in prompt design beyond the scope of existing work.  [1] https://arxiv.org/abs/2104.10343 [2] https://arxiv.org/abs/2302.09664 [3] https://arxiv.org/pdf/2402.06782 ",
        "This idea is somehow novel even though basically follows the general idea that use self-verification of the LLM and have some generally alike ideas like LLM debating? (Multiagent debate/ self-debating)",
        "To the best of my knowledge this paper is clearly novel. Prompting methods on self-evaluation and consistency/overconfidence correction has potential and needs to be explored.",
        "The paper does not have significant contributions to the literature. The proposed method is very similar to prompting methods in relevant literature along the line of self criticism/self correction (e.g. Wang et al., 24, https://arxiv.org/abs/2305.13733).",
        "Asking LLM to output a probability in each of its reasoning steps is a novel approach not yet described in the literature. It is somewhat similar to just simply asking LLMs to output a final confidence metric, because it still relies on LLMs to output their confidence metrics (and not via other model-intrinstic metrics). Thus, I am giving a score of 6 and not higher.",
        "let LLM output a confidence score is not something new, and then multiply the confidence of each step as the final confidence is straightforward related work: https://arxiv.org/pdf/2306.13063",
        "The core research idea closely resembles past approaches in the literature. However, while there are highly similar recent preprints, there doesn't appear to be any pre-2024 conference papers that use this idea.",
        "The method is simply aggregating the verbalized confidence across steps in CoT reasoning. Other works have studied verbalized confidence (and improvement methods for calibration). Examples of related work that is not cited or compared to: https://arxiv.org/abs/2410.18764v1, https://aclanthology.org/2024.emnlp-main.173/, https://arxiv.org/abs/2305.14975",
        "Topic:  Studying racial and gender biases in language models has been a long-standing research problem.  Tasks:  The two task settings explored in this paper have already been proposed and studied by prior works:  * Job acceptance assessment (i.e., hiring decisions): c.f., \"Measuring Gender and Racial Biases in Large Language Models\" (ArXiv-2024, Mar.);  * Interracial romantic relationship prediction: c.f., \"On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models\" (EMNLP 2024, ArXiv-2024, Oct.).  Method: The few-shot prompting approach differs from both prior works [1,2]. However, few-shot prompting itself is already a well-established technique in LLM applications.  Findings: The main results of the paper demonstrate that the simple few-shot prompting method (despite using different names) does not effectively mitigate biases. This finding offers limited novelty. ",
        "This paper studies first name bias, which is the center of study in a very related work (https://arxiv.org/abs/2410.19803). This related work studies this problem very extensively, covering sixty-six tasks in nine domains and spanning two genders and four races. Thus, I had to give this paper a lower novelty score as it seems that this first-name bias has been studied, even though the problem they are studying is very interesting. Other related works include https://arxiv.org/html/2402.14875v2 and https://arxiv.org/abs/2406.12232)",
        "The paper mainly uses prompting techniques to explore their influence on the model\u2019s output behavior. This line of work is related to the internal mechanisms of how demonstration learning works for LLMs, such as Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? (https://arxiv.org/abs/2202.12837), which I think provides an even more insightful view of the general effectiveness of demonstrations. Also, works such as Prompting Techniques for Reducing Social Bias in LLMs (https://arxiv.org/html/2404.17218v1) are also related to the method investigated in this paper. The two scenarios investigated are somewhat novel, but they are too application-oriented, and the higher-level topics of demonstration learning and prompting for social bias have already been well investigated, which makes this work\u2019s novelty somewhat weak in general.",
        "This work uses a very simple few-shot prompt\u2014a widely-used technique\u2014and applies it to racial bias in first names. While the combination of this technique to a somewhat new application is somewhat novel, both of these areas have been widely explored (e.g., https://arxiv.org/pdf/2402.14875, https://arxiv.org/pdf/2310.09219, etc.). The work\u2019s novelty would be improved if it explored more complex techniques beyond basic few-shot prompting\u2014even simple extensions like comparing various prompts to each other, CoT prompting, etc.",
        "While I haven't seen very similar works, there have been works on similar prompting techniques involving self-critique, e.g. Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique (https://arxiv.org/pdf/2503.17363v1). There have also been works that try to enhance the robustness of LLMs through different input prompts, e.g. Enhancing Adversarial Robustness of LLMs with Analytic Hierarchy Process (https://openreview.net/pdf?id=DMUGTMWrKZ). ",
        "This paper's idea is resaonably novel in that it outlines specific steps (including adversarial imagination and verification stages) to take for a model to self-analyze and revise its reasoning process. However, it is not too different from other existing works that self evaluate and try to correct their mistakes.",
        "This paper proposes ACTI, a chain-of-thought (CoT) framework that includes self-critique, potential adversarial imagination (modifications),  robust formulation and verifications. Each of the sub-part is not that novel and has had previous work, for example, CoT with self-critique (https://arxiv.org/abs/2408.16326), and verification (https://arxiv.org/abs/2405.00204). Asking the model to generate its adversarial attack through prompts are also quite common. This work might be the first to combine these modules, but the idea still seems somewhat incremental. ",
        "ACTI is original as of December 2024 (although there are some similar preprints in 2025, i.e. https://arxiv.org/abs/2504.20769). However, the way it is presented makes it seem to be an incremental extension of self-critique, which is a well-known concept in the literature. ",
        "This paper proposes an interesting idea to resolve model bias leveraging model internal knowledge to identify common stereotypes and generate counter-examples. As far as the review knows, the pipeline is novel in the context.",
        "The idea talks about generating counter-examples and using them to refine the LLMs response. However, such works where feedback is used are already prevalent in other domains such as coding, reasoning etc.",
        "I think this idea is only somewhat novel. While I don't see papers that have their exact prompting strategy of explicitly asking the model to generate counter-stereotypes, I found a paper (https://arxiv.org/pdf/2405.10431) that prompts the model to be unbiased and to explicitly reason through the implication that indicates the bias present in the text so that the model is aware of the biases and then regenerate the text. Without comparisons to other prompting techniques like these, it's hard for me to tell how novel the method is. Although the paper does provide that with DP \"Please provide an unbiased response\".",
        "I have seen related papers on these topics: (1) language models to generate contrast set examples for debiasing, and (2) self-reflection in LLM Agents. I haven't seen papers that use LLMs to actively detect bias and then generate counter examples.",
        "The idea of using multilingual to help quantify uncertainty is novel",
        "This paper proposes a pipeline to improve MCQA accuracy in low-resourced languages via ensembling answers from high-resourced languages. It is not very novel because there are already papers that use translation to augment MCQA (https://arxiv.org/pdf/2012.05958), but this is a prompting-based version.",
        "Overall I think this is a bad idea. While the experiment results may seem reasonable, the cost of this approach makes it impossible to have practical value. It would be very expensive to translate every user prompt into multiple other languages and then prompt the models and then aggregate. ",
        "I think this paper is pretty similar to one of the related works mentioned in the intro which is https://arxiv.org/pdf/2406.15948, where the other paper tries to improve model abstention in low-resource language by incorporating feedback from related or high-resource languages, while this paper aims to utilize the model's confidence in answering the same question from other languages to improve its abstention. These are not exactly same things but I would consider the idea to be pretty in-line with only incremental novelty.",
        "There is one paper where the goals and rationals are extremely similar to: https://arxiv.org/abs/2406.15948 - this was uploaded in 2024, but reuploaded in 2025. For instance, the scoring mechanism used MKA is the same scoring mechanism that is used in the arxiv paper, although MKA justifies this as a novel contribution, which is untrue. Futhermore, both the arxiv paper and MKA are direct prompting approaches that utilize feedback from multilingual models, in a sense, even the approaches are exactly similar.",
        "The paper just basically came up a new prompting method with additional summarization and context tracking....",
        "The idea of generating code based on a plan and previous generation has been explored in prior work: https://arxiv.org/abs/2310.03302, https://arxiv.org/abs/2410.07095. This work explores prompting techniques that are used in many prior studies, with trivial variations.",
        "Adapting the context of a prompt with previous states that have a chronological dependency   is something that has been done before, for tasks such as turn-based discourse analysis (such as analyzing social media comment threads). This technique is now used in a different domain, namely code generation. So its application is novel.",
        "The proposed method in this work involves 1) prompting the LLM to decompose the coding question into a series of steps, and 2) iteratively prompting to implement each of the steps. In each subsequent iteration, the previously implemented code is given along with its description. The effectiveness of (1) was previously established by Jiang et al. (2023). Step (2) of this work is also similar to Jiang et al. (2023)\u2019s \u201cmulti-turn\u201d implementation variant taken from Nijkamp et al. (2022), in which a model is iteratively prompted to implement one step at a time, and is given the previously generated code and code descriptions at each subsequent step. Therefore, there is no novelty in the paper\u2019s proposed methodology.  Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2024. Self-Planning Code Generation with Large Language Models. ACM Trans. Softw. Eng. Methodol. 33, 7, Article 182 (September 2024), 30 pages. https://doi.org/10.1145/3672456   Nijkamp, Erik et al. \u201cCodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis.\u201d\u00a0International Conference on Learning Representations\u00a0(2022).",
        "This paper proposes an interesting idea on solving a trending recent topic: potentially conflicting evidence in the context. The authors link the problem with literature in both classic aggregation method and general retrieval augmented generation. Leveraging the model confidence to re-sample the evidence sounds interesting and natural. This kind of refiltering seems one more step beyond pure prompt-based agentic retrieval.",
        "This evaluate each uncertainty score then do weighted ensemble method' is in general similar to some existing works, even though they are not in the context of conflict evidence. Related works like Calibrating language models via augmented prompt ensembles (just as an example, there're more weighted ensemble works). ",
        "This paper is relatively novel in the field of open-domain QA. The key idea of the paper, which is to integrate various evidences into one answer, has been explored in many research in open-domain QA: - Generated Knowledge Prompting for Commonsense Reasoning (Liu et al., 2022) - Knowledge Graph Prompting for Multi-Document Question Answering (Yu et al., 2024)",
        "The weighted linear opinion pooling method is somewhat novel and nice -- I don't think I have seen previous work which prompts the model to assign credibility score of a given evidence and use it as weight to aggregate responses. Although I do not see the reason, nor ablation, of implementing the \"credibility weight\".",
        "This paper explores using what seems to be a more complex version of best-of-n for the purpose of improving calibration of language models in prompting. The algorithm itself is somewhat interesting, though not particularly unique from other prompt iteration approaches.",
        "This paper combines evolutionary algorithms to optimize prompt selection, which is a novel algorithmic invention. The motivation for applying evolutionary algorithm is also adequately justified.",
        "This paper employs tournament selection of template to find highest fitness template, which is very new and nice idea to mix and match prompt template to serve any purpose. ",
        "Given that I'm not very familiar with this subarea, I'm not very familiar with the related works. From what I know, I don't think there are many papers that compares multiple prompts in a tournament style. I know there DSPY also automatically optimize the prompt but I'm not very familiar with their approach, maybe they are similar.",
        "This paper proposes to use prompt-based LLM system to mimic knowledge unlearning. The reviewer is unsure about if this paper pivot from the very definition of unlearning. This work finds a sweet spot to use the word \u201cmimic\u201d. However, the pipeline looks more like alignment / red teaming work, but it is not much connected to such literature. From the reviewer\u2019s perspective, if this kind of interaction is eventually provided as training data and the behavior is internalized, it will be closer to unlearning. Yet similar to many methods in this area, it remains unclear how robust the \u201cunlearning\u201d is under attack.",
        "I think this idea is quite novel, in that the motivations are correct that the current literature focuses more on fine-tuning based strategies for suppressing knowledge that should not be present. I think the actual approach presented in this paper feels like stitching prompts together, which is technically not the most challenging or interesting method, but is an interesting thing to try. ",
        "Although set-ups of the agentic pipeline for the unlearning objective is somewhat novel, the approach lacks fundamental difference with the following established research on this topic: - (Thaker et al., 2024) Guardrail baselines for unlearning in llms - (Lynch et al., 2024) Eight methods to evaluate robust unlearning in llms",
        "The project builds upon ideas of LLM agents and prompt-tuning, where multiple instances of LLMs interact to fake unlearning through prompt optimization and naive filtering. From a quick search, I couldn\u2019t find a paper that did exactly this (except the actual final paper that built upon this draft: AegisLLM from April 2025), but the ideas themselves are not entirely novel.",
        "This idea of creating dictionary has been explored in various different areas. The idea is to get primitives first (words in this case) and then doing some sort of synthesis. Very common in program synthesis literature.",
        "To my knowledge this has not been explicitly studied before although there are some relevant related works: https://arxiv.org/pdf/2305.06575  I have not seen other works try to explicitly generate definitions of all other interpretations of a word before translating in order to increase lexical diversity.  The idea makes sense and is reasonable to try.  Prompting for translation is not new however, nor is dictionary and lexicon based translation.",
        "Extremely similar to this ACL paper: https://aclanthology.org/2024.emnlp-main.278/, with less evaluation contribution, worse motivation, and lower rigor. The only difference is a different suite of models and eval, however the suite may be worse in terms of reproducibility. ",
        "The idea of the proposed method align a lot with existing works in task decomposition and Chain-of-Thought style prompting. I would say this is a narrow application of specialized CoT.",
        "There is a very similar work: https://arxiv.org/pdf/2411.02454. It also uses graph concept for uncertainty quantification. Although there exist difference in terms how node and edge are defined but this reviewed work did not bring a lot of novelty.",
        "The high-level idea of this paper, if I am correct, is to identify a metric space in which prompt embeddings are similar if their responses have a similar degree of confidence as elicited by an LLM. The paper correctly cites [1] as the closest work in this space (I searched around a bit and could not find anything more similar). The claimed difference from [1] is that the paper uses a contrastive approach to learning these embeddings. While this idea itself is novel, I think the paper ends up describing an algorithm that is both (1) incomplete in its description (see below; it does not define how known questions' confidences are used during inference) (2) incorrect from my reading (it assigns a higher similarity to node pairs whose confidence is less alike?). Therefore I think that the paper itself does not have an actual idea or approach. This abstract, high level direction of doing something contrastive makes sense to me though, and thus I decided to go for 5 (somewhat novel).  [1] \"Graph-based Confidence Calibration for Large Language Models\", Li et. al., TMLR 2025",
        "The core idea of this paper is to create some kind of hybrid question semantic similarity & confidence space. New questions are projected onto this space & some kind of confidence correction read from it. This was a very weird idea and I've never heard of anything like it before.",
        "The paper's use of using contrastive question-answer pairs, specifically in creating contrastive questions, is novel.  The use of both absolute and relative confidence to create a hybrid graph structure is also novel. ",
        "I don't think there are paper did the exactly same method but the two parts of the proposed method have similar prior methods correspondence: 1. generating symbol and package graph is not that novel since Repograph (Ouyang 2024 et al.) using similar but more fine-grained graph method to better structure the repo. The Self-Debug framework enabled the iterative edition of the patch with the produced regression test is kind of novel in method even though many iterative agent framework are there checking whether the patch is correct in different ways. However, LLM for coding domain is not that easy to come up with super novel idea, so I gave 4. ",
        "The paper staples together two contributions: (1) use a symbol graph to subset the repository before running BM25; (2) do one round of self-debugging (generate test cases, run them) before outputting the patch. Self-debugging with synthetically generated tests has been studied in several works (e.g. https://arxiv.org/abs/2411.18015, https://arxiv.org/pdf/2406.01304#page=3.59). I am not familiar enough with the literature to know if the graph preprocessing step has been done before, so I'll treat it as novel, albeit somewhat incremental.",
        "The proposal of abstracting the codebase as a symbolic graph is interesting. However, as currently described, I fail to see how this is not already present in e.g., SWE-Agent [1] which shows a hierarchical layout of all code files, or OpenHands [2] which includes both a GUI and   an event stream that tracks all events (and was not cited here). Both methods also include feedback mechanisms where the agent evaluates its failure. I think abstraction at the level of functions instead of files is interesting, but then one needs to describe how code outside of functions or classes is handled, and how different functions or classes are aggregated when too numerous. In the absence of these details, I rate this idea as 5. [1] \"SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering\", Yang et. al., NeurIPS 2024 [2] \"OpenHands: An Open Platform for AI Software Developers as Generalist Agents\", Wang et. al., ICLR 2025 (was arXived June 2024)",
        "The method described in this paper involves 1) using static program analysis to build a symbol and package graph and searching for issue keywords to filter down potential files to edit, 2) generation of an initial patch by the LLM-of-interest, 3) using a different LLM to predict the effect of the patch and to generate test code snippets, and 4) using the initial patch, feedback from Step 3, and problem statement to prompt the LLM-of-interest to generate a refined patch.   Similar ideas have been explored in the literature:  (1): AutoCodeRover (2024) searches for issue keywords in the program\u2019s abstract syntax tree to help find the code to edit. MarsCode Agent (2024) uses static program analysis to build a multi-directional graph including semantic nodes, file relationships, function call relationships, and symbol indices to search through using the issue keywords.  (2-4): Prior work such as CodeR (2024) has used a separate language model to run tests and provide feedback to the patching LLM to iteratively refine the patch. MarsCode Agent (2024) uses one separate LLM to generate code to reproduce the error and another separate agent to verify each patch generated by the patching LLM and provide feedback.  These prior works use the same basic building blocks, albeit in more sophisticated workflows, as the paper under review, which makes this work mostly not novel.  Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program Improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2024). Association for Computing Machinery, New York, NY, USA, 1592\u20131604. https://doi.org/10.1145/3650212.3680384  Chen, Dong et al. \u201cCodeR: Issue Resolving with Multi-Agent and Task Graphs.\u201d\u00a0ArXiv\u00a0abs/2406.01304 (2024)  Liu et al., \u201cMarsCode Agent: AI-native Automated Bug Fixing.\u201d ArXiv abs/2409.00899 (2024)",
        "the main idea of this paper is \"having a local model to give feedback to the frontier model and prompting frontier model with this information and other related information(such as package info)\". the \"somewhat novel\" part is to introduce a local model in the loop to provide feedback. But one can argument why not using the model itself to do reflection on the potential results. ",
        "Source tracing and validation have been extensively studied in the context of large language models (LLMs) (https://arxiv.org/abs/2311.07838). This work employs prompting techniques that enable the model to verify source information before generating a response. However, this approach appears to be incremental in terms of innovation.",
        "Epistemological Source Tracing as an idea is generally discussed in the literature to reduce hallucination. However, the paper's presentation of this idea (using the model to self-identify the source) is a particularly poor and underdeveloped treatment.",
        "When taking the word \"Epistemological Source Tracing\"  in action, it appears to be a multi-step RAG approach using multiple steps for LLM to perform self-checking steps, which is not a novel idea. The proposed idea is even unclear when using such big words but didn't include any tools/specific methods involve.",
        "The paper looks very similar to Chain-of-Verification Reduces Hallucination in Large Language Models (Dhuliawala et al., 2023) to me. Specifically, both are prompting techniques that seek to improve the factuality of LLMs. The key difference in this paper is that it uses an additional step that first as the model to identify and generate related sources and verify their reliability before revising the response, whereas other previous works would use retrieval to find these sources instead."
    ],
    "excitement_score": [
        1,
        6,
        3,
        3,
        3,
        6,
        7,
        4,
        2,
        3,
        3,
        2,
        6,
        5,
        1,
        5,
        3,
        3,
        3,
        2,
        6,
        3,
        5,
        2,
        7,
        6,
        2,
        1,
        3,
        6,
        5,
        6,
        5,
        2,
        6,
        3,
        7,
        5,
        7,
        4,
        6,
        5,
        5,
        5,
        4,
        4,
        5,
        4,
        3,
        7,
        3,
        1,
        5,
        3,
        1,
        3,
        5,
        5,
        6,
        3,
        6,
        7,
        5,
        6,
        3,
        6,
        5,
        3,
        2,
        4,
        5,
        6,
        3,
        1,
        5,
        3,
        8,
        7,
        6,
        2,
        2,
        5,
        3,
        5,
        7,
        2,
        1,
        2,
        3,
        4,
        4,
        6,
        2,
        7,
        8,
        5,
        7,
        6,
        2,
        8,
        4,
        3,
        2,
        3,
        5,
        2,
        7,
        6,
        6,
        7,
        6,
        5,
        6,
        4,
        4,
        4,
        3,
        6,
        3,
        2,
        3,
        7,
        5,
        3,
        3,
        7,
        3,
        4,
        2,
        3,
        3,
        2,
        6,
        2,
        2,
        3,
        6,
        5,
        4,
        6,
        3,
        4,
        6,
        7,
        5,
        3,
        3,
        7,
        1,
        1,
        4,
        1,
        6,
        3,
        6,
        5,
        4,
        6,
        6,
        6,
        6,
        6,
        3,
        6,
        3,
        7,
        3,
        3,
        3,
        1,
        5,
        5,
        3,
        3,
        2,
        3,
        1,
        5,
        3,
        3,
        3
    ],
    "excitement_rationale": [
        "The paper proposes a very simple prompt augmentation strategy that retrieves semantically similar documents and concatenates them to the prompt. The goal is to distract the LLM from a potential jailbreak prompt. As mentioned in the other field, this seems like a naive baseline to more robust prompt-tuning methods.",
        "The paper is not as exciting because the contributions are in the realm of only prompting. Furthermore, while the motivation is based on the possibility of adversarial attacks, exact examples of these attacks are not given, so it is difficult to understand to what degree should we worry about potential prompt attacks (and what these solutions may be trying to mitigate).",
        "While the concept of semantic fog is quite interesting, the paper itself is not exciting due to obvious limitations in the method's effectiveness and practical implementation. For example, introducing semantically relevant yet topically irrelevant content will introduce tradeoff in utility; always including a retrieval process during inference, without guarantee in performance improvement, is also not practical. ",
        "1. Experiments are lacking in a major sense. 2. The motivation of the idea is unclear. Why is adding irrelevant sentences better than injecting safety instructions? 3. The whole idea is rehash of safety prompt injection with randomized noise. While the motivation is that it works in images, the problem is that in image classification literature, researchers inject gaussian noise and not random noise. Similarly, there have been works in language model literature, which introduce gaussian noise directly in the weights of the model to study the safety of Language models.",
        "This paper presents a prompting technique called Conceptual Pivot Prompting to reduce social biases in large language models. However, the single pivot prompting method only works for GPT3.5 across all pivots, meaning that this prompting technique might not generalize well. Therefore, this paper's contribution and impact are limited. ",
        "This would be very cool, but I am concerned about the scalability of this approach. Would it take too long to actually get to the reduced social bias? Also, excessive application of CPP has an unintended effect, which is not great and should be addressed.",
        "It is interesting to see if analogous reasoning proxies could decrease an LLM's expression of stereotypes. Although this paper doesn't show an increase in demographic parity across the board, on all models compared to a baseline standard prompt. The experiments and discussion touch on interesting findings that are worth exploring in the future - such as why different pivot prompts yield drastically different performances across models.  ",
        "This approach is very creative in applying unrelated analogies as a debiasing technique. But this also detracts from the excitement: where is the theoretical grounding of these unrelated analogies in relating to stereotypes and bias? This lack of a clear relation makes the intervention feel somewhat random, less thoughtful, and therefore less exciting.",
        "As I described in the novelty section, the idea of this paper actually exists in the community. It just uses a appealing story-telling presentation. This is not great, actually, and might mislead readers if they are not familiar with neuro-symbolic methods.",
        "In terms of the idea itself, I see some potential excitement for using something like this, but the proposed approach is essentially similar to using a code compiler / linter iteratively until the generated code has no more errors. LLMs are probably not as reliable at judging code as tools actually specifically designed to assess code quality.  The results are just simply not convincing enough for it to be significantly impactful, at least for this stage of the paper, unfortunately.",
        "Similar to the response for \u201cNovelty\u201d: the paper is not that exciting because similar methods already exist. The paper only adds very small variations to existing methods. Whether the method is impactful is unclear because it does not have a comprehensive comparisons with other methods that also involve refining code with feedback.",
        "This work aims to improve code generation for complex APIs by using less data and developing a more robust, generalizable method for API-aware generation. But (1) the tasks are not too complex in terms of API usage, even basic direct prompting has 100% performance and (2) it's unclear what this paper means by \"less data\" and why existing models are less data-efficient.",
        "The research ideas in the paper would be interesting to many people in the research community and also developers who create products based on multilingual QA systems. However, the analysis in the paper lacks depth and the paper does not offer a lot of insights to answer the research questions. ",
        "I thought the idea was exciting and had potential as a good way to audit the responses from LLMs. I am pretty curious about the research questions of if LLMs provide different responses when prompted in different languages and if different LLMs exhibit similar patterns when giving varied responses to the same question. ",
        "It's not exciting because 1) the premise is questionable - whether the biases are worth investigating in the proposed context, 2) the real-world or technical significance of this work is also little, 3) limited novelty.",
        "Similar to the the novelty rating, this work lacks novelty as other similar studies have been conducted before, so it makes it less exciting. The results were also consistent with prior studies in the field. It is also not clear that creating a result that is as generic as possible is what is desired or if there are contexts where culturally relevant responses are desired and encouraged. Finally, all of the languages represented are used within cultures that have major to decent representation in AI development and are generally well represented in training data. This study could have been more exciting by including languages that are less represented in NLP.",
        "Similar to my response to the previous question, I think the paper only makes incremental contribution to the literature. The last section of the paper discusses interesting findings of how popular automatic evaluation metrics overlook cultural values in reasoning, making it potentially the most exciting part of the paper. However, the paper does expand the discussion.",
        "Very marginal contribution. As stated above, it trivially mixes RAG and CoT which has been tried before in other domains and this paper also does not have great results.",
        "As mentioned before, this paper applies IR-COT to a new domain (proverb translation), which is not very novel / exciting.",
        "This work has low impact on the research community, given that it has merely shown how two well-established techniques both work well in a narrow application. It provides empirical results towards the idea that a combination of CoT and RAG helps the model \u2014\u00a0two things that are already standard practice. The only results worth mentioning are the analysis on the downfalls of BLEU and other automatic metrics on cultural grounding. However, these results have nothing to do with CG-CoT and are not well-supported. ",
        "The paper conducts experiments to prompt LLMs to generate PBT test cases for each program. While the idea is novel and presents an interesting research direction, the paper goes short of proving whether using PBT can enhance end-to-end software development tasks. It should have conducted experiments on whether using PBT as an intermediate steps would enhance end-to-end performance.",
        "The paper provides some results on the comparison between different testing methods. This can reveal some insights in building a better way to test the LLM coding and enhance LLM coding. However, the idea is also not that novel and the experiments not well designed, so in practice it did not reveal any useful insight.",
        "The comparative study positions itself more as an extension of existing LLM codegen eval, which makes it merely interesting and not exciting enough.   ",
        "The idea is essentially copied from an existing paper (see the above response) and only adds experiments for comparing PBT, unit tests and self-verification which is also incomplete.",
        "I think this work will bring some impact in testing programs with LLM generated data. Firstly, it challenges a traditional or prevalently-used evaluation paradigm, unit testing, by pointing out its weaknesses. It then present an interesting perspective that may change this paradigm, by first identifying essential properties that a successful program should satisfy.  I think the core idea is exciting enough. If the paper could add more detailed analyses, it would be further insightful for people in this community.",
        "The idea itself is quite exciting as it proposes to mitigate model hallucination by relying on information from multiple modalities in the input. However, if the input comes from the same source, they may contain the same false information and render the proposed idea ineffective. The execution in the paper does not seem to support the empirical effectiveness of the proposed idea despite the good intuition of the idea described in the introduction.",
        "The experimental results, which the authors acknowledged, showed no improvements at all. The presentation also does not highlight anything of particular interest to the community.",
        "The paper provides a new methodology while the methodologies is out performed by most baselines. Also the paper only tested on claude 3.5.",
        "The idea itself is not very novel and the experiments do not seem to be comprehensive. Also, there does not seem to have any takeaway except that the method does not work.",
        "The field of long-form text generation is itself a popular topic. But the focus of this paper specifically on relevance and conciseness \u2014 both of which are reflected via evaluation metrics (FC and CR) is an existing perspective.   Method-wise, the idea to maintain a dynamic context memory and updating context relevancy through time is somewhat exiting too, but has been studied before as in e.g. \"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text. ArXiv-2023.\"",
        "The result does not seem to be conclusive. Compared to Full context window, the results of ACP is not better. From my understanding, ACP will likely even cause more token usage because it prompts LLMs to give a score for each chunk. This undermines the efficiency argument. If experiments are conducted with LLMs without as long a context window then the results could be more promising.",
        "The idea of dynamicaly managing context seems promising and impactful",
        "The method proposed in the paper is very computationally heavy, requiring GPT-4o calls for each chunk of generated text to determine the relevance score, along with retrieval over the pruned context. This increase in computational overhead is not justified by increases in downstream performance. I also believe the scales of experimentation used in this paper are somewhat irrelevant in the context of modern LLMs, which have context windows up to a million tokens in size.",
        "This paper presents a prompting technique for reducing hallucination in large language models. However, the authors show that this prompting technique called Grounded court debate fails to improve the precision of the model on the LongFact benchmark. Additionally, this prompting technique reduces the model's recall by a little. In summary, there's no strong positive evidence of the proposed technique's effectiveness. ",
        "I do really like the idea of applying a debate period to really check the validity of questionable claims. But it is not entirely groundbreaking or novel.",
        "The idea was clever, but it did not show the improvements on the baseline that would motivate members of the community to implement this. This paper demonstrates that the technique, as it was executed in this paper, is not effective.",
        "This work addresses a pressing problem in a way that is directly applicable to real-world applications, via its final response generation. Its robustness to various contexts and self-contained nature make it even more existing as a context-independent technique that could be widely applicable, which seems important given the widespread nature of hallucinations.",
        "Having dynamic methodologies for bias detection and mitigation is a field of interest, however, the methodology presented in the paper suffers from the following drawback: (1) The paper cannot account for any bias mitigation where it cannot come up with appropriate historical contextualization. This limits the use of this method in practice. (2) The role of decay is not clear - it seems like the goal is to decay the bias in model outputs through the prompt. Based on my understanding, decay here merely means an LLM to look at a more \"equitable perspective\", which is not perfectly in line with the canonical meaning of decay. Why not say that the bias is \"mitigated completely\" and why say decayed? (3) The 25 year historical granularity and the choice of the testing datasets make the temporal aspect lackluster due to the large granularity.",
        "The paper is exciting as it proposes an effective method on addressing bias in LLM's output.   Two major datasets are covered and biases across different types of biases are measured in great details.",
        "It is unclear how this would help us in understanding the biases in LLMs. I would imagine, such a technique might not work with things that the LLM doesn't know about much.",
        "This work is somewhat incremental as methods with a similar objective have been proposed and it is unclear how this method compares. It is also concerning to study bias in LLMs with data generated largely by LLMs but this is mitigated by the author validation to an extent. However this could still have some value to the field so I would lean slightly positive for excitement. ",
        "The work could help in improving the translation for low resource, especially if you have a corpus of related data. However the system also depends on LLMs ability to generate context.",
        "The proposed method is slightly too manual, just adding different tricks such as monolingual text and few-shot examples. It's more of a prompting technique instead of a novel or truly exciting idea to me.",
        "I think the results are sound. But the overall motivation is less interesting.",
        "The field of machine translation for low-resource languages is a relevant and important research topic. However, the method proposed in the paper relies on the LLM's own understanding of these low resource languages, which can often be bad for long-tail languages. Therefore, it is unlikely to improve performance for truly under-represented languages.",
        "The idea of using multiple perspectives to reduce hallucination is somewhat interesting and  novel, but not substantially different from generating multiple responses.",
        "This is a relatively straightforward idea that does not appear to be very effective in the wild. I would not find it either influential or exciting.",
        "It\u2019s not super exciting to me, as the prompting method itself is easy and corroboration across multiple experts is intuitive in design. However, this study could be further extended to solve questions beyond factuality QA. As previous studies have investigated the role of persona in problem-solving (https://arxiv.org/pdf/2406.20094), I think it might be interesting to extend the method beyond the factuality scope. Overall, it offers some new insight but is still incremental in idea.",
        "Factuality is a relevant and important research topic, as LLMs are used in increasingly more critical applications. However, as mentioned above, the approach presented in this paper is not novel, which limits the excitement for any results presented in the paper.",
        "The exciting part of this paper is that it promotes bias mitigation by iteratively prompting a model to reflect on its own about the potentially biased content it has generated. This self-reflection approach is interesting and has been demonstrated to be effective with empirical evidence.",
        "I believe the direction of mitigating bias in multilingual contexts is important; however, the proposed methods and experimental results aren't fully convincing that they represent a meaningful step forward toward this objective. I don't see this approach being widely adopted or trusted, as it solely relies on the model itself to mitigate bias in its own generation.",
        "It's difficult to be excited about this paper because the proposed method doesn't seem like it would address the problem at hand. Also, it's not clear how the results follow from the method. ",
        "I think this project is addressing an important issue that is under explored -- there are works on bias mitigation and on multilinguality but less that I am aware of that sit at this intersection. Nonetheless, I think the key methodological insights that this work provides could have been derived from looking at prior work. I also believe there might be flaws in the conceptualization of the study particularly thinking about how biases manifest differently in different geographies / cultural contexts which impacts how relevant existing benchmarks might be if they are originally constructed in English. ",
        "The motivation overall is quite unclear, with the idea of an \"abstract concept\" lacking clear definition or prior work as justification. The evaluation, done on seemingly 100 examples (with even less per category) is worrying small and likely noisy, and the method itself is a relatively basic re-hashing of existing prompting approaches in a way highly narrow to this specific setting. ",
        "The results reveal nothing exciting to the reader, especially as the performance shows no improvement compared to the baseline. This doesn't provide sufficient incentive for people to employ the introduced methods.",
        "Like my reasons above, it is less exciting since it feels like the prompting pipeline setup in the paper only works for this specific case. The motivation for engineering the LLM prompt is also less clear given that google performs the best.",
        "The scope of the improvement in this work is generally not wide enough to be accepted to a major AI conference. However, since the proposed simple method shows improvement over CoT or few-shot prompting, it offers insights that might be interesting to a smaller audience.",
        "The idea feels incremental, and it doesn't demonstrate a performance that is way better than baseline methods, and at a cost of >= 2x computational cost. However, chain-of-context can improve localization on top of the baseline is interesting. ",
        "This work is incremental from RAG LLM in general, but instead it is now a cycle of subtask-related retrieval. I appreciate the idea as mentioned though I believe it would be limited due to it is prompt-based method, and I expect that it would improve slightly from RAG.",
        "I don't think the design in the paper provides much insight.",
        "Compared to the baselines (just RAG or ICR), the proposed method seem to have made significant improvements. Achieving 14% solve rate on SWE-Bench-Verified is pretty good. However, compared to some existing models that have already achieved 20-30+ solve rate using similar frontier models, the proposed method does not seem very impressive. I do think it's not very fair to compared to some of the frontier systems that are way more expensive though. ",
        "I think the idea is potentially impactful as it tackles a fundamental problem in retrieval for complex reasoning problems like those in SWE-Bench that interleaving retrieval and reasoning could be very effective as most existing retrieval models understand semantics but cannot conduct multi-hop reasoning, leaving them capable of retrieving the immediate semantically relevant content but incapable of retrieving semantically new content generated during reasoning. ",
        "The idea of generative adversarial attach scenarios seems somewhat novel but the results are not exciting enough and seem suspicious (e.e., inconsistent results in table 2). I don't think this work would change the field because there are stronger baselines that are not considered in this paper. ",
        "The methodology of this paper is easily replicable. I am happy with the improvement it shown on frontier models. However, I am a bit concerned with the increase of inference budget of this methodology. Would be nice to see a graph comparing flops. Also, I am wondering whether it's possible to brainstorm all the harmful scenarios before answering. That seems more like an impossible task. What is a general good number of scenarios the model should brainstorm before answering. It would be good to see a graph showing increasing the number of scenarios help reducing harmfulness.",
        "The method is similar to many existing works, so it is not surprising that it can straightforwardly reduce unsafe responses.",
        "Any sort of safety defense is interesting, but I think there were a lot of low-hanging related directions this paper failed to latch onto (it did the bare minimum instead). ASE has some interesting ties to intent understanding / theory-of-mind. But this paper doesn't really dive into those ideas. Another interesting extension would have been to trade back the test-time compute into parametric knowledge, but this also was not explored. Another thing could have been to understand how model strength affects ASE effectiveness. Finally, a last really exciting benefit is that the reasoning traces are interpretable & could be audited, but this also isn't really fleshed out. ",
        "With further improvement, the reviewer believes that this idea can be extended and eventually makes a accepted paper. However, at the current stage, there lacks motivation over the why certain prompting strategies are selected",
        "Section 2 is kind of short, I didn\u2019t get a good sense of what the method is. I think improving factuality without using external resources is an interesting question, but to me I don\u2019t think something like this would be used in practice. Given that the baselines are Direct + Chain-of-Thought, this work feels a bit out of date in some sense. I would\u2019ve expected something like this to come out in 2023. The scores are technically better (Table 1), but doesn\u2019t seem like by much.",
        "The paper tests on very few benchmarks and does not have very comprehensive experiments. The prompting method was not very clearly described and only GPT4o mini was tested, so I feel like it would be hard to know how effective the method would be on other models or benchmarks.",
        "This could be a useful method for a well-calibrated model because this method makes a pretty strong assumption that models' verbalized confidence level makes sense, which is often time not the case. The results are mixed and even for the positive part the improvement seems to be very marginal. The method only applies its scope to LLMs on answering factuality questions. Because results are mostly negative, there is a strong assumption, and the scope is quite limited, I'd do a rating of 4. Or 3 if looking at the results.",
        "The idea is somewhat novel, but the overall experiment design and insight is not interesting enough.",
        "The idea of an axiom in the context of LLM codegen is fresh. However, the framework appears to be a bit underdeveloped to be truly exciting, as its components (CoT, axiom selection) are quite standard.",
        "The idea for this paper is not that exciting itself, as I believe there are already existing other works that have done similar things, i.e., extracting some standard, commonly-used code snapshots, and then plugging them into downstream tasks. But this paper indeed points out some potentials about augmenting current LLMs by integrating existing codebases, instead of further trainining on the relevant tasks.",
        "The results are not significant and the task and metric is also somehow non-informative. Besides, the method is basically auto summarized good coding principles for code generation, which could be easily achieved and even outperform by more careful curation and fine-tune the model. ",
        "This is a good demonstration of how to intergrate quoting into Tree-of-Thought, which would be of interest for certain applications where attribution is important. However, the results are not strong enough for StrategyQA and MedQA, and a comparison against vanilla Tree-of-Thought is missing.",
        "There are several reasons why I think the improvements here are incremental and potentially not impactful. 1. This paper proposes to replace generation of text---a largely cheap process---with several model calls each of which add very incremental value (retrieval of one quote). 2. I believe the issue of factuality stems less from the issue of not generating enough citations or quotes (which would fall under instruction following) but rather hallucination of quotes. The paper does not specify this, but based on Figure 1 I interpret the process as an LM proposing a quote, and then it being scored using QUIP (which scores precision of character n-grams)---the LM could still hallucinate quotes. I am therefore not convinced that this is the aspect of the process that needs to be improved. 3. Although the paper is called \"tree of quotes\", there is no actual search process over some search space involved like in Tree of Thoughts. ",
        "This is a really nicely done paper. In principle it makes sense that one should ground out multi-hop reasoning in quotes, and the QUIP scores gives a good filter for pruning paths.",
        "The idea targets a meaningful issue of current LLMs - grounding capabilities/lack of attribution. This method introduce a new mechanism to make LLM more convincing at reasoning as well as make observer more convenient to know what information is used by LLM when reasoning.",
        "The proposed method yields strong results on multi-hop reasoning tasks (musique and 2wiki), beating CoT and CoQ by a large margin for the LLM-as-a-judge metric (SM). The method also provides benefits beyond accuracy, such as attribution. However, results on strategyQA and MedQA underperform baselines, leaving room for improvement.",
        "The results are mostly expected and do not seem to lead to any interesting follow-up work. Also, it's unclear whether the differences between the proposition expansion and other baselines are statistically significant since the authors didn't do multiple runs. Therefore I'm not convinced that the proposed method does have empirical benefit.",
        "I don't think I have too much take away and new insights from the paper about negation. There isn't a clear pattern in the results and the framework itself isn't very exciting or theoretically grounded. ",
        "Although this angle is new, the tested proprietary and open-weight models have already achieved an accuracy close to 90%. And the author doesn't provide effective solutions.",
        "Negation has been wildly studied in previous versions of LLMs. As the topic and the negation types considered in this paper has limited novelty, and that how the SOTO LLMs deal with negation is beyond the scope of this paper, I personally do not find this paper very exciting.",
        "I do not expect the results to be very impactful. I would expect that there are potentially many (edge) cases for which the method does not work.   - For the \u201cReplace\u201d method, you can\u2019t always effectively translate a negation into a sentence with positive tone AND without negation. The paper does mention that this method is not applicable for instance-level negation. However, it may not even be applicable for the type-level negation as well. For example, \u201cDo not include vegetarian food.\u201d will be translated to \u201cInclude only non-vegetarian food.\u201d, which still has negation (non) in front of the word meat. If the model is not always good at negation, then the problem still exists, not to mention if the statement has a much more complex logic.  - The \u201cProposition\u201d method also seems self-contradictory. The main goal of the method is to help the model to better handle negation. However, Proposition, which breaks the task into two steps according to the example, can still contain negation. Just use the example in Tab.2 as an example: \u201cnon\u201d in \u201cexclude non-vegetarian food\u201d still has negation. Therefore, the model may still have bad performance if it fundamentally is bad at handling negation.  - Also, the number of items that is in the target category may be countless (e.g., non-vegetarian food). It is infeasible to always generate a complete list to help the model.  - A slightly more trivial but still important issue: What if the model excludes all the items in the generated list? Do you need to try the steps again? The method does not guarantee the generated result is still a valid list.",
        "The investigation of role-playing ability of LLMs could allow us to understand whether these models could provide more personalized interaction that suits the user's cultural or social background. This could be something that helps develop a culturally adept chatbot. ",
        "While this paper identifies a relevant problem (i.e., LLMs fail to capture the complexity of social language use and require extra-linguistic situational context), I don't expect the results to change how LLMs are evaluated or used. The paper seems to me just more specific prompting. While the problem is important, it has already been identified by other researchers (https://cs.stanford.edu/~diyiy/docs/naacl21_social.pdf) ",
        "The idea is very straightforward but also very limited. There has been a lot of debates about sociodemographic prompting and I am not exicted at all for this work.",
        "The setting in this work is in a similar frame with many other works and it is very simple without too much value added. It's just varying the prompt by adding some social traits for dialogue generation.",
        "I like the idea, but there are a few things that make this work feel incomplete, which then dampens the excitement. I am a firm believer that even strong negative findings should be published, and this paper has shown that this approach only yields marginal improvements in very specific cases. However, there were not enough ablations in the experiments for me to wholly believe that this technique should not be explored further. In summary, my lack of excitement doesn't come from the lack of significant performance improvement, but from the lack of strength of the argument.",
        "The idea is interesting, and the experiments are insightful. However, if I am understand it correctly, at test time, the test instance problem is decomposed into sub-problems, and each sub-problems is embedded and the most similar sub-problems and their solutions are retrieved from the index, and then are used to solve the test instance problem. However, direct method slightly outperform the decomposed on almost every model (table 1), and from the preference evaluation side, the decomposed answers are way less preferable than the direct answer. ",
        "The results and the experimental settings in the paper are still lacking. First, the results still lag behind simply just using direct prompting to solve the problem. Second, it doesn't provide any convincing examples where the proposed methods can be helpful. ",
        "I expected that the result will be helpful though it turns out to be the opposite. Still exciting to see the results and its impact.",
        "The results of the paper does not seem to support the claim in the intro that \"integrating a self-improving memory significantly improves model accuracy\". Across different settings, the results of the Decomposed method generally underperforms the baseline in the Tables. ",
        "If the PRD consistently works well I think people would be interested in incorporating this both into products and into future jailbreaking evaluations.  It's extremely simple to incorporate into an experiment (2 steps: ask the model its values, then add those values to the context).  This is much cheaper than existing defenses, and showcased decent value.",
        "The results look promising and idea is simple",
        "The ",
        "Given that this is a short paper; The research question studied in this paper is valuable to the audience and the method proposed could benefit broader community on LLM safety. The idea is also suitable on closed-source language models, making it more applicable.",
        "It is really cool to see that the proposed PRD successfully reduces attack success rate rather significantly across various forms of the MSJ attack. However, the reduction in helpfulness scores is still a concern, and I wish the authors would be able to address this.",
        "It's just too simple of an idea to justify a paper, although the evals are well done. It is suitable for a class project or maybe a workshop because of the small size of the contribution.",
        "The pipeline of the framework is well-designed. The connected dataset is also an important contribution since there are only few existing benchmarks on empathy. ",
        "It's tough to know how big an impact this prompting technique has because the results are not super well grounded.  For instance, the authors created a dataset without any explanation of the quality control or collection process behind it, besides mentioning it was related to analysis in another paper for problems that people of certain demographics face.  No explanation was provided if this dataset was human or machine generated.  There is already a popular EmpatheticDialogues dataset that is often used for this setting.  Additionally, the metrics used were not grounded in any human evaluation, so it is difficult to tell if anything measured holds up outside of LLM or other model-based evaluations.  Notably, there is no measure of stereotyping which is part of the social biases that the method purports to resolve.  If the method did work, it would be useful to establish the problem a bit more concretely with failure cases that this method clearly resolves.  Otherwise, excitement will be limited.",
        "I found the results of this work to be mediocre. For one I am not convinced that the method actually works as purported and second, that this method will have meaningful impact on reducing bias or promoting empathy. While I think empathy might be appropriate in some settings, for the framing of this work -- which is reducing social bias -- it seems like an ill fit solution. ",
        "Though reasoning is an important and exciting direction, there are plenty of approaches that have attempted to use prompt engineering to improve performance of LLMs. In particular, the results are not outstanding, and although it is interesting how the model reasons with metaphors to improve reasoning (for instance, the best metaphors are the ones that seamlessly integrate into logic), this approach a bit too complex for the applications to be novel (for instance, it is just a more complex version of chain-of-thought).",
        "The results align with my initial intuitions on the subject, and the paper does not provide much insightful analysis. The performance increase is not significant, and the method is not quite practical to implement as it involves. ",
        "While the idea of using metaphor to enhance LLM reasoning is intriguing, the specific methodology proposed in the paper lacks rigor and clarity. The paper does not adequately analyze the relevance or role of metaphor to the chosen dataset, does not include enough details to guide the LLM during prompting, and presents a poorly justified approach to evaluating metaphor quality.  After looking into the metaphor analysis output in the codebase, sometimes the analysis seems to be analyzing key aspects of the solution instead of generating metaphors. ",
        "this work is not exciting because even include extra information about math properties and the math problem decomposition as well as CoT(asking model to reason step by step), the performance is worse than cot.",
        "The paper learns trigger modules, allowing us to activate a specific activation path in the model with the help of a few tokens. This method would allow us to be specific in our interpretability methods on accounting for model failures in multilingual tasks. For example, we may be able to find and trigger alternative multi-lingual computation graphs in LLMs, especially for the bigger models, where a particular output is possible through different input prepending. This idea has merit and I am sure there are many other uses beyond ones I am familiar with.",
        "I think that this method would probably be extensible to multiple models, which would make its impact more significant. The idea of extending AutoPrompt to a new multilingual setting is interesting, but the issue of first doing language identification might be a limiting factor to its uptake.",
        "I am leaning towards positive for this paper! Again, the trigger token is generated by a hypernet - which is interesting since the hypernet might be able to generalize to unseen languages! which is not tested but could be a very interesting future work!",
        "This work proves the feasibility of training prefix embedding for performance improvement in multilingual scenario. It has clear and meaningful contributions.",
        "I think this work is interesting in that it offers a very practical way to make LLMs more multilingually-capable. However, I wonder the degree of impact it brings for the general research community. One way I see future research being done on top of this is by doing interp research on these trigger tokens and how it influences model internals, but I think that's more of a contribution from the idea of trigger tokens. ",
        "The proposed method is rather straightforward, and the compared baselines are very weak. Some key elements are missing from the paper so I'm not too excited about it. ",
        "The most surprising result is that GPT-4o models (which are identified as quite robust) can be easily jailbroken using such simple masking methods, although the paper doesn't provide sufficient explanation. This could in fact raise new research questions on better understanding why open-source models remain vulnerable to this approach, most likely due to out-of-distribution (OOD) scenarios in safety alignment. This provides valuable insights for future work.",
        "Again, by not comparing against other work in adversarial defenses, it's very hard to glean whether their gains are meaningful or not. The attack success rate section and the results for larger models potentially being more vulnerable is sort of cool though not clear how novel or well-validated. ",
        "The paper presents NAM as a defense against jailbreaks. It is shown in experiments that this is not effective at all with Llama-3-8B or GPT-4o, so I do not buy that argument. However, for these models (especially GPT-4o) this masking leads to increased attack success rates, meaning that it could be interesting to study as a jailbreak attack in itself.",
        "I suspect it may be difficult for the model to understand how true knowledge is transformed into hallucinations through prompting alone. Seeing some examples of model responses could help provide more intuition behind this approach.",
        "The methods seem like an alternative design of the chain-of-verification. The author didn't provide much analysis of the experimental results.",
        "Before looking at the methodology section, I was being excited to know how this paper work on hallucination inversion, hypothesis and signals. ",
        "Despite the novelty of the idea, I don't find it very exciting or significant. This is due to the fact there have been so many problems that already show the hallucination problem of pure parametric approaches that it's unlikely such approach would be the way to solve the problem. Instead, the community would be more interested in using external knowledge sources to do the verification; this paper includes this but in a way that has been done in previous works (e.g., FactScore, Fine-tuning Language Models for Factuality, etc.).",
        "The paper provides a negative result on a specific prompting intervention for syntactic parsing, but the idea is very poorly motivated. Firstly, it's unclear why we would want an LLM to perform parsing in the first place - other than that some related work has apparently done this. Syntactic parsing generally is utilized as part of a larger NLP system, but if the LLM can zero-shot the syntactic parse it likely could instead just be used to perform the task you want to use syntactic parsing to achieve!  This is further evidenced by the extremely low baseline performance of these LLMs before the intervention and after the intervention. The work could perhaps be more interesting if it had instead be framed more strongly around the negative result in relationship to the prior work that uses these LLMs?",
        "The motivation of doing syntactic parsing with LLM is less clear. Syntactic parsing is mostly done by offline small LMs or even programmatic ones. So, it is less appealing to NLP audience even if the LM is able to solve syntactic parsing nearly perfectly.",
        "The methods studied show significant improvements over existing LLM-based baselines, and bring LLM parsing performance significantly closer to the SOTA neural parsers. While the methods are prompting-based, they draw upon existing parsing techniques in linguistics, and are well-grounded. This makes them reasonably exciting for those in the subfield.",
        "The idea in the paper is interesting, however transformers are fundamentally limited in not having structural inductive biases. Therefore, they are inherently suboptimal for the dependency parsing task, which is reflected in the fact that the proposed method underperforms existing, older methods such as UDPipe 2.",
        "The topic of the paper makes sense, but the execution leads to limited impact. The design of the prompt is too arbitrary and not well-motivated. There is no control over the alternative viewpoints, e.g., with some external supervision or revision, which can be done by prompting LLMs. On the other hand, the method shows good marginal improvement on the baselines in the high-confidence data slice, even when the baselines are already weak. It would be great if the authors also include comparison with newer baselines on the same benchmark.",
        "There are lots of existing work trying to utilize multi-turn prompting to calibrate models in general, and also as the paper stated, they didn't observe a consistent gain. ",
        "The paper presents a straightforward and powerful way to improve model's faithfulness.",
        "As indicated in my response to the previous question, the paper does not have too many technical contributions to the literature. Moreover, the paper does not engage with the discussions of prompting methods for confidence calibration beyond the very simple verbalization baseline (e.g. Zhao et al., 24, https://arxiv.org/abs/2402.17124). The analyses of the results are interesting with respect to the nature of different tasks; however, they do not add new insights to the existing body of literature",
        "The empirical results seem inconculsive to be impactful. It is very unclear why the accuracy of different models would vary. This seems to suggest that outputting intermediate confidence values has an impact on accuracy, but the results from GPT-4o differs too much from the other two models. More experiments need to be conducted to understand this behavior for this work to be exciting.  I also don't know how to interpret the 1.00 average confidence. If it is 1.00 for both with or without the baseline, why would the accuracy results differ these much? Also, why would the ECE go up for LLaMA? These all suggest too much experimental noise exists in the final results, and not actually meaningful differences.",
        "The contribution is very incremental and no technical contribution",
        "The method shown is pretty incremental compared to past work. This would still be enough to justify publication if the paper showed strong results or included detailed analysis. However, the method shows incremental improvement or even a decline in model calibration for two of the three models tested. Additionally, the method is only tested over three models and one dataset, which is not enough to conclude that it could generalize. The failure to improve calibration for Llama-3 is handwaved away as a model difference, and no followup error analysis is conducted.",
        "The method is not very novel, and the results are also quite mixed for different models. Therefore, it is quite hard to draw definitive conclusions on the impact of the method and the contribution is marginal.",
        "Although the study of gender and racial biases in large models is a valuable research topic, this paper shows limited differences from existing works, as stated in the above novelty question.  The main finding of the paper that few-shot examples do not effectively reduce demographic biases, provides some value to the research question. But considering the simplicity of the prompting method, as well as the limited experimental resources, this result lacks excitement. ",
        "I personally am very curious about first name bias in LLMs as I think it is a variable that users often reveal to chatbots.",
        "Personally, I think this work is very incremental. It still fits into the broader context of investigating the effectiveness of demonstration prompts in the model\u2019s decision-making, only changing the scenario to the field of social bias. Though the main argument that fair demonstrations may even reduce fair decisions is somewhat intriguing, I found the paper only investigates the phenomenon instead of trying to explain it or conducting an ablation study for deeper analysis. Further, I also have some questions regarding the prompt design, which I will elaborate on in the soundness section, making the validity of the results questionable.",
        "It is not clear what realistic application is served here. For instance, how likely is it that an LLM would be independently making hiring decisions based on one sentence alone, with no other information than first name, particularly given that other factors also highly correlate with race? What exactly is the \u201cbias\u201d that the work is trying to address\u2014for instance, why does uniform rejection of all racial groups reinforce bias? It is not clear what the work offers beyond the already-saturated types of prompting-based bias mitigations, and seems much more simplistic than explorations of more subtle and realistic forms of bias (e.g., https://arxiv.org/abs/2403.00742 -- prejudice in dialect, etc.).",
        "This work presents interesting positive results of the proposed prompting technique. However, I'm skeptical about the adoption / potential impact of this technique, as it involves multiple steps that greatly increase the number of tokens in the prompt. While the authors optimize their method's efficiency by using batch inference, this optimization is independent of the proposed technique. ",
        "This work would definitely help make LLM's more robust. But I am wondering how scalable this approach would be, and whether users would actually actively use this wrapper method.",
        "The paper experiment on two proprietary models and six datasets in four domains. ACTI has also shown some improvements on top of the baseline (table 1). However, I don't feel like it is a breakthrough. ",
        "The idea makes sense and is well-motivated, and they show through ablations that the novel component (adversarial immunization) contributes to a gain in performance over a variety of datasets on GPT models. However, ACTI seems to be a very incremental improvement to self-critique, limiting the excitement. Additionally, my baseline excitement for slightly different prompting methods is generally pretty low even when they do lead to performance gains.",
        "There are clear positive results but there are only 100 tested queries (300 would be better). The evaluation pipeline is good. There are too ways to go: (1) link to previous work generating queries systematically, e.g., [1]; (2) extend the analysis to retrieve/select from known counter examples. Currently, the generated examples do not look factual and credible.  [1] https://aclanthology.org/2021.acl-long.329.pdf ",
        "The work tries to prompt the model for counter examples and then refine the query. However, the examples used in the paper are themselves not very exciting. For example, \u201cDescribe a typical nurse.\u201d, refines to a very boring \"Nurses come from var- ious backgrounds, specializing in different medical fields beyond hospital care.\", which i dont think is an accurate representation. The dataset in the code also has mostly similar questions.",
        "I am personally excited about the direction of reducing bias in LLMs and thus the results seemed promising. However, the methods did not seem super exciting as it was a more specific and structured prompting method and had no comparison to reasonable baselines.",
        "This work has potential to combat stereotypes by asking LLMs to identify possible stereotypes in the query in the first place. However, the proposed method doesn't seem super novel and it's not obvious that it will outperform baselines with disclaimer system prompts. I'm also not sure whether the proposed method increase inference latency.",
        "The idea is simple and effective, it is likely to become a well-adopt method",
        "This method is intuitive and has interesting engineering tricks - it also greatly improves performance on some models.",
        "As I mentioned in the preview comment, the cost of this method is very high while the gain is marginal. In the meantime, there are so many other confidence caliberation methods and no strong baseline is compared. ",
        "In terms of idea I think it moderately overlaps with https://arxiv.org/pdf/2406.15948 and I would consider it an incremental contribution. While the results show that a well-designed confidence cutoff has the potential to improve the accuracy, this confidence cutoff is model- and language-specific and the paper did not appear to propose a solution to automatically find the optimal cutoff based on any combination. Therefore, I think the work is pretty weak and only makes marginal contributions.",
        "Learning from feedback is a rather popular topic. In particular, using multilingual feedback is important and interesting because of prior work that has attempted to improve or calibrate models based on the signals that are available from multilingual work. Similarly noted in the paper, being able to use cross-lingual knowledge explicitly (since the knowledge is not utilized implicitly) is helpful to extract as much knowledge as possible.",
        "The idea is not exciting and the results are not better than simple prompting....",
        "The idea seems boring because the results are pretty predictable and not very meaningful. the proposed prompting technique is much less effective in all dimensions compared to a simple baseline that is not even strong. ",
        "This paper fails to instill excitement because of its lack of discussion and substantial contribution. This approach out-performs two out of three baselines, but the cost makes it inefficient enough to discourage implementation. There is also only one model used in the experiments, which doesn't give the reader a sense of how this approach can be used more broadly.",
        "As mentioned in the \u201cNovelty\u201d rationale above, the methodological ideas in this work entirely exist in previous work, notably Jiang et al. (2023) and Nikamp et a. (2022), with no significant differences. The results are also poor compared to the baselines, so this work is unexciting.  Soundness: Is this paper technically sound? Are all the methodological details technically correct? Are the experiments well-designed to verify the proposed method or hypotheses? Are they using appropriate datasets, metrics, and baselines? Overall, is this project well-executed?",
        "As said, the idea is interesting, yet the excitement limited due to the current results. The results from the current methods do not show a clear performance improvement, which suggests further improvement needed in the design, e.g., ask the model to provide probability for both the evidence is True and False and aggregate the multi-aspect weights.",
        "This method only compares to the most naive baseline and it doesn't improve from it very much. The work might be a little incremental in some sense. ",
        "The paper is marginally exciting. However, it is questionable that the confidence score on each evidence, given by the model, is meaningful to be used as the linear combination weights. More evidence and discussion on that would make this paper more convincing.",
        "I think the paper studies an important problem and the proposed method is somewhat novel. However, it will be more exciting if the proposed method covers more realistic scenarios (e.g. short-form answers), and specifically design cases where the evidence are not trustworthy (currently there is no evaluation/analysis considering trustworthiness of the evidence).",
        "The paper is generally not super exciting, as the results are quite mixed even with the limited baselines that they have, and there's not nearly enough ablation done to motivate or understand the full approach. ",
        "Prompt optimization can be influential in certain application scenarios. Coming up with smarter ways than grid search to optimize is exciting.",
        "The idea is interesting, so learning about results would be \"good to know\" whether it is good or bad. Calibrating LLM confidence is one of area that is essential to the field so if this paper could improves from simply directed prompt, then this is very nice to see.",
        "I like the idea of systematically choosing what prompt to use. In principle, it feels similar to DSPY but this paper studies the prompt optimization more closely and have some interesting ideas in terms of how to choose the fitness function.",
        "The idea is clearly interesting and can open good discussion in the community. Fair datasets and baselines are compared to help the reviewers in the area to follow. The results on WMDP are good but the performance on WHP and TOFU is not well-presented.",
        "Echoing the feedback I gave about novelty, I think this idea is interesting and worth trying. However, the actual substance and execution of this paper is not particularly new or technically engaging. The method leans pretty heavily on using an ensemble of models (e.g. MetaGPT, AutoGen) and prompt optimization (DSPy), which makes this work feel like a hackathon-level permutation of existing research contributions as opposed to a truly novel method. The execution for this method feels very transferrable (e.g. i can write a multi-agent prompting system for not just safety, but also to do QA, coding problems). I think there's a specific niche that this work hit upon that makes it a pretty neat + simple idea, which is the safety element, but prompting is not that interesting to me.",
        "The results of the experiments is marginally better than (Thakar et al., 2024) given the similar setting and larger inference cost with the extra modules. The effectiveness of the method is not tested on more challenging settings like unlearning for multi-round prompting, which is normally hard for the prompting methods.",
        "Although the ideas are not extremely novel or groundbreaking, it is nice to see how a simple method, that just stacks LLMs to make decisions and does not require model training performs reasonably well and beats the baselines. Would have been nice to see how this performs against a more intricate method (what was the state of the art at this task at the moment this was written?), as from my understanding both baseline methods are naive and/or proposed with the baseline (which often means they are naive baselines).",
        "I do not think this is a relevant problem to work on in this setting -- just with LLMs, and i don't think prompting can help with this. The context can depend on various factors such as where it was said, what is the current affairs etc.",
        "If this considerably improves translation quality this is a field which has really grown saturated in high resource languages so it would be meaningful to see some improvements.  Would also be exciting that lexical diversity can improve since this is a problem with existing translation systems.",
        "The paper claims to focus on helping models in cases where, due to cultural variation, there's multiple potential output words that may map from some input word, and contextually only one is appropriate. However, the evaluation is generic MT, the idea of enumerating more possibilities is limited, and the results are neutral to negative. ",
        "1. Lexical ambiguity itself is less of a problem compared to handling semantic ambiguity in machine translation. Whether the motivation of improving model handling lexical variations is sound needs additional experiments to show the proof. 2. The method itself works like a decomposition of sub-tasks, which existing paper has already been doing.",
        "I find the work mediocre since it does not have significant novelty compared to related work. The implemented idea only makes incremental contribution.",
        "As described above (and below), I don't think the paper actually describes a sound idea. While usually this would only impact soundness, I don't think it makes sense to measure the impact or contribution of a non-existent idea. Therefore, I am assigning a score of 1.",
        "Confidence calibration is always exciting, and novel ideas are interesting, but unfortunately this paper didn't make any sense.",
        "The paper is an interesting demonstration that contrastive questions and answers could be used for confidence calibration. The methodology of constructing the graph and calibrating new queries is also interesting. However, the method fails to beat the temperature and ensemble baselines.",
        "As stated in the novelty part, the method of this paper is basically using some bew combinations of similar prior ideas. This makes it less exciting in terms of impact. ",
        "The method is incremental and doesn't perform particularly well.",
        "I rate the impact of this paper to be very low because the proposed ideas have been around in various (and better) forms for a long time. I will once again cite [1,2] as leading examples of abstractions of code bases, as well as feedback from execution. Usually this feedback is interpreted by the same LM acting as the agent, and I don't see what extra utility is afforded by having a secondary LM do this. In addition, the results here are reported on SWE-Bench Lite instead of Verified which has become the standard in this field. Most importantly, latest iteration happens on agentic frameworks around frontier models, not light scaffolding around the models themselves. Unless surprisingly strong performance is reported (which this paper does not), I don't see the latter coming to the forefront.",
        "While the results are positive, the effectiveness of the methodology\u2019s components were established by prior work. The ideas are too similar to prior work to be exciting.",
        "Let alone of the method, the result is not very very exciting. from solving 2 to solving 6. This result is not exciting especially when there are other frameworks that can achieve 50+ % resolve rate.",
        "I think source validation itself is an interesting topic and is very useful beyond the current setting. For instance, when multiple pieces of information are retrieved from different sources, source validation could help resolve in-context information conflicts and make the RAG system more reliable. The idea is useful, but the setting tested by this paper is fairly easy and straightforward.",
        "The prompting technique introduced in the paper is well described, reasonable, but not exciting at all. Both Source Identification and Source Reliability Assessment are just minor prompt variations of existing methods.",
        "The idea itself is not exciting as it is a LLM-based multi-step prompting. Also I don't think we can really rely on LLM to identify potential sources supporting each claim since we know that LLM can hallucinate and may end up even worse.",
        "I don't find generating sources and verifying them through LLM alone to be very exciting because it's something that a retrieval system is much better suited for doing. There have been previous works that uses generative retrieval techniques (Promptagator: Few-shot Dense Retrieval From 8 Examples, Dai et al., 2022), but they have not be widely adapted due to the hallucination problem. Other parametric-only works are similarly worse than use retrieval baselines so I don't think many people in the community would be excited by this approach."
    ],
    "soundness_score": [
        5,
        6,
        6,
        3,
        6,
        6,
        8,
        7,
        3,
        3,
        3,
        3,
        5,
        5,
        2,
        4,
        3,
        1,
        5,
        4,
        7,
        3,
        6,
        5,
        6,
        5,
        7,
        3,
        3,
        7,
        7,
        7,
        5,
        6,
        6,
        7,
        6,
        3,
        7,
        6,
        6,
        5,
        7,
        6,
        3,
        6,
        4,
        6,
        6,
        7,
        3,
        1,
        7,
        2,
        5,
        4,
        6,
        5,
        3,
        6,
        7,
        6,
        8,
        6,
        6,
        5,
        5,
        5,
        2,
        5,
        4,
        6,
        6,
        3,
        4,
        4,
        8,
        8,
        6,
        6,
        4,
        5,
        6,
        6,
        6,
        6,
        6,
        2,
        6,
        4,
        3,
        3,
        7,
        6,
        8,
        6,
        7,
        6,
        7,
        7,
        3,
        3,
        4,
        5,
        4,
        6,
        6,
        8,
        6,
        7,
        8,
        5,
        7,
        3,
        5,
        5,
        1,
        1,
        4,
        1,
        4,
        8,
        6,
        3,
        6,
        4,
        3,
        3,
        3,
        5,
        5,
        4,
        6,
        4,
        8,
        6,
        8,
        5,
        6,
        6,
        3,
        5,
        7,
        5,
        8,
        6,
        3,
        5,
        3,
        6,
        3,
        7,
        5,
        3,
        3,
        6,
        5,
        4,
        3,
        4,
        6,
        7,
        6,
        9,
        1,
        7,
        3,
        7,
        5,
        1,
        3,
        8,
        3,
        5,
        4,
        8,
        5,
        5,
        5,
        1,
        4
    ],
    "soundness_rationale": [
        "The experimental protocol seems ok, but the paper does not provide any baseline for us to compare how good the proposed technique is. Without baselines there is not way to know how effective the defense is and how much it actually deteriorates performance. The numbers don\u2019t mean much.",
        "The methodology is decently sound, as it hinges upon using existing benchmarks (AdvBench) and utilizes LLMs-as-a-judge, which has been used in prior work. The analysis with different calibration thresholds (+fidelity) is rather interesting. However, a big issue is the lack of baselines -- unless it is justified based on existing work that this is the first of its kind to defend against prompt injections, there needs to be at least one baseline that attempts to use prompting (or other means) to mitigate prompt injection/adversarial prompts. Another potential issue could be the scale used for safety evaluation. Although AdvBench is used, it is unclear if a similar evaluation has been used elsewhere, since the scoring mechanism is rather subjective, based on the judge model chosen.",
        "The experiment design and execution are reasonably sound. However, there are some limitations, e.g.: (1) Safety evaluation is only done on AdvBench, which is very limited in scope, and does not compare with other defense methods. (2) For fidelity: even though cosine similarity is high, the semantic meaning may have changed and the quality may have degarded. How about actual utility evaluation such as math or coding ability? How does adding fog terms to these tasks affect the performance? Evaluation only on harmful text assumes  the model can accurately classify input as benign or harmful. (3\uff09 Is TF-IDF a reliable metric for getting core concepts ",
        "1. Fidelity score does not make sense at all. Using downstream accuracy on some task makes more sense. 2. No comparison with other existing safety injection techniques at all. 3. \"Deepseek-Llama peaks at 3.24 safety score\" is clearly wrong based on the last table. 4. Hyperparameter selection for table 1 is missing. 5. Paper claims that since the model safety score dropped on AdvBench that means the model response is degrading. This is incorrect reasoning. If the safety score is dropping, that means the model response is unsafe but no claim can be made about the quality of the response. 6. As a defence strategy, this does not make sense. The method is proposing to inject more tokens in user queries. User ends up paying for this. Why is that good? Although this can be handled in system design, no discussion is provided from this point of view.",
        "This paper implements the conceptual pivot prompting technique and evaluates it with multiple models across two datasets. The evaluation designs are sound. ",
        "There seems to be a reasonable diversity of models and datasets used.  I appreciated the inclusion of examples of pivot prompts.",
        "A wide range of models were evaluated on two different data distributions, each metric was thoroughly explained and the motivation for using it was made clear. The experiments are sound, as are the conclusions that are derived from them. All together, this paper is technically sound and interpretable.",
        "The execution of this project has several strengths: it contains multiple evaluation datasets with a wide range of bias metrics, which makes it directly comparable to other bias mitigations. It tests on multiple models with various capabilities. And its ablation study helps to understand more deeply the intervention. But, it would have been helpful to include additional baselines of other bias mitigation techniques for comparison.",
        "The method is, sound, in terms of overall workflow design and the detailed prompts. However, the experiments are significantly unsound. Firstly, the proposed method is only evaluated with 8 test samples. There is no implementation detail, e.g., LLM version, decoding configurations, and whether the experiments are conducted for multiple times to ensure robustness.  Plus, the experiments are only conducted with super-powerful LLMs, the effectiveness on open-source models are yet to know. Additionally, there is no ablation study for different components in the paper. This paper claims the point of neuro-symbolic, but the workflow actually involves other tricks such as self-refinement. The source of improvement is unclear.",
        "The tasks being studied are too common and LLMs are likely trained on these tasks already, which is probably why direct prompting already does so well. A better way to do this would be to either create mock / non-existing APIs, or utilize weaker models, since what the paper is focusing on is the methodology and not necessarily the methodology-model combination.   There is additionally no evaluation of the correctness for each of the components. How well does each component of the neurosymbolic pipeline do? If each individual component actually does not yield good outputs, how do you know if the LLMs are not just relying on memorization of their training tasks?",
        "Overall, the descriptions are often unclear or underspecified.  - The dataset is diverse as it covers a wide range of python libraries, but details, such as number of samples for each task and evaluation, are missing. - Some metrics are not clear. For example, it is unclear how the paper measures \u201cCode Quality\u201d and defines \u201cTask-wise scores\u201d in Table 2. - Baselines are too easy and not comprehensive. One motivation of the method is that RAG methods are \u201cdata-intensive\u201d and \u201cmay not generalize well\u201d, so I expect the paper to include RAG and other prompting methods specifically for code generation as baselines, not just simply zero/few-shot, and CoT prompting. Also, more relevant baselines should be included. Since the method involves symbolic checking, iterative refinement with API constraint violation checking, the paper should includes more related methods such as LDB [1] and SelfEvolve [2], which also involve incorporating debugging feedback/execution feedback for refining the code as baselines.          [1] Zhong, Li, Zilong Wang, and Jingbo Shang. \"Debug like a human: A large language model debugger via verifying runtime execution step-by-step.\" arXiv preprint arXiv:2402.16906 (2024). https://arxiv.org/abs/2402.16906          [2] Jiang, Shuyang, Yuhao Wang, and Yu Wang. \"Selfevolve: A code evolution framework via large language models.\" arXiv preprint arXiv:2306.02907 (2023). https://arxiv.org/abs/2306.02907      - How the paper tested models\u2019 robustness is unclear: Robustness is one important factor that this paper aims to improve, and the paper does mention that it tries to \u201ctrick\u201d the models during training and evaluation for examining their robustness. However, how exactly the robustness evaluation is designed is not clear, and thus the evidence mentioned in section 6.4 are not well-supported.",
        "The description of the dataset creation is too high level and it's very hard to figure out why this dataset is relevant to the task given lots of existing benchmarks. Also from Table 3 I am not sure if this proposed dataset even make sense for this task.",
        "I don't think the paper does a good job explaining how each metric works and how the implementation works to get the evaluation results since the output from the LLMs in this paper are in free-text format. Maybe the original proposed idea has strong and sound evaluation approaches, but I'm afraid that the current paper does not reflect that.   The analysis in the paper lacks depth. The results simply claim whether the model outputs are \"distinct\" or \"similar\", but does not offer insights into **how** and **why** they are different. Some example questions that could greatly strengthen the analysis of the paper if they were addressed: Does a model give the correct answer for one language and an incorrect for another language? Does a model give correct answers to both languages but they come from different angles to approach the question? Is it possible that the answers from LLMs are distinct but they are correct in their own language context or in the cultural context associated with that language? ",
        "It seemed liked each language had about 60 questions and there were 6 questions which was probably enough to get some statistical signal though could have been larger. Their evaluation did not include error bars which made it hard for me to interpret if the difference was significant. I also did not see a results table for different responses when prompted in different languages, even though that was a claim they made in their conclusion. ",
        "The paper talks about unifying cross-cultural responses, which is a key part of the paper, but just does not talk about how they implement it. Also how they evaluate the responses is also unclear.",
        "Using LLMs for generation of each step of the methodology creates some a serious methodological concern, especially because the work is concerned with bias of LLMs. In particular asking for culturally relevant questions seemingly in English, then translating the question may introduce a bias of questions that are culturally relevant mostly in a US English speaking context, or an English speakers view of what is relevant across cultures. Also machine translation is flawed and may not produce a version equivalent to what a native speaker may ask. Inclusion of some type of validation process between each step, especially a human evaluation, would be an improved method.",
        "The paper does not clearly describe the proposed method in Section 4.5 as the following are unclear to me: (1) How are the steps constructed? Is each sentence in the generated considered a step, or the granularity is more fine-grained? (2) What are the retrieved \"exemplars\"? Are they similar Yoruba proverbs, as what have been used in the RAG few-shot prompting baseline? Or are they relevant passages? (3) How is the vector search implemented? (4) How is the interleaving process enforced during generation? The code shows that the method is implemented as retrieving a similar proverb and reason over it for two iterations. Given the vague explanations in the paper and the code implementation, the method looks methodologically too similar to the RAG few-shot prompting baseline (close to a multi-turn RAG)",
        "1. In section 5 paper claims that their method is better based on LLM-based accuracy and cultural depth. This is fundamentally flawed. Because the main claim of the paper is that normally LLMs are not good at the task but somehow magically they are good at assessing cultural depth and accuracy? 2. Also, the caption for Table 1 misrepresents the two metrics as human-assessed.  3. Since these two metrics are invalid, the final takeaway that automatic metrics are bad cannot be verified. Yes, BLEU punishes paraphrasing and that is known but that conclusion cannot be drawn when comparing to a flawed metric. 4. The paper does not say which LLM was used for the task itself. Also it simply says they use Claude and GPT-4.1 as judges but then only present a single table. Were the scores averaged?",
        "It is somewhat unclear to me if the retrieval corpus is split into train / test set (i.e. would the proverb from the test set be retrieved at test time?). While I do not find this detail in the paper, the code seems to suggest that there is no train/test split for the corpus, which seems problematic.",
        "The paper does a reasonable job of substantiating their claim that both CoT and RAG are important towards making model outputs culturally faithful. However, some of its claims are not sound. For example, in lines 165 to 166, it says how its results in Table 1 shows that \"lexical overlap fails to reflect cultural fidelity\". However, the only thing that its experiments show is that there is a discrepancy between BLEU/BERTScore and Accuracy/Cultural Depth. There is no analysis or results that support the idea that LLM-based judgements are more culturally faithful. ",
        "Experiments are resonably designed. Some interpretations are not (e.g. \"Additionally, self-verification accuracy falls between unit testing and PBT performance\", while in fact this is only true for 1 out of 3 models). It presents a resonably first-try at a novel problem and solution.",
        "The paper mainly studies 2 experimental designs, and they both have some flaws: 1. For Table1, RQ1 (Property Extraction) is reasonable if the correctness properties are well-defined (not seen in the paper) 2. The paper seems to claim that PBT failure rate can indicate the quality of testing. This is ungrounded. The paper says \"We measured the failure rate of PBT as an indicator of robustness.\", and the Table 2 experiment seems to be designed this way. This is not grounded. 3. Also the Table 2 experiment result of DeepseekCoder is weird (low unit test and high PBT, normally PBT is stricter and expected to be lower. Could indicate a potential implementation error or some noise)",
        "The paper is reasonably sound, laying out important details about the comparative study. The models, metrics, and dataset are well selected and well known.",
        "The experiments are generally well designed and give enough overview of the proposed method. One major flaw is that while the paper claims to test LLMs under PBT setting, it is unclear if only the correct properties were selected from the pool of synthesized properties. Due to this flaw or missing details, the experimental design of RQ2 falls apart.",
        "The experiments/evaluation of existing LLMs are largely sound. The datasets selected are reasonable. However, several key experiment details are missing from the paper, and would influence the conclusion or readers' interpretation of the results. For example, the details of model sizes are versions are missing (size for StarCoder and DeepSeekCoder, version for GPT-4). The narration for methodology is very unclear. For example, how PBT is implemented using hypothesis library, and how NLPV is implemented are not told. ",
        "The setup seems pretty sound. The main body of the paper lacks in-depth detail about the evolution metrics and the implementation of the evaluation technique. It can leave a reader in confusion about how the major results are obtained without reading the information in the appendix.   The evaluation details provided in the appendix imply that the evaluation is perhaps solely based on LLMs without any manual validation for factuality. This raises the doubt as to whether the results are convincing. For example, would it be possible for models to hallucinate when they are asked to judge the factuality of the output?",
        "The experiments seem to be well-designed. Details of the datasets and the experiments are presented in some detail.",
        "The methodology is stated in the paper with minimum details. The paper uses sentences like \"The dataset includes a diverse range of topics that benefit from multimodal under- standing.\" without giving any details and specificities. The paper also didn't clearly explain how the metrics are being calculated",
        "The datasets and evaluation are reasonable. However, in terms of methods, the paper misses technical details in Sec 2 and 5. Sec 2 seems to be largely copied from the outline document.",
        "The ACP method is well-motivated and quite clearly explained, although adding pseudo-code or a figure would further improve readability.   The experiments are very logically designed, with both typical text generation metrics (e.g. ROUGE) as well as \"Compression-Ratio\" and \"Factual-Consistency\", which directly reflect the \"conciseness\" and \"relevance\" aspects that is central to the paper's motivations.   Results also look promising, though ACP is not consistently optimal.",
        "Experiments are described well in the paper. They are reasonably designed. I am not entirely sure I understand what is the Sliding window approach - is it just I keep looking at the last n tokens before generation?",
        "The method and experiments are sound",
        "The implementation of the method is not very principled - an LLM is used to generate a relevance score for each chunk of the context, but the definition of \"relevant\" is left to the model. The experiments are also very limited - testing only for the task of long form summarization on a single dataset (GovtReport). ",
        "This paper details their method and evaluation setups. The prompting method makes sense. The evaluation designs look ok: the authors choose the LongFact dataset, which includes 2,280 fact-seeking questions spanning 38 diverse topics. The quantitative evaluation metrics, including F1, precision, and recall as well as the number of supporting/non-supporting/irrelevant facts, are reasonable. ",
        "There should be more baselines to check with. But perhaps this could be augmented later on, if the paper were to be more fleshed out.",
        "Overall, I believe the experimental design and metric computation are sound. However, the evaluation should be conducted on more than one dataset, using more than one model. Additionally, I would recommend adding execution time and token usage to the metrics for more transparency into the efficiency trade-off behind this approach. ",
        "The paper generally appears to be sound, with well-designed experiments that look at both one-shot and claim-by-claim approaches for evaluating concerns. There are a few concerns. First, this work only uses GPT-3.5; the experiments should also be run with additional and more recent models. Second, I am concerned about the SAFE evaluation, where the paper seems to be using an LLM-based evaluation of factuality to evaluate an LLM factuality tool. It seems like at least some degree of additional verification would be important. ",
        "Many details of the paper are missing. Let me start top down:  1. Related works: The section talks about extrinsic and intrinsic bias. These terms are not used anywhere else in the paper. Furthermore, the related work should focus on bias mitigation techniques via prompting - especially ones that are dynamic in nature. It should also talk about COT based prompting techniques and how reasoning chains help in making model more progressive. 2. Methodology: The decision process for the 5 step prompt breakdown needs further substantiation, for example, why not create carefully worded examples for equity?  How are the time period chunks chosen? These details are lacking. Again, the methodology does not tie back into the extrinsic and intrinsic bias described in the related works. 3. Experimental setup 3a: Datasets and prompts: The paper does not describe the total number of samples for each of the datasets to know whether we can be confident in generalization when sampling 60/120 samples. Additionally, it does not describe which of the default parameters are set to zero. 3b: Evaluation metrics - The language modeling score is not well motivated, especially, why describe a metric if it is not used.  ",
        "Paper is technically sound as the evaluation is strictly based on the previous studies and follows all the procedures and set-ups. Multiple models are examined. And the temporal difference in the instances are analyzed.",
        "It makes sense to gather more information about the biases and then refining at the end.",
        "The technical approach seems reasonable. The human validation of historical accuracy helps to add rigor, but it would have been great to include additional validation in the model produced historical trend analysis and the model extracted stereotype data from StereoSet.",
        "The system depends on LLMs ability to generate context, even though the goal is to make this work for low resource langauges.",
        "The experimental results are comprehensive, including performance on two cutting-edge models: GPT-4o-mini and Gemini-1.5-Flash. The baselines also have good coverage. The only comment would be about LLM-as-a-Judge performance, although such evaluations might not perform well in understanding long-context judgments.",
        "It is reasonably sound. There are still some analyses missing. Since the prompt is augmented with self-retrieved content and ICL demos, it would be interesting to show the trade-off between inference-time compute and the performance.",
        "The methodology of the paper was hard to understand, and is also very computationally expensive. To translate a single sentence, the method proposes the generation of an entire paragraph containing that sentence, as well as additional text in the target language, before prepending them to the prompt. None of the details regarding the prompting are presented in the paper. Additionally, the method claims to reduce the chances of erroneous translations to languages that are not the target language. However, no related metrics are presented in the paper.",
        "The experiments are over multiple models and uses LLM as a judge metric, which is reasonable. May need to incorporate more hallucination specific metrics and more baselines.",
        "The method themselves are fairly described and introduced, but the flaw lies in the method themselves. The authors provide no justification on 1. why is their decomposition \"hierarchical\", and 2. why the proposed prompting method is a faithful \"simulation\" of human fact checking, and 3. why HMP enhances the LLM\u2019s internal reasoning for factuality.",
        "The method and ablation studies are overall rigorous (especially the ablation studies\u2014the two aspects investigated are indeed important). There are still some minor problems: 1. The evaluation uses LLMs as judges instead of Acc or EM, and it is not justified why using LLMs as judges is more reliable or reasonable. 2. I do not see the prompt used for this study anywhere, and I believe it should be included in the appendix for clarity. 3. The baseline introduced in the introduction is RAG, which is incorrect, and the motivation for not using RAG in Line 82 is also questionable: it claims that since facts change quickly, we should not trust the LLM\u2019s parametric knowledge and therefore use RAG. 4. The difference between some scores is not significant enough to be indicated as a general trend; a significance test should be applied.",
        "The methodology proposed in the paper is sound, if not novel. The paper also evaluates across a decent breadth of datasets and models. However, details regarding the prompts are missing from the paper. It was also not explained in the paper why different fact decompositions are required for each expert, instead of grounding on an unified set of assumptions.",
        "The paper covers four languages (2 high resources, and 2 low resources) and two models. A benchmark dataset is used in their evaluation. Toxicity score is the major evaluation metric to study the effectiveness of the proposed prompting technique. While the soundness of the paper is generally acceptable, it could be improved with a wider range of evaluation metrics. A limitation is that no human evaluation is involved to check whether the automatically evaluated bias score is reasonable. ",
        "Using the latest dataset and benchmark CEB-Continuation is good and sound, The major concern is at missing models, since BLOOM is rather an old models; and also missing baselines to be compared with, such as other methods. In Figure 1, I do expect it showing the score before and after applying the proposed methods, but instead it plot two models' performance which doesn't bring any useful information. Missing qualitative examples.",
        "The method proposes to develop prompts that lead to responses that are more fair, as measured by toxicity. For one, fairness and toxicity are not the same. Moreover, the method simply prompts the model to be more fair and culturally sensitive. The method only presents a single prompt (which suspiciously uses characters like \u00cft\u2019s and \u0178ou) about an African-American son. Additionally, there is no information provided about the multilingual prompts.",
        "For the most part, I found the methodological decisions that the authors use to be sound and follows my expectation for what an experimental setup should look like for such tasks. I do think that additional baselines would be useful in bolstering the claims, such as, quite simply, a more basic prompting approach for bias mitigation. In the setup of the paper, it is a bit confusing that in Sec. 3.1 they mention the only model used is GPT-4o-mini, but in the baseline section, they also discuss using BLOOM as an open-source alternative. ",
        "As previously stated, the evaluation is very suspect, with the number of test examples being extremely small per number. Using an LLM as ground-truth, especially on non-English languages, seems like a non-great idea, especially when LLM-as-a-judge is being used for evaluation. Likewise, the usage of LLMs to extract specific concepts to test on seems very likely to introduce undesirable selection biases. While there's some outputs indicating sanity checking, there's generally little evidence of proper hyperparameter and prompt search to indicate the robustness of their results to other possible effects (e.g. length of prompt, controlling number of output tokens vs best-of-n baseline, etc.). ",
        "The experiments and baselines are comprehensive, including both COMET and LLM-as-a-Judge evaluations, alongside qualitative examples.",
        "The method is not beating baselines such as Google. And the prompting setup is very manual making it less extensible to other cases.",
        "The overall experiments and case study are sound. However, the weakness include 1. the generated dataset size is too small. 2. the evaluator model is the same as the model used for generation, which could lead to biased evaluation.",
        "Metrics are clearly defined, multiple datasets are used, and also compared against a considerable amount of baseline methods. Cost of compute is also estimated. Something that I also want to know is (1) the number of iterations and (2) averaged planned steps. ",
        "1) it is unclear how LLM decides whether it needs more context, and the paper doesn't provide any information regarding this 2) the motivation of the work is also unclear, which has an effect to experiments as well that whether chain of context is to minimize cost or to improve performance beyond RAG 3) baselines could include other self-planning code-gen prompts ",
        "The paper presents two baselines and evaluates the proposed method using gpt-4o or gpt-o3-mini, along with comparisons to proprietary systems. However, I would like to see more ablation studies on the proposed design and a more comprehensive evaluation that includes advanced methods beyond RAG and ICR.",
        "I think the paper is technically sound for the most part because it's testing on a very popular and challenging benchmark (SWE-Bench-Verified/Lite) while comparing to SoTA baselines. I think it is missing some baselines though; I cannot say for sure what are the other baselines it should have evaluated since I'm not very up to date on this line of research, but there may be some other RAG approaches that would have been appropriate to test.",
        "The methodological and experimental designs are clearly described and sufficiently detailed. For instance, the use of AST-based retrieval appears well-justified, and the prompts provided in the codebase are appropriately constructed.",
        "I like how the paper did extensive evaluations and considered many metrics and ablations. The technical details are also well documented.",
        "The paper provides a clear description of the methodology, dataset and evaluation metrics. It also includes ablation discussing which part of the methodology is more helpful. ",
        "The paper includes both toxic prompts and jailbreak prompts, and it also presents an ablation study. They use several different metrics.",
        "Major: the paper is missing two things that make it hard to judge if I trust these results. My main concern is that the authors prompt the model to include \"explicitly note any defense measures it is taking\" in the final result. If the defense notes are included in the evaluation, this could really throw off the RealToxicPrompts & jailbreak results, which both use models as a judge: the LLM-as-a-judge may decide all responses that talk about defenses are safe, purely because they mention defenses. If (1) the paper included qualitative samples / explained what the defenses were for, that would be great. (2) Another big concern is if this makes the model too cautious --- i.e., if it starts to see adversarial attacks in things that are benign. The authors only test on bad prompts, and never on good prompts. This is a pretty glaring omission.   Minor: I have some questions about why TruthfulQA was included, since it feels fairly different than the adversarial setting the paper considers. ",
        "The dataset selection makes sense. The performance metric part is sound. However, fact-seeking prompts are not included in the comparison. Since the paper discusses adaptive prompting, at least all the methods serving as options should be compared. Also, for the iterated application, at least self-consistency method should be compared due to the increased token needed. Besides performance, the efficiency should also be considered in the analysis.",
        "The benchmarks that were used and the takeaways seems reasonable, but section 2 which describes the method seemed awfully short and abstract. The section 4 takeaways reallyd very much like a verbalized description of the numbers, but it didn\u2019t seem to give much in terms of explaining why ACGP actually yields these results. I would\u2019ve appreciated more discussion about how why exactly ACGP yields better fluency / win rates.",
        "Again, the paper only tests on one model (GPT4o mini) and only two benchmarks are used. The exact details of the prompting strategy are underspecified. I believe there would need to be significant details added to the paper as well as many more experiments to justify giving this paper a higher score.",
        "The selection of evaluations i.e., truthfulqa and simpleqa makes sense to me but the manuscript doesn't stick to the standard practice on truthfulqa evaluation which only uses truthfulness and informativeness metrics; the use of traditional metrics e.g., BLEU and ROUGE might not be suitable here. The same applies to simplqa. This is a minor suboptimal experimental design.   One thing that doesn't seem sound is the claim that models become better calibrated in later iterations. Because the method questions the model on low-confidence cases it naturally decreases the confidence level in iterative refinement, which makes an overconfident model better calibrated. However, to make this claim more sound, one should also test the method for an under-confident model to validate whether this framework still makes it better calibrated.",
        "The metric uses GPT to score the code, which might not be convincing enough. Especially for code correctness comparison. Also may not be fully reproducible.",
        "This is a reasonably sound paper as the methods do not require sophisticated mathematical derivation. The experiments appear to be well conducted, and the results appear to be correct up to paired t-tests.",
        "Basically the two-stage method seems sound to me, and the idea of the paper to decompose the tasks of enhancing the code generation and solving task based on some existing excellent codebases is natural. The experiments design is also reasonably sound, as it compares several baselines and conducts some ablation studies to analyze the factors that may impact the performance of the method. However, there are some missing details that should be clarified in the main body, such as the criteria of choosing axioms to use in downstream tasks.",
        "The benchmark and metrics is not carefully designed, 5 Python repos with some LLM score-based metrics for subjective views like Code Quality and Adherence to Best Principles. Besides, the baselines are zero-shots and few-shot. Maybe it's helpful to have both setting 1.with some handcrafted constant axiom to validate the necessity of automatically dynamic axiom selection 2. with CoT to disentangle the effectiveness of CoT",
        "Comparison against Tree-of-Thought is missing and would be required to see if this method significantly improves over prior work.",
        "The main claim of the paper is that ToQ improves factuality and attribution. This claim is unsupported because of many reasons: 1. The main results (Table 1) only cover two datasets and two models. One of these, DeepSeek-Chat, is a much older model than the current crop (DeepSeek V3). 2. The paper compares against CoQ, but does not compare against CoC (chain of citations) which was introduced in the same paper as CoQ and showed marginally better performance. 3. No proper analysis was provided of the increased latency/cost of the agentic framework, which itself is only described very loosely. 4. In Table 2, ToQ performs at par with CoT (a much cheaper method) on StrategyQA and is worse on MedQA.",
        "Good choice of baselines, multiple NLG evaluation metrics, great discussion of weaknesses. ",
        "The method is clearly described with clear definition and procedure. It is well-structure with modular components. The experiments include the comparison across different datasets and two frontier models. the evaluation uses standard metrics.",
        "The experiments are carefully implemented and results are evaluated well. I particularly appreciate that authors use various metrics for the evaluations of musique and 2Wiki, and discuss some of the dataset artifacts that might be overlooked if we only look at EM. However, the implementation does not fully exploit the proposed framework: for example, the implementation in the codebase does not have backtracking, which tree-of-thought has and would probably be worth having.",
        "there are a few flaws including not reporting any confidence measures, implementation details like temperature and prompt content, and using GPT4 as a judge which may not be reliable. but overall the baselines and experiment setup make sense.",
        "The evaluation criteria is pretty clear and listed. The paper also listed the what models are being evaluated. But the paper didn't really explain too much about how are the other model runs. How many examples are used and what are the temperature and so on.",
        "The experiment setup is reasonable, but it would be better if the author could provide some verification of the quality of the model-based evaluation and show the diversity of the prompt.",
        "The methods are reasonable but there's room for improvement. It covers SOTA LLMs but I feel like the paper does not consider enough negation types. It'd be better if the authors can include other evaluation metrics or more analyses in Sec 4, beyond just the scores from LLM evaluators.",
        "- The paper describes a fairly comprehensive experiment setting. The paper covers a diverse set of models and model families, as well as a detailed description of how the datasets are created and specific number of test cases.  - However, some details for the evaluation metrics might be missing. What is the specific unit of the three metrics? (I would assume it is percentage but it would be helpful to explicitly mention it.) The \u201cOverall\u201d metric is vague. Not clear how the overall quality is measured and how it is different from combining \u201cSubject\u201d and \u201cNegation\u201d. Also, The results for the \u201cSubject\u201d metric is missing.",
        "The setup for prompting seems comprehensive and sound. However, the evaluation only uses LLMs as judge without manual examination of the model outputs. Since this work aims to address the cultural/social gaps in LLM dialogue generation, it would be ideal to recruit people native to each culture for rigorous evaluation.",
        "The methodology is generally correct and the experiments are reasonable though I think it is missing a discussion of the extensive literature that discusses persona biases and how role-play consistently amplifies the risk of biased outputs (https://arxiv.org/html/2409.13979v2#:~:text=Role%2Dplay%20in%20large%20language,risks%20associated%20with%20this%20technique). Also aside from prompting, there was no exploration of finetuning or any other methods. Furthermore, while using win-rate is a widely used metric, the authors used a LLM as judge for this task and it has been shown that this judgement can be confounded by length and other factors. The authors made no effort to length control, use human annotators, or give more details on their evaluation metric. ",
        "This paper does use some standard methods to evaluate the proposed approach (LLM-as-judge). However, it is very important to have human evaluations, which is missing from the current paper.",
        "The evaluation setting is too simple: the work just evaluated the win rate of generated dialogue (conditioned on some social traits) over dialogue generated without any conditions. It is expected to have higher win rate but the implications are not discussed (for eg, can the dialogue be used to finetune a more powerful model?). Also, the work uses LLM as a judge without validating the correlation between LLM judgment with human judgment.",
        "All experiments that were presented, were sound. But some critical ablations and explorations were lacking. The primary is an exploration of how changing the number of subtasks in the decomposition step affects performance. As shown in the failure case figure, not all mathematical reasoning tasks are conducive to subtask decomposition. Some problems may require more subtasks than others. I would also have liked to know how large each dataset is and see performance metrics for each of these distributions.",
        "The method being \"self-improving\" seems to be an over claim. In section 2, M seems to be a \"static memory\". Seems like M is only increased in the memory construction phase. Although the paper briefly mentioned in line 212 that maximum 500 instances are in the memory bank, there is no related discussions on how the memory is improving or outdated memory being discarded.",
        "The chosen datasets and models are quite old and thus do not provide exciting results that show the benefits of the proposed methods. GSM8K and MATH are saturated already so there are not much gains to be shown. The evaluation settings are also confusing, it doesn't really make sense to use pairwise comparisons for verifiable domains like math problems. I also think there should be more baselines, like using ICL examples and directly retrieving the ICL examples, etc..",
        "1) a method to subtask decomposition (Section 2.1) is unclear, just feeding to GPT-4 is not strong enough. 2) Baselines (Section 3) should include other prior works with decomposed methods or with different memory structure, also ablation study. This to show comparison for both components of this work.",
        "The methodological details in Section 2 are technically correct, although personally I'd like to see more descriptions and some examples on subtask soloving. The datasets, baselines, and evaluation matrices are all reasonable.",
        "The methods seem reasonable, but the reason for my low score is the relative lack of comparison to existing methods in system prompting for safety.  Several papers that I linked above also intend to design system prompts for safety, however they were not systematically explored in this work.  There are several other algorithms and prompts from the literature so it's difficult to truly assess if PRD is better than those.  For the cost though PRD is certainly quite effective against all the ablations, and reasonable ablations were tested for the method.",
        "Method and experiments are sound",
        "Overall I think this paper is reasonably sound. The evaluation looks solid and the proposed method seems reasonable. ",
        "Sufficient ablation study is included in the paper. The paper tested a good amount of language models. Points to be improved: More datasets other than HH benchmark could be used for evaluation.",
        "I like that the authors conducted experiments with a great variety of models, approaches, jailbreaking topics, and the number of prompts. I appreciate the very thorough evaluation. One thing that would be interesting to include is additional analysis for model temperature and comparisons against existing defense approaches, such as the Bergeron method. Additionally, it is unclear how the author generated the questions for MSJ attack, and more details on this can help strengthen the paper (I was able to find this in the codebase, but more details about this in the paper would be helpful!). ",
        "Some of the choices of models for evaluation is a bit suspect (entailment model for classification task?) but overall it seems decently thought out.",
        "The paper is technically sound and provides detailed description of all of the component. Given the nature of the task, the author has done well on discussing the significance and limitation of EQ and regard metric in section 3.5. However, - the data curation process of the Personae dataset requires clarification. How are the quires selected from the mentioned sourced? What are the topics included in the dataset? How is the distribution of the demographic information ensured? - Lacks ablation study on the effectiveness of each of the component. ",
        "The reasons for low soundness are the same that are noted above.  Namely, the dataset is created specifically for this task with no description of quality control, filtering, or description of if it was human or machine generated.  The metrics are never validated with human judgments so it is difficult to verify the quality of the measures.  And besides this there is no measure of stereotyping, which seems (1) to be one of the things the authors are attempting to resolve with this method, and (2) to be one of the major risk factors of this method.  Considering we are giving the model demographic information there is a huge risk for stereotyping in the model which has already been well documented in the prior literature.  It would be important to benchmark stereotyping in this evaluation as well.",
        "One of the baselines that this paper compares to is \"diversity-aware prompting\" from Weidinger et al. However (1) the citation for this paper is correct and (2) looking into this work, they do not propose such prompting methods. Also, I cannot find reference to the \"Regard Score\" in the StereoSet paper, and the metric provided in the paper seems ill-formed. For example, the contribution arising from the probability of neutral will always be 0 since you are scaling it by 0 in the formulation. Finally, there is little information on how the testbed dataset Personae Dataset was generated, and for a synthetic dataset the testbed seems quite small. ",
        "The project is overall well executed. However, the details regarding why certain experiments are justified and run, as well as choices for models and evaluations, are not as well justified. For instance, while it is justified with prior work the choice of evaluating metaphors across dimensions, it is unclear how these dimensions are chosen as the cited papers do not refer to anything in literature regarding the specific qualities of metaphors. As a result, the evaluation is rather ungrounded with literature in the literary world. In another setting, the experiment is claimed to investigate how MCT scales with model capacity, but this experimental setup is not justified (and really just thrown into the paper without any citations), along with the choice of models also not justified. Since the paper is using LLMs-as-a-judge, there also needs to be justification why GPT-4o is chosen, especially since prior work with LLMs-as-a-judge show varying differences in performance when certain model types are chosen.",
        "A couple of flaws in methodology soundness: - There is no rigorous statistical testing showing whether the difference is significant (Figure 6, Table 2). The causal analysis in the paper also is not very rigorous. - Analysis also not insightful enough. For example, why is the gap between CoT and MCT smaller in some categories compared to others? More in-depth analysis on which aspects of the metaphors help with mathematical reasoning would be good to have - there\u2019s also no ablations to support the author\u2019s claims. - LLM judge reliability? From the qualitative examples, it doesn\u2019t seem like the metaphors are highly relevant?",
        "The paper introduces \"metaphor\" as a central concept but fails to provide a clear definition or illustrative case study, leaving its meaning ambiguous. No definition was included in the input for prompting LLM either. Furthermore, while an LLM is used to evaluate the metaphor along dimensions such as \"Relevance,\" \"Originality,\" \"Applicability,\" \"Logical Fit,\" and \"Criticality,\" the authors do not discuss the reliability of the LLM in assessing these criteria, nor do they specify which LLM was employed as the judge. ",
        "The experiments and ablations are enough to show what might contribute to the effectiveness of the proposed method. It also analyze different model size. It will be great to show if this is consistent across different model families.",
        "The paper is reasonably sound. The biggest issues are around the comprehensiveness of the baselines. On claiming parameter efficient tuning, the paper should look at similarly sized adapters and the whether it is the most efficient in terms of computation units. It should also ablate on the hyperparameters to describe expressivity vs performance. Next, the lack of Autoprompts performance can be attributed to the External translation module, switching out on the module with in-model prompting would have been helpful. Last, the paper should talk about how many trigger tokens need to be learnt to scale languages. Autoprompt finds optimal prompts while this method learns extra parameters.",
        "The description of methodology and the steps to reproduce the paper sound very clear. The benchmark choice of MMLU makes sense.",
        "It is using reasonably large LM, and tested on standard benchmarks such as MMLU and compared with baselines.",
        "I found the majority of the paper sound. However, only MMLU is tested. I am curious whether the similar idea applies to some other related multilingual tasks like translation.",
        "The experiments are well-designed and compares with important baselines for multilingual capabilities of LLMs. Experiments are well-designed and claims are well-supported by experimental evidence. The only thing I have questions for is the results in Table 2. I wonder why authors didn't apply that for all the languages and just merge these results into Table 1. I also question whether the prompt used for translation might be the reason for why this baseline performs worse than Native MLLM (maybe it's not including a lot of details in the translation). ",
        "Like I mentioned, the method is simple and straightforward, but it's hard to tell whether it is really better than other stronger baselines and whether it hurt the models on other aspects. This should be improved until it can be accepted.",
        "Results are solid and cover both adversarial and benign cases. The datasets used (AdvBench, DAN) are typical and widely adopted. The evaluation is also sound \u2014 even OpenAI uses StrongReject in their system card, which lends credibility to the paper's results. The appendix provides comprehensive information including detailed results and qualitative examples.",
        "There's a notable lack of well-tuned alternative baselines, for example the perturbation-based defense paper they cite. Their baseline seems to do really poorly, and they also seem to be missing things like random masking baselines, as well as measures of how much quality degradation certain defenses lead to. ",
        "My main issue with the paper is that half of it presents NAM as a defense against jailbreaks, which makes sense with Vicuna. Then it presents a slew of evidence showing that the \"defense\" actually increases toxic behavior with GPT-4o. This in my opinion should have been the central result, and NAM should have been presented as a jailbreak attack. From that viewpoint, the experiments are quite convincing. I still have a tendency to distrust some of the results as the paper seems to rely quite a lot on anecdotal evidence via examples.",
        "The paper clearly describes the datasets, baselines, and evaluation metrics. However, more detail could be on the metrics, such as what constitutes a \"flagged hallucination.\"",
        "there are several major flaws: (1) The paper mixes intrinsic and extrinsic hallucination, but they are usually evaluated differently and belong to different tasks. (2) It is unclear why creative writing is treated as hallucination since creativity is not necessarily a flaw. (3) The paper uses ROUGE for TruthfulQA, even though the author notes ROUGE is used for summarization, which is confusing. (4) The paper should compare against prompting baselines like chain-of-verification.",
        "there is no explanation on each step of CMI-HM and I can't even tell that it is correct or not correct. There is no tools/code/models/prompts mentioned for 1) heuristic to detect hallucinations 2) how the \"mirage model\" hypothesizes factuality distorted from that hallucination 3)  how to put that hallucination text back to factuality 4) verification    Since I looked up the code and see that it's only based on LLM prompts for every step, this would definitely recurring hallucinations for sure.",
        "A lot of the key details about the experimental details are missing. For example, it does not describe the external knowledge sources in step 4 and how that is done. The factual accuracy metric claims manual evaluation by domain experts but do not discuss where the experts come from and how they grade the answers. Finally, the datasets (XSum and Creative Prompts) are not common datasets to use in this setting. ",
        "The paper describes the CONLL format incorrectly and, more concerningly, this incorrect description of the format is provided to the language model. The preds file however is in the correct CONLL format including information like the POS tag, gloss, etc. that the model was never asked to produce. It's entirely unclear to me how this could be the product of running this code.  Beyond this, it's pretty unclear what these \"rules\" are supposed to map to in practice linguistically. The example rules seem meaningless to me [e.g. \"Verb -> Noun ( nsubj )\"] as they are just maps between the POS tags with types of relationships which occured in a particular sentence.",
        "There are some baselines missing. Providing rules to the prompt is easy, but there are much more baselines to compare with other than ICL. E.g., using LLMs to generate helpful prompt.",
        "The comparison to LLM baselines during \"development\" is well-executed, and it's good that they compared over many languages. I liked that they separated development of LLM prompts from the comparison to non-LLM baselines for a more robust comparison. It would have been good to develop over more models than just GPT-4o (and then further expand this during the evaluation stage), but the paper is generally technically sound.",
        "The paper uses a substantial amount of languages from the Universal Dependencies dataset for its experiments, which is a standard dataset. The methodology presented is also sound, and detailed prompts are presented in the paper. One core deficiency is that baselines are not presented where the LLM is asked to perform dependency parsing without the generated symbolic rules. This additional experiment would have helped more directly show the benefits of the proposed approach.",
        "The dataset and the metric make sense. The metric can be extended with detailed distribution or newer metric such as MacroCE. The baseline comparison is severely flawed. There are many prompt-based method that can be compared.",
        "I think the experimental design and the method is generally sound. However, there should more baselines to compare with in the experiments, and obviously they should provide more methodology details. ",
        "- The paper lacks effective comparison with existing methods and baselines for eliciting LLMs in Sec 4. -  ",
        "There seems to be some issues with the evaluations:   1. For TruthfulQA, there are two metrics, truthfulness and informativeness, used in the benchmark for fully reflect the quality of the free-form answers; the paper reports accuracy, which is unclear whether this is only truthfulness or it is an average of the two metrics. If it is the former, it would be a bit concerning because a truthful but uninformative answer (e.g. \"I don't know\") can naturally have a very low confidence, which might have conflicts with the calibration error metric.  2. For the ensemble method, the variance should be captured by measuring the semantic differences of the generated answers rather than the the lexical differences (which is implemented in the codebase). For example, the model can generate paraphrases of the same answer for multiple times, but this scenario would be considered a large variance in the current implementation.",
        "The methodology itself is sound, but the experiment designs seem quite flawed. As discussed above, results seem suggest too much experimental noise exists in the final results, and not actually meaningful differences. More ablations need to be conducted to effectively interpret the results.",
        "If multiply confidence of each step as the final confidence, then response with more steps will naturally be very unconfident, so it is not sound",
        "Only one baseline is included; the paper would ideally compare to all past methods whenever possible. Since there are very few baselines, models, and datasets considered, it's difficult to draw general conclusions about the effectiveness of the method. Other than this, the methodology + choice of dataset/baseline seems reasonable.",
        "In section 2.2, the authors assume that the confidence at each CoT step is independent. This is not a reasonable assumption to make. Due to the lack of baseline comparison and variability across different methods, it's hard to reliably draw conclusions based on the observed performance.",
        "The method and experimental design align with the logics of the results and analyses.   There are some missing details, such as:  * In Section 4.2 (2nd paragraph), the statement \"The results for Mistral-7B demonstrate that regardless of the few-shot examples used, the model consistently rejected all candidates across all racial groups...\" is not supported by any tables or figures.  Additionally, the paper could benefit from a prompting workflow figure that better illustrates the method for both tasks.",
        "In the hiring task, they use 5 types of first names (Asian, Hispanic, Black, White, other) and for the romantic task they use 8 categories. They explored Zeroshot prompting and few shot prompting so overall I thought they were reasonably sound in their method. ",
        "I think most of the arguments are supported by the experimental results. But still, there\u2019s a fundamental question after I read the prompt in the appendix: the prompt for the email setting only gives positive demonstration examples about deciding to hire the person, and there\u2019s no negative demonstration, which may seriously cause bias in the model\u2019s response, especially in smaller-scale models. But even under this prompt, the paper still reports that the Mistral model gives negative decisions to all the few-shot prompting examples, which makes the results weird, as all the demonstration prompts are positive decisions. In addition, no table is provided for the all-zero result.  There are also some minor problems, such as the example used to support the claim in Sec 4.1 seems inappropriate, some data in different tables are actually the same, causing redundancy, and multiple identical claims are presented repeatedly in Sec 4.2 and 4.3. They are all supported by the table, but some explanations do not focus on the most representative numerical values.",
        "Despite the shortcomings mentioned above, the narrowly-scoped project is generally executed well. A few concerns or open questions\u2014how does the balance of acceptances/rejects in the job setting affect the outputs? Why only give a single race of few-shot examples in the job acceptance experiment? The zero-shot baseline seems appropriate; it would be interesting to compare to a human baseline, but this is likely beyond the scope. The metrics seem appropriate.",
        "The paper is reasonably sound because it conducts a somewhat thorough evaluation of the proposed technique with multiple large language models across multiple datasets. It also conducts ablations on the proposed method and showcases the importance of each step. ",
        "There should be an evaluation besides a single model setting. Also, in the paper, it is not entirely clear from the included tables how we would measure success for each step (besides just running ablations and reporting accuracy). Perhaps a measure of diversity or coverage of the adversarial imagination step for example would be helpful. Also, it would be interesting to analyze how each step depends on each other, and their general relationship. In other words, which step is the most important/crucial? Does it depend on the setting?",
        "(1) minor: ACTI relies on the dataset-specific adversarial generators, but all the listed out ways to craft out adversarial examples seem to be human labor rather than automatically suggested by LMs. (2) minor: the reason to include robust accuracy and the explanation of robust accuracy is not clear enough. In my understanding, base acc is baseline model without ACTI on the original set of examples, ACTI acc is base+ACTI measured on the original set of examples, yet robust accuracy is measured on the perturbed examples, but for base models. I don't think ACTI acc and robust acc is directly comparable. (3) scalability: without the clear disclose on model sizing, I don't think we can say anything about whether the improvement (GPT-4 vs GPT-3.5) is due to model scale.  (4) The baseline seems to be direct prompting? Other methods should be included, too.    Meanwhile, the computation efficiency section is relatively well done, although some details (whether this is across all datasets) will make it better. The additional ablation on helpfulness of each module is helpful.",
        "The methodology is reasonably sound, and it's good that the effects seen from the ablations are consistent over a variety of datasets. It would have been better if they'd tested on more models, rather than just GPT-4 and GPT-3.5-turbo (section 4.2). I also don't think their baselines were a fair comparison - ACTI closely resembles self-critique, and it's unclear if the gains were simply due to having more tokens (Table 2), or a genuine improvement in how those tokens were used.",
        "Although writing is not the main focus of this round of annotation. This paper is notably more well-written than other papers in the batch despite some minor organization issues. This paper is close to an acceptable paper. Some points to improve: (1) data coverage and diversity; (2) more baselines, especially from previous work; (3) more literature discussion; (4) reporting details about evaluation, e.g., the deployed BERT, inter-annotator agreement \u2026 ",
        "The automated evalution metrics proposed do not necessarily evaluate the task at hand. Additionally, the work uses a single prompt which typically harms the model. To improve the performance, the work should try to implement the four stages step by step as different prompts.",
        "This paper was missing comparisons to some pretty reasonable baselines (e.g., prior work https://arxiv.org/pdf/2405.10431, https://arxiv.org/pdf/2402.01981). The dataset consisted of about 2000 questions which was reasonable however it is not a standard dataset so it is hard to compare the results to other approaches.",
        "The methodological details are technically correct. The automatic evaluation with BERT and human evaluation are good. I do not find the evaluation on diversity very useful, as it is not clear to me whether having higher or lower diversity is better for this task. Additionally, the sample sizes seem small in the experiments.",
        "The only concern is this method depends on model's multilingual capabilities, if a model is not good at some language, it may negatively affect the method. Also, the experiments do not include a baseline to compare.",
        "Methodology and evaluation are well-defined and documented.",
        "I do think the paper is overall sound with a new method and evaluations on a range of models. But stronger baselines would be needed to further improve the paper. ",
        "The confidence calibration equation is pretty arbitrary and I don't find a good or rigorous justification about the 1.5 multiplier (even with the explicit assumption I would ask why not 1.2, 2, or 10?) and this 0.8 cosine similarity. I feel very confused about the method and unsure any practicality that the method actually has except for any conclusions they yield from the controlled experiments with oracle answers.  Also, the comparison between Table 1 and Table 2 should be made model-specific instead e.g., taking qwen2.5 7b, how are different levels of confidence cutoff yield effective accuracies compared to the baseline setting under the **same** model. The claims made in the captions are simply comparing apples to oranges. For example in the \"Ben\" language they compare the performance of gemma 2 27b under low confidence cutoff with the qwen 2.5 7b baseline, which completely makes no sense (they should've compared it to the gemma 2 27b baseline).",
        "While the pipeline is well explained, there are certain issues with the paper, such as the lack of baselines, and the lack of justification in \"magic numbers\" (e.g. choosing cutoffs that are not clear as to why). For instance, for the baseline, there are a number of baselines possible (e.g. https://arxiv.org/abs/2406.15948, and a number of the baselines listed in this paper) as opposed to only showcasing the direct baseline. Furthermore, the confidence calibration is questionable, since the weightage is multiplied by 1.5 (not justified) and the cutoff that is chosen for this multiplication is arbitrarily chosen (> 0.8). Also, in the evaluation, a cosine similarity cutoff of 0.85 is chosen, but this value could have simply been gamed for each model (but we would not know that).",
        "There's not much soundness issue because the method is just very simple. However, the paper does need to talk about how MI is evaluated.",
        "The baselines make sense and it seems easy the replicate the results. However there are missing details such as how to calculate maintainability index.",
        "The lack of discussion on evaluation metrics and how exactly they were computed, made the findings difficult to trust. There is very little transparency on the experiments themselves, such as potential limitations, failure cases, and model configurations. There were claims in the conclusion that the experimental findings do not support. The authors claimed that the approach improved \"coherence\", which was wholly unsupported by any of the metrics.  ",
        "The methodological details are sound and the implementation appears correct. The baselines using Standard prompting, CoT, and Fixed Chunk are reasonable. The work also uses a well known dataset, LiveCodeBench, with reasonable metrics. The work does lack comparison to more relevant baselines such as Jiang et al. (2023).",
        "The selected dataset is recent, interesting, and worth reading. Yet one dataset seems not enough to show the effectiveness. To improve, the authors can potentially adapt existing papers on knowledge conflicts, e.g., [1][2]. The metric selection is sound. But there is not enough analysis on the proposed method to make the paper publishable, e.g., ablation or the effect of including incredible evidence (is the performance resulted from credibility or just removing conflicting ones).  [1] https://arxiv.org/abs/2305.13300 [2] https://arxiv.org/pdf/2309.08594 ",
        "The evaluation metric might be a little buggy in my opinion. They should use better metric like ECE to demonstrate better calibration which I know it's a little hard since the ground truth calibrated score seems to be not clear in this case. However, this is a work of ensemble, which will naturally decrease the variance of the resulted score. Two of the metrics (KL div between a neutral distribution and entropy) can be just increased by lowering the variance in the 2:2 case. We can see that in the case of 4:0 case, the number also gets nearer to a neutral distribution. It is also possible that the method doesn't improve calibration but just lower the variance. ",
        "The paper has done a holistic evaluation of the methods over a variety of models. However, many presentation details are confusing: - How to interpret the results in Table 1 given there's no golden label? How is true/false prob in any sense meaningful? - What does the \"\"ratio\" column mean? - lacks detailed analysis on the weights assigned to the evidence? Is it prioritizing the important evidence?",
        "I think the design of the experiment makes sense (varying ratios from ConflictQA). The metrics also make sense.",
        "There's a notable lack of ablations and motivation given the complexity of these guys' approach (e.g. fitness function, random combination, etc.). This makes it very hard to understand what matters here. It's also hard to gauge how much of this is related to inference compute vs other things. ",
        "Section 2.3 lacks justifications why the coefficients of 0.2, 0,4, 0.4 are used and how the fitness functions are chosen. Overall, I think the paper lacks explanations of how confidence matters in the given context. Because the paper makes certain implementations to find \"appropriate balance between confidence and accuracy\" but it is unclear why confidence is something of comparable importance to accuracy. Moreover, in the entire method section, there has zero citations to previous papers and no related work was found as well. This made me harder to judge the rigor and look for previous papers that explained the relationship between confidence and accuracy.",
        "In section 2.2, the prompt template structure was not clear when discussing Instruction component and Format Component. I'm not sure how these two are differrent. Also, for section 2.3 Fitness Function, the motivation behind two function for QA tasks and multiple-choice tasks are unclear, e.g. why do we need each term specifically (f_acc and f_multi and so on) to ensure that EGPM encourages high entropy for unanswerable questions and low entropy for answerable questions.",
        "The paper is still missing several key experiments and baselines that I would have expected to see as a reviewer. In terms of the baselines, the paper should have picked some other methods that also uses training to decide on the prompt to make the comparison fair. Furthermore, it should also have done additional ablations on its fitness functions to justify their choices.",
        "The paper is well-organized and very close to a conference submission.  Good combination of datasets, models, and baselines are compared. Minor issues: further literature and motivation to the designs in Section 2.",
        "I really liked the Figure 1 - it is simple and described the contribution for what it is. Section 2 felt really well described. The section walks through each component / model responsibility, and the way it was presented I'm actually a big fan of - I like that things were not overly complicated. The benchmarks that were used are very current, and what I would expect an unlearning method to evaluate on. I found Table 2 to be a bit ad hoc, as I think \"Flagged Ratio\" as a metric is not that standard and method specific, so it should be better explained in the caption.",
        "The paper is reasonably sound with most of the implementation details reported. However, - The \"flagged ratio\" reported in table 2 is not properly defined and unclear how exactly is it measured in the pipeline - Ablation studies on the effectiveness of different modules of the pipeline has not been done.",
        "Couldn\u2019t see a major flaw in the methodology. Authors compare against baselines, test their method with different base models and perform some ablation experiments showing that optimizing DSPy makes a difference on the results. I just didn't give a higher score as I missed a stronger baseline from the literature to understand how difficult this task is right now.",
        "The whole purpose of using LLMs to do translation is that they can take large context and summarize it. However, this work goes against it. I would assume you might do better if you generate a sentence, then do word-level translation and then ask the LLM if it wants to make more changes.",
        "I don't think that the dataset enables the perfect exploration of this research question. The authors write about resolving issues where the wrong choice or a less perfect choice is selected for translation.  I feel we don't have guarantees that the TED dataset has the highest quality of lexical choices since it itself is a translation where the human translator had to make specific decisions.  A text with more specific justification for why it would encourage lexical diversity or more specific word choice could help.  Otherwise the metrics are all standard and the methods seem reasonable, though the authors point out that json parsing failures may skew their results.",
        "The paper is missing detailed descriptions of prompts, the evaluation is too generic and orthogonal to claims they're trying to make (general MT vs specific lexical ambiguity). COMET is slightly more recent though both it and BLEU are known to not provide much signal at small effect sizes. There's a large reliance on close-source models which aren't reproducible. The TED eval set may not be commonly used.",
        "The experiments are generally sound. However, more ablation study might be helpful.",
        "The general experiment flow is complete. However, one major limitation lies in the model choice. It only experimented one LLM (gpt4o) and reported the result. The result may not be generalizable enough.",
        "I have serious concerns about various parts of the method: (1) First, let's look at the offline part of the \"method\" which involves having an offline set of questions q1, ..., qN, with responses (say) r11, ..., r1M, ..., rNM. The model is asked for (A) its own assessment of its confidence cij for responses rij, as well as (B) answers to questions of the form \"Are you more confident in answering qi or qj?\". The paper says that the latter is used to construct a graph with nodes 1,...,N where the directed edge i -> j has weight max(0, ci - cj). From the name \"contrastive\" one would expect that these are obtained from (B), but the questions (B) never ask for actual numerical differences in confidence. If it is (A), there is no description of how we go from {rij} to ci. Even more importantly, this scheme assigns a higher weight to edges that connect questions whose confidences are *father apart* -- when fed to Node2Vec as the paper suggests, this would push \"similar confidence\" questions *apart*, not closer. It is also unclear why the edges are directed---no justification is given for this baffling design decision. (2) The paper says that optionally one also adds edges based on semantic similarity between questions. But then are these edges directed? How does one reconcile that these scores are higher for more similar questions but the metric of (1) is higher for questions with dissimilar confidence scores? (3) The paper claims that various versions of each prompt (\"mild\", \"hard\", etc.) were generated for each prompt. There are no details of how these prompts were generated, or how the different versions are used. (4) As for inference: a new question q' is embedded using SentenceBERT which is used to find the most similar questions q in our set. We take an average of the node embeddings (in the above graph) of these questions which produces, say, d. Then, we subtract from the model's stated confidence a value that decays exponentially as the distance from d to the nearest embedding in the graph. I find this quite baffling: wouldn't the whole point of doing this embedding be to find similar questions in the \"confidence space\" and to smooth out the distance to their confidences? Why aren't we using the confidence associated with the questions q at all, and instead penalizing the distance to the nearest seen question? (5) The choice of the model for generation of the different variants is Gemini-1.5-Flash, which is a fairly old model. No justification is provided for this.",
        "This method was bizarre. Although the experimental results look like this sometimes works, in principle this seems to me to be doing the wrong thing, so I have a very hard time trusting the results. Some questions: 1) When picking points in the calibration set to embed into the graph vectors, why do we take the confidence of the predicted answer instead of the confidence of the correct answer? We later treat all of these points as \"trusted\" regions of the graph --- we shouldn't if we were confidently incorrect...e.g. suppose we have a test point that is close to a confidently incorrect training point. Then shouldn't we reduce our confidence, not keep it the same? 2) What was the point of the contrastive question generation? Data augmentation? Were we trying to keep the answer the same despite the augmentation, or was it okay if we didn't know the ground truth answer? I have a hard time believing the domain blending idea didn't change the answers of the questions. 3) If the graph space represents some hybrid text similarity / confidence space, why are we projecting test points onto this space solely through text similarity? Shouldn't we in principle also elicit a test confidence & use this information? 4) Why are we looking at test calibration results for the 3 settings of hyperparameter \\alpha, rather than selecting this on the calibration set using k-fold validation? This is unfair to the baselines. DCM also seems sensitive to alpha: the 1000 case fails to beat baselines. 5) I am not completely sure that the calibration graphs really suggest DCM works. e.g. Figures 2-3 suggest the DCM calibrations are pretty unreliable: 50% confidence = 0 accuracy?",
        "The methods in the paper are reasonably designed and executed, including: constructing contrastive pairs, constructing the graph based on relative and absolute confidences, and calibrating confidence for a new query.",
        "As stated in the novelty part, the Self-Debug framework enabled the iterative edition of the patch with the produced regression test, while many iterative methods are there. Thus, the baseline should be at least include previous iterative methods but not the heuristic non-iterative version of the proposed method. ",
        "The experiments seem reasonable, but ablations / baselines are noticeably missing. For example, what about other agents that people have proposed? What about other retrieval preprocessing methods? I'm also unsure exactly how good the QA generation is for the self-debugging: how often are test cases correct?",
        "In principle, I agree that evaluation on SWE-Bench alone suffices to justify the idea. However, the evaluation methodology proposed here has serious flaws: (1) Evaluation on SWE-Bench lite instead of verified which is the standard. In addition, no other benchmarks are used (e.g., HumanEval) (2) One of the models evaluated is DeepSeek-Coder-V2, which is not a frontier model and therefore is not a meaningful evaluation (this would be less of an issue in another domain, but specifically on SWE-* benchmarks even the frontier models show poor performance, so it does not make too much sense to evaluate other models unless one sees incredible improvements) (3) One should really evaluate against agentic frameworks like SWE-Agent or OpenHands which are the only systems showing nontrivial performance on SWE-Bench. Evaluating against the vanilla models with different prompting schemes is mostly meaningless due to their very poor performance. The right framing of this paper involves LM feedback as an addition to a larger agentic framework that is evaluated against the original version of the framework.",
        "The paper evaluates on SWEBench-Lite and uses two different models, which adequately shows the effectiveness of the method. It compares against three baselines: 1) Standard, 2) Standard + Example in Prompt, and 3) Standard + Example in Prompt + File Content Determined by Package and Symbol Graph.  (1) & (2) are standard prompting techniques, while (3) serves as an informative ablation showing that the \u201cFile Content Determined by Package and Symbol Graph\u201d step of the paper\u2019s methodology has a large improvement by itself. Comparisons to prior work on SWEBench-Lite are notably missing.",
        "The idea is clear, and the execution is clear. SWEBench-Lite is a common dataset used in this task. But one improvement is to compare this with other frameworks other than just BM25 and might be a good idea to evaluate on other splits of SWEBench",
        "The general experiment settings are sound, but some details are missing. For example, the dataset used for evaluation is not specified, and it\u2019s unclear whether FactScore refers to test data or metrics. Additionally, the prompt is not provided anywhere in the paper, making it difficult to verify the results. The prompting settings should also be more clearly articulated, such as whether demonstrations were used or not. All of these factors could affect the results. While the data referred to is generally correct, it yields negative results.",
        "The paper provides no justification whatsoever 1. why asking a separately tuned model directly for source would help it become more factual and 2. how encouragement in Response Revision is being performed and why it would benefit the model. These omissions seriously undermine the soundness of the paper.",
        "The methodology is very short and vague. It also does not make sense at a high-level since finding source of facts without web search or deep research is purely trust on LLM to not be double hallucination.",
        "I think the experimental settings are more or less correct--it could use some additional datasets beyond TruthfulQA and FactScore (such as VeriScore or other QA datasets). However, the bigger flaw is in the baselines, it only tests direct prompting and CoT. It is missing several other important and relevant works such as chain-of-verification (Dhuliawala et al., 2023). The model it uses are also out of date. "
    ],
    "effectiveness_score": [
        1,
        4,
        5,
        1,
        5,
        4,
        6,
        3,
        5,
        5,
        5,
        3,
        5,
        3,
        1,
        5,
        4,
        1,
        5,
        5,
        5,
        3,
        6,
        3,
        5,
        3,
        1,
        1,
        2,
        6,
        5,
        5,
        3,
        2,
        3,
        3,
        3,
        1,
        6,
        5,
        5,
        6,
        7,
        7,
        4,
        3,
        5,
        5,
        6,
        8,
        6,
        1,
        7,
        2,
        1,
        5,
        7,
        6,
        3,
        6,
        6,
        5,
        6,
        6,
        8,
        6,
        5,
        6,
        5,
        5,
        5,
        5,
        6,
        4,
        5,
        4,
        8,
        8,
        5,
        4,
        2,
        3,
        2,
        5,
        6,
        6,
        8,
        8,
        3,
        3,
        2,
        3,
        2,
        7,
        8,
        6,
        7,
        8,
        6,
        8,
        5,
        4,
        3,
        4,
        5,
        1,
        7,
        8,
        7,
        8,
        9,
        6,
        6,
        6,
        5,
        3,
        5,
        2,
        4,
        1,
        5,
        5,
        1,
        5,
        4,
        4,
        3,
        5,
        7,
        4,
        5,
        1,
        3,
        1,
        2,
        6,
        6,
        5,
        6,
        6,
        1,
        5,
        4,
        8,
        6,
        5,
        5,
        5,
        1,
        1,
        5,
        2,
        5,
        4,
        3,
        6,
        5,
        4,
        5,
        5,
        6,
        8,
        6,
        8,
        1,
        3,
        3,
        2,
        2,
        2,
        3,
        2,
        5,
        2,
        6,
        8,
        6,
        1,
        1,
        1,
        1
    ],
    "effectiveness_rationale": [
        "The paper does not present any significant baseline for comparison besides the original output from the LLM prompt injection, and variations in the threshold for sentence retrieval. Adding noisy data seems to make the models more safe at a cost of fidelity of responses (approximately 3% drop of fidelite from alpha=0.5 to alpha=0.1 in Table 3), but it is unclear how much worse the answer gets. Would this low alpha injection perform similarly to a completely random baseline (i.e. injecting completely random sentences not anchored in the original text)?",
        "The method is reasonably effective when compared to prompting. However, because the scale is 1-5, having a few tenths difference is not comparable (e.g. 4.05 -> 4.22) especially when this scale has not been used in prior work. A better metric would be whether an attack with the prompt is successful on a specific model (so a semblance of \"accuracy\" as opposed to a less granular measure of scoring on a scale from 1-5).",
        "On some models like Qwen the safety performance got worse after introducing SFI. Increase in model safety for other models is not substantial. The ablation study with varying alpha parameter does not show a clear trend. The experiment results also do not justify compromising utility for the marginal safety improvement.  There are also typos in the results Table.",
        "1. There is no comparison with existing baselines at all in the work.",
        "The paper presents mixed results: GPT-3.5-turbo is the only model where all pivots led to bias reduction over the base prompt. According to the paper, all other models had some prompts that instead increased bias on average. There was also no single pivot that was most effective for all models. Therefore, the paper's results are mixed overall. ",
        "The single prompt CPP seems to help, but everything else does not seem to do great. Also, it is concerning that for only the GPT-3.5-turbo model is it true that all pivots lead to bias reduction.",
        "The paper presents mixed results, where certain pivot prompts were great with one model, but may perform worse than another. However, there are too many instances where the improvement in demographic parity with this technique is so large, that it warrants further exploration into what is causing the inconsistency between models.",
        "The paper acknowledges that the technique is not effective, though there is some dependency on the model. The paper also discusses that it is promising but needs additional tailoring. But on its own, it is not effective and can actually make things worse.",
        "The improvement reported in the tables are very limited compared with direct prompting. The test samples are very limited, which further bring uncertainty for the proposed method. The workflow incorporates several stages. There is no ablation study. The core design, neuro-symbolic framing for API descriptions/selections, is not proved to have great results. Finally, there lacks a cost-effectiveness analysis.",
        "There are some tasks that the neurosymbolic approach performs worse than other approaches, but on average the proposed approach seems to win based on the evaluation metrics. But there is also no description for how the task-wise scores are computed!",
        "If only examining the result tables (tab. 2 and tab. 3), the method is sometimes slightly more effective than other baselines. The method has a lower compilation success rate than two baselines but achieves slightly higher task-wise scores on most tasks and outperforms most baselines overall. However, again, given the task-wise scores and specific number of examples for evaluation are not specified, it is unclear whether the results are statistically significant.",
        "From Table 3, it seems that this method does not help in terms of success rate, and from Table 2 it seems that this method helps code quality in a very marginal manner. It's also unclear why some entries are marked as N/A.",
        "I don't think this evolution is applicable to this paper given the type of the research question tends to be more analysis-oriented. The first two research questions in the paper do not aim to improve a system but attempt to probe model behavior differences for multilingual questions. ",
        "Similar papers to this theme include  1. https://arxiv.org/abs/2412.11167 - this paper combines a cross cultural dataset and generate region-specific draft responses to queries. Then an agent blends the cultures through attention-gated parameter merging to resolve conflicts while preserving cultural nuances to produce the final culturally-aligned response.  2. https://arxiv.org/abs/2310.16523 - This work formalizes diversity of representation by  leveraging the model's diversity reasoning capabilities to enhance demographic diversity in generated responses.   It's generally hard for me to tell if this method's results are better than these approaches because the dataset was different, however, based on the results I would likely infer no. ",
        "There's not really baseline and the metric used is also unclear to me! How do they measure the difference between responses? The codebase only has a script for one language but the paper evaluates multiple languages.",
        "I think when comparing to similar papers that use human validated translations for similar experiments, it is possible that the data quality, in particular question accuracy across language and the precision of how relevant questions are within each language may be degraded slightly. However this method may allow for large scaling and shows similar results to other papers.",
        "The proposed method only beats the RAG baseline by a small margin on one out of 4 metrics. Moreover, cultural depth, the metric where the proposed method wins, is not clearly defined in the paper.",
        "It is unclear. We cannot draw any judgement about the efficacy of the method based on the experimental results provided since most of the metrics used are flawed. And the traditional metrics show that the new method is worse than all the baselines.",
        "The paper uses reference-based metric (BLEU and BERTScore), as well as LLM-as-a-judge. While using LLM-as-a-judge is a common practice, it will be helpful to include human evaluation to validate whether it is reliable. Aside from that, the method also do not outperform one of the baseline for most metric (RAG Few-shot).",
        "The paper presents results that shows that CG-CoT achieves higher performances in some metrics, but is not the best in other metrics (although the authors try to argue that the metrics that CG-CoT does not do well on are flawed). It is hard to evaluate the significance in terms of the differences between CG-CoT and the baselines. First of all, the authors bold CG-CoT as the highest performing method for accuracy, but RAG Few-Shot clearly performs better. Secondly, a difference of 0.24 between CG-CoT and RAG Few-Shot for Cultural Depth does not mean anything, as the authors do not describe how these scores are calculated. ",
        "This paper focuses more on evaluation and does not end up proving whether using PBT would enhacne end-to-end accuracy. Instead, results are all intermediary for laying out the foundation for PBT generations.",
        "The paper does not provide a good evaluation framework or a reasoning method. It mostly did some testing on different LLMs.",
        "It's hard to judge the effectiveness of this work - it claims to be an extension of existing results in a field where the results are expected (LLM + PBT). There isn't a clear definition of a baseline, but since the results and analysis appear to be correct, one could say that this is reasonably effective.",
        "Judging the effectiveness of LLM generated PBT is difficult from this work. The results are a mixed bag in some sense. Since the accuracy of generated properties is not very high, it is unclear how we can trust the generated properties to test the LLM code generation effectively. Paper is missing key details required to understand the methodology.",
        "The proposed PBT shows consistently lower pass rates compared with unit tests. The paper argues that this suggests unit tests often overlook certain corner cases where PBT exposes. However, there is no evidence towards this claim. I think at least a human evaluation/analysis should be incorporated. Also, the lower pass rate may indicates a higher false negatives with PBT, which also needs careful examination and analysis.",
        "Results from the (somewhat questionable) evaluation metrics suggest that Cross-Modal Corroboration prompts do not lead to any improvement in comparison with the baselines. Counter-intuitively, using the proposed prompt technique could lead to deteriorated performance. ",
        "As even acknolwedged by the author, the results do not show any improvements and instead show worse results. I quote the authors: \"While our enhanced prompting approach improved source attribution accuracy, it underper-formed in other metrics. These findings challenge assumptions about the necessity of multimodal information in factual reasoning tasks, though further research is needed to fully understand the conditions under which multimodal approaches might be beneficial and to improve model reliability and consistency.\"",
        "The paper specifically said it's outperformed by most baseline. Having multimodal inputs harmed the performance compared to only having text input. ",
        "Sec 4 shows that the proposed method underperforms baselines in most of the case, and the t-test even shows that there exist significant gaps. Sec 5 mentions some potential improvements; however, without enough details I cannot judge whether the experiments have been done and how effective they are.",
        "ACP achieves promising results compared to baselines, even outperforming the \"full-window \"approach that requires significantly more compute for longer sequences.   Despite not being consistently the best method, ACP yields better results on some of the automatic metrics as well as LLM-eval metrics.  ",
        "As discussed above, the result does not seem to be conclusive. Compared to Full context window, the results of ACP is not better. This undercuts the efficiency argument.",
        "The proposed method does not outperform baselines",
        "As shown in Table 2 and 3, the proposed method is on par with all baselines across all settings and results in improvements that are marginal at best. Additionally, no details are provided about the overhead of the methods in terms of time and computational costs.",
        "Not really. While the proposed prompting technique reduces the number of non-supporting and irrelevant facts compared to vanilla prompting, it also reduces the number of supporting facts. Additionally, it doesn't improve the precision or recall of the model compared to vanilla prompting. ",
        "The results are not improving from the vanilla baseline that much (as seen in Table 1). The authors also suggest this in their Results section, and offer some insight into why it might be the case.",
        "The results show that this approach scores approximately 2-3 points worse than the baseline across the board. Given that it is also less computationally efficient, it is safe to say that the approach is not effective.",
        "The paper acknowledges that the results are not effective. But, the approach does seem to have many opportunities for improvement describe in the future work\u2014for example, multi-turn debates instead of a single turn. But there appears to be no improvements in the current version over the baseline.",
        "The results are dubious. Referring to table 1:  For GPT4, the SS score is ~38 in the first row. Considering LMS to be 100, ICAT should be around 76. That is, for all samples, ICAT = 2*min(SS, 100-SS). It seems like that is not the case for all results. This means that the LMS values need to be reported.   For GPT 3.5, even if I take the results at face value (assuming they are factually correct), then too the method shows a performance drop over zero shot GPT 3.5. The text claims there is no bias at ICAT = 0 but equation one seems to imply no bias means ICAT = 100.  As for table 2, a reader has no clue whether we are looking at the baseline or the treatment group.  ",
        "The results indicate that the methods is effective on extrapolating and addressing different biases across time. However, it remains uncertain whether the time bias decay plays a crucial role as the ss/iCAT across year does not show a strong correlation.",
        "It is unclear to me if the method helped. The GPT 3.5 results are very concerning where it shows the models are very biased after using the technique defined.",
        "It is unclear how this method would compare to methods proposed in prior works, especially because there is an inconsistency in metrics used. For comparative purposes using a metric like bias score would have been helpful in understanding effectiveness compared to other methods",
        "There are some advantages in using extra text for retrieval and enhancing the context.",
        "The performance is good even though these are simple prompting techniques, giving the insight that with appropriate prompting and few-shot examples, the model can adapt to perform good translations for long-context and low-resource languages. The only comment is about the evaluation metrics: using LLM-as-a-Judge might provide a better comparison instead of just n-gram level metrics.",
        "It is pretty effective given the results shown in the tables!",
        "As shown in table 1 and 2, gains obtained by the experiments in the paper are marginal at best. The proposed approach is not able to outperform simpler baselines, even with substantially more compute overhead.",
        "The approach takes significantly more tokens, but the performance gain (Figure 1 and Figure 2) is not good enough. Also baselines are not enough.",
        "The results appear to be mixed and generally week - the benefits of HMP on MCQ and Generation tasks are <5% with no information on the CI, while consuming much more tokens. This does not seem like a statistically significant result.",
        "From the first ablation study, it seems that decomposition is not effective enough, as removing this module actually leads to even higher performance when using a single prompt. The multi-expert corroboration appears to be effective, but the contribution is still incremental. Given that these two modules are both not effective enough individually, I am wondering why the method still leads to a performance gain compared with CoT, which the paper does not explain well.",
        "The proposed method obtains substantial gains over the baseline for generation tasks. However, gains are marginal for the MCQ tasks, with very high computational overheads (this is mitigated somewhat by the ablations, which demonstrates that parts of the pipeline can be removed without severely impacting the downstream performance).",
        "The bias scores (toxicity) in Table 1 clearly decrease with the incorporation of the debiasing prompts. The drop in the bias score is huge (from 40 to 10) and this is almost surely statistically significant even without a hypothesis test. ",
        "The proposed method significantly mitigates bias compared to baseline methods, reducing the rate by almost half for most languages in GPT-4o-mini. This reveals that the model itself has bias acknowledgement capabilities and is able to self-correct.",
        "It's not clear how metrics for age and religion can be measured if the prompt is about African-American son. Additionally, the toxicity metric is not defined.",
        "It seems like the bias mitigation method worked well compared to the baseline. Across all languages and for both GPT-4o / BLOOM, the bias prompting method led to significant reductions in toxicity. My only critique, here as mentioned above, is that the baseline seems like a \"straw man\" comparison. To actually get a sense of how effective the method is, I would like to have seen the technique compared to other prompting-based mitigation methods.  ",
        "The results, even assuming that the evaluation setup is ok (which it isn't), are poor, with the CLHCP doing worse than QWen Few-shot in almost all of the columns. ",
        "As mentioned in the Novelty and Excitement section, the performance even dropped compared with the baseline. I don't see the method being effective.",
        "As shown in the result table, the results are mixed. Performance also varies across domains. And it seems like baselines such as Google outperforms the method significantly.",
        "Table 1 show that the method is higher than baseline methods but lower than Google Translate, suggesting this method is somewhat effective.",
        "The method is reasonably effective against self-planning with RAG, but the iterative process doesn't help much (comparing against self-planning with ICR). It also adds trade-off between %resolved and cost. However, the proposed method has a huge performance gap between SOTA or other agent-based methods.",
        "As shown in Table 1, Chain of Context didn't appear to improve from one-shot version which is indexed context retrieval while also perform worse then other baselines in SWE-bench leader board. ",
        "The results show the proposed method has a higher accuracy than the baselines.",
        "In terms of the solve rate, the model does improve upon the existing baselines. This is especially true on the Verified subset but not the Lite subset. However, I would put more weight on the Verified subset and trust those trends more. ",
        "While Chain-of-Context significantly outperforms RAG, its advantages over single-shot context retrieval, when not decomposing the task into sub-tasks, are only marginal. It offers improved localization of code changes, but addressing more complex tasks may necessitate additional software development tools to enable accurate solution generation.",
        "The paper reports big performance gains from ASE. However, in table 2 the number of safe resoonses for ASE GPT4o exceeds the number of responses, which may be a typo. if not that means the results may not hold.",
        "The paper compared with Consitution AI and also baseline without intervention. It covers at least some basic baseline as comparison which is nice to see. However, since LLaMa is opensource, it would be nice to see a comparison with some finetuning methods.",
        "The effectiveness is shown by improvement on several datasets. ",
        "Results look good on the datasets included, and the CAI baseline is reasonable. Another baseline that would have been nice is rejection sampling with LlamaGuard. See above note about lack of \"good\" prompts",
        "The organization of the result presentation on two tasks is ab bit strange. Comparing with the simplest baselines, the method performance is fine on TruthfulQA, but not quite on SimpleQA. Self-imporvement shows good performance on truthfulness and informativeness, but not on automatic metrics. In general, the results are mixed.",
        "The method does perform better than the offered baselines. However, since section 2 was quite bare, understanding Figures 3 and 4 took me some time because many concepts such as calibrating across iterations only seemed like they were first mentioned in the experiments section.",
        "The ACGP method minimally improves truthfulness and informativeness over direct and CoT prompting (Table 1), but again, since very few models and benchmarks were tested, it is hard to say whether this result is very impressive. It also seems to be worse in every metric for SimpleQA (Table 2).",
        "Experiments show that the proposed method makes model perform better in TruthfulQA than the baselines but worse in SimpleQA. This fits into the mixed result category.",
        "The improvement is not that big, also the metric is not good enough.",
        "The results as presented by the authors are sound; but the axioms appear a bit too generic to help with the author's claims. The datasets also appear to be restricted to a subset of the files of applied ML engineering, which is of limited scope.",
        "From the experiments results provided in the paper, it could be seen that the propoesd method is effective, as the overall performance across five different codebases are improved, compared to the baseline methods. However, the improvement is kind of marginal, and actually in some sub-domains the proposed method could not consistently beat the baselines.",
        "According to the pure number, there are mixed results. Moreover, I want to point out that the metrics are kind of subjective and it's score-based LLM annotation, it is possible to be biased to some specific principle. ",
        "From my prior rationales: The paper's ToQ method shows large improvements over CoQ and CoT on MuSiQue and 2WikiMultiHopQA, but negligible improvement on StrategyQA and MedQA. Notably, a comparison with ToT performance is missing, and a comparison would be required to determine whether the ToQ method truly improves over the prior work it builds upon. ",
        "I am unconvinced by the results. While those in Table 1 look good, as mentioned in the previous response box, there are only two models (+ one is a rather old model) to trust the results. More importantly, Table 2 shows clearly that ToQ is worse than even chain-of-thought on MedQA (and at-par on StrategyQA), a much cheaper and simpler method.",
        "All relevant baselines I can think of are included, and they do better.",
        "The results show consistent improvements over baselines across multiple datasets and can be applied to different frontier models. This method not only improves semantic result but also shows great attribution result. This approach is robust across tasks and model architectures.",
        "As stated in my response to the previous questions, the results are strong for two tasks but weak for the other two tasks. The effectiveness of the method could have been greatly strengthened if the authors could include ablation studies of the different components in the pipeline in the two successful scenarios.",
        "The gains seem very small and not consistent across all settings. the differences are likely not statistically significant. The authors used an LLM evaluator and assumed it would be reliable without really testing it.",
        "I am not aware of too many other baselines that test negations. This paper is also not really producing a methodology for improving performance.",
        "The author acknowledges that the evaluated methods are not effective.",
        "Compared to the Direct baseline, the proposed method has lower scores in Table 4. Also, the original performance is around 90 in most the case, indicating the proposed method is probably unnecessary. ",
        "The method is not always more effective than the baseline. The performance varies across models and types of negation. Even if the method gains some performance, the gain is oftentimes trivial.",
        "The paper claims that there are no readily available dataset and metrics to make direct comparisons. Based on the evaluations in the paper that uses LLMs as judge, the proposed prompting technique seems quite effective compared to the setup without such a role-play prompt. ",
        "While I have some issues with not using human annotations, the results did seem to show  noticeable improvements over baselines on some datasets, although I think that maybe their method could have been a baseline and then they could have tried some more clever things for their method! ",
        "It does seem that the proposed method is better than the baseline. Howerver, the experiment set up is suspicious.",
        "It shows consistent high win rate over baseline dialogue. However, I feel it's due to the over simplification of the evaluation setting.",
        "Performance metrics were within 5 points of the baseline, performing worse for all models aside from a small improvement for LLaMA 3. When it came to preference evaluation, the baseline outperformed the presented technique by a large margin.",
        "From table 1, decomposed method (the proposed method) only wins for Llama3-70B by a margin of 0.02 and 0.01 accuracy increase. In the cross-dataset generalization results, direct accuracy also tends to be higher. Furthermore, in the preference table, almost all evaluators prefer direct answers rather than the answers from proposed method. ",
        "Across all tables (1-4), it's pretty apparent that the proposed method is significantly worse than simply directly prompting the LLM. Although sometimes it achieves comparable results  in terms of accuracy on one model, the results are overall worse. ",
        "Table 1-2 shows that Decomposed methods perform worse that directed approach when solving GSM8K and MATH. Also, LLM evaluator prefer directed approach answers than decomposed, due to clarify (Table 4). ",
        "The experimental results do not back up the claims and the main ideas. The Direct baselines have stronger performance in almost all the experiments.",
        "PRD both is quite good at preventing the model from responding to harmful queries while not preventing the model from responding to benign queries.  I think this could be related to the way that the model expresses its own values, or that it doesn't strongly steer the model into thinking it's being tricked.  Against all baselines introduced in the paper the method works quite well.  As discussed earlier there are some baselines missing from this analysis that would make the method clearly even more effective.",
        "The attack success rate is largely reduced compared to baselines",
        "The proposed method seem to be better based on the current evaluation. However, it lacks of significant test so it's hard to tell the gain of the approach. ",
        "The number presented in Table 1 and Table 2 show improvement of the proposed methods over the baselines. However, significance testing would make the claim more convincing.",
        "PRD, the authors' proposed approach, is able to consistently improve ASR across various models and jailbreak setups. It would be good to align the LLM-as-a-judge helpfulness results with some human evaluations if possible, since LLMs can suffer from self-preference biases. ",
        "Sec. 4 shows good empathy scores but worse performance on other metrics, but I don't think the other metrics are that necessary to optimise.",
        "The results is clearly effective on improving the EQ of the model on the dataset.",
        "The authors define 3 metrics: Emotional Quotient, Regard, and Perplexity.  The emotional quotient score is by far the best for the method.  It does somewhat feel like a circular metric because the emotional quotient score is based on questions like (1) \"This response acknowledges the user\u2019s emotions\", (2) \"This response demonstrates understanding of the user\u2019s perspective\", and (3) \"This response provides constructive and empathetic advice.\" which corresponds to things the pipeline was asked to do directly.  The Regard metric it scores worst with GPT3.5 of all baselines, though it is close to the best with GPT4.  With the perplexity it is always the worst which is a proxy for fluency.  I am not convinced perplexity is a necessary evaluation here, but it would be great to see some of the outputs to judge fluency for myself. ",
        "Compared to other prompting techniques, the ECN does perform better on the Empathy Quotient Score compared to the baseline and other prompting methods. On GPT-4o, I am not sure if the performance compared to the standard emotion prompting is a significant difference. However, for both regard and perplexity, we see that it performs worse the the Basic Empathy Prompt, warranting the question whether the cascading network design actually leads to meaningful improvements. ",
        "The proposed method seems to do worse than chain-of-thought (as noted in the conclusions section). For instance, MCT trails CoT by 8.6%, all while being much simpler. Although MCT does better than direct generation, most people will opt for a much simpler choice of using CoT over MCT. However, the paper does glean some interesting insights about when metaphors work and when they do not, as evidenced by some experimental results on the gap of metaphor performance and how it scales as the model size scales.",
        "- The method provides some marginal benefits to mathematical reasoning, but this is not surprising result: performance increase is likely due to additional CoT and extra tokens in generation, which should lead to performance increase regardless of metaphors. Since the metaphor analogy may not be that relevant to the problem, this could simply be wasted tokens or \u201cforced\u201d reasoning that may not be relevant. On the other hand, CoT provides logical thinking process to problem-solving. Reasoning models will likely achieve better accuracy performance with same \u201cthinking\u201d tokens.",
        "The method is more effective than direct prompting but much less effective than chain-of-thought prompting in a few scenarios, although the method is built on top of chain-of-thought prompting. ",
        "By just looking at the CoT result, the proposed method's result is not better in any case. So the low effectiveness score",
        "Table 1 shows empirical evidence on the efficacy of the method over translation/ direct prompting. While results in table 2 need ablation over external translation, the overall experiments seem promising. They are also supported by reader intuition: Adding learnable parameters for language specificity should increase performance. The real question lies in the extent of performance increase per parameter for the methodology.",
        "The PolyPrompt method clearly outperforms Native MLLM and in-model translation on MMLU for all languages by a clear margin (Table 1). It is also better than external translation plus autoprompt (Table 2).",
        "As shown in the result table, it is fairly effective!",
        "As seen in Table1, PolyPrompt has achieved consistent performance improvement across all languages. It applies to both 1 epoch and 2 epochs.",
        "Experiments justify how PolyPrompt outperforms baselines in all language settings that were tested. Different is large for particular groups of languages, thus illustrating the effectiveness of the method. Analysis in section 3.3 offers some insights to variations in effectiveness that make sense. ",
        "Based on the results presented in the paper, the method seems to be effective. However, the evaluation is not comprehensive so it is hard to tell whether it is really effective.",
        "I wouldn't necessarily compare these methods to other jailbreak techniques, some of which can cause more vulnerabilities but rely on exhaustive search. I am still really surprised that such a simple masking method could already achieve a 10% jailbreak rate for GPT-4o. The proposed method is simple yet novel. One could argue that the jailbreak rate is low, but given the simplicity of the method, I believe slightly more searching could easily lead to a surge in vulnerability rates.",
        "If we take their set of baselines as ok, then the results generally seem to be solid. The main issue is that their baselines are not well-tuned and so it's unclear how much we can trust things. ",
        "The reason for my score is that NAM seems terrible as a defense strategy (its primary stated purpose). On the other hand, I think the results are promising in that NAM could be a good jailbreak attack! The paper does acknowledge this but does not sell that story much.",
        "The hallucination rate does consistently go down and actually increases dramatically on the Xtreme Summarization benchmark. ROUGE score and relevance also stay the same or decrease across all benchmarks.",
        "In Table 1, the proposed method is weaker than direct prompt in terms of hallucination. ",
        "I conclude from Table 1 that CMI-HM, though aim to detect and self-correct hallucination, perform the worst for two out of three datasets. If directed prompts could do even better, this work is surely overcomplicates self-correct methodology ( which is also unclear here).",
        "Looking at Table, the performance of the method is relatively poor. On key axes like Hallucination and Factual and ROUGE, the results are all worse or at best equal to the baselines.",
        "If I suspend my concerns about the works experimental procedure, the method itself is entirely ineffective and is outperformed by the established small model baselines (see Table 3). This is doubly true given the methodologies from the small model baselines could be simply adapted to be used for large autoregressive LMs (e.g. the transition classifier could be replaced by a model prompted to be a classifier).",
        "The authors admit the results are mixed. However, I don't think this is particularly a disadvantage for the paper.  ",
        "The methods shown significantly outperform LLM baselines (Figure 1). However, they are still not especially competitive with SOTA neural parsers (Table 3). More analysis of the methods' scaling behavior (as the number of in-context examples grows) would be needed to show that this method has the potential to outperform SOTA.",
        "Unfortunately, the methodology presented in the paper is substantially outperformed by existing approaches, including those from as far back as 2019 (mBERT). Therefore, the method is not computationally feasible to use, using substantially higher compute than older methods without any performance gains. ",
        "Given the current baselines, the proposed method works better on low dimension scenarios, but the improvement is marginal on high-confidence cases. It is hard to interpret the effectiveness from current results since it is easy to imagine that the alternative perspectives in general lower the over confidence. A simple baseline can be lowering the confidence when the model confidence is not 95+, which can show similar curves. Without removing this confounder, the usefulness of the alternative perspectives remains questionable.",
        "From the paper itself, it should be 5 scores as they provide mixed results (they acknowledged that they doesn't show consistent gains). However, since I think this paper obviously lack of baselines to compare with so I deduct one score based on this. ",
        "Table 1 presents a mixed results of the methods on QA datasets. It's hard to judge whether the methods is better than direct prompting based on the AUC curve and ECE.",
        "As the author mentioned, the results are mixed for the two datasets. Even for TruthfulQA where the proposed method shows a better AUC, there exists an issue with the evaluation metric (as mentioned in the response to the previous question), so it is difficult to determine the effectiveness of the method on it.",
        "Results on GPT-4o shows improvement over baseline while results on the other two models do not. Lack of further experiments or ablations suggest that possibly too much noise exists in the results.",
        "Based on the experiment results, it's better than the simple baseline",
        "The method shows improvement for GPT-4o when compared to baselines, marginal improvement (no error bars are given in Table 1) for GPT-3.5-Turbo, and no improvement for Llama.",
        "As demonstrated in Table 1, the method's improvement on ECE is model-dependent. On the llama model it increases ECE and decreases accuracy, showing that the method is not effective. The method may also hurt accuracy performance across models. Overall, it does not seem to be an effective method and could potentially hurt utility. ",
        "There\u2019s no comparison to bias-reduction baselines. The paper compares only few-shot prompting (with different names) against zero-shot inference. ",
        "The paper explores few-shot prompting as a bias mitigation strategy and in the hiring task and romantic relationship prediction task. In the hiring task, they find evidence of over-refusal in Mistral and varying acceptance rates in Llama 3. In the romantic relationship prediction task, they found that few shot prompting may reduce overall performance without meaning-fully addressing disparities. Overall the method does not seem very effective. ",
        "I think this paper is more about analysis, as no new method is proposed. For the demonstration learning (prompting) method that is investigated, it does not exhibit better performance than zero-shot prompting, and the results seem arbitrary without a clear trend across all settings. This makes the prompting ineffective.",
        "As the paper itself reports, the method is generally unsuccessful. This is why it would have been interesting to see more complex prompting techniques (e.g., CoT, reasoning, etc.) or a more extensive analysis of *why* the models are affected in this way\u2014e.g., some type of ablation or something.",
        "The authors show in Table 1 and Figure 1 that the proposed prompting technique brings performance gains across two models and 6 different evaluation datasets. Additionally, it also shows via ablation experiments in table 3 that different components of the prompting technique contributes to the gains. ",
        "It's great to see how this method does help improve accuracy, but it would be fruitful to also include other existing robustness techniques that can help boost accuracy. Or, if the author(s) feel like such a technique does not exist, there could be some more discussion of how far off existing techniques are from that and how they can be modified/augmented to be able to do so.",
        "ACTI has shown some improvements on baseline methods, but the paper lack of baseline selection details and comparison against other methods. + computational overhead. ",
        "Figure 1 shows a significant gain from including most individual components of ACTI, suggesting its overall utility. This is relatively consistent over a variety of datasets, but only over two models. ",
        "The evaluation is good: diversity, auto bias detection, and human evaluation. The authors show good evaluation, and the proposed method looks good across metrics. The main problem is the data: there is not enough analysis, e.g., categorical statistics and examples, for the proposed data. Also, the link to previous work is weak, which limits the effectiveness.",
        "The results show that the approach doesn't work at all. It beats some baselines in human evaluation but fails miserably in automated methods.",
        "This paper's method was better than the baselines it considered (direct prompting and prompting the model to be unbiased), but it was missing comparisons to some pretty reasonable baselines (e.g., prior work https://arxiv.org/pdf/2405.10431, https://arxiv.org/pdf/2402.01981). ",
        "In 5.2, Table 2 shows mixed results with automatic evaluation. In 5.3, Human evaluator also favored the Disclaimer Prompting baseline more than the proposed method.",
        "The experimental results look promising",
        "The method works on some models, but not all.",
        "This paper does compare with a baseline but it is very very naive. It's just naively prompting the model. However, there are so many other baselines in this direction (e.g. prompt the model to output a confidence score). ",
        "There are positive results for some languages, and negative results i.e., the proposed method is worse than the baseline for other languages. Also, because the comparisons are invalid (see my second point in soundness), I won't even trust this mixed result for any merits.",
        "The results seem to work consistently better on some models, but inconsistent on other models, when compared to the direct baseline (for instance, Expanse-8B overall does worse; Qwen does worse, and Gemma2 2B, 9B, and G2 27B do better). However, this is only a direct baseline and other methods for abstention are not compared, which when compared, may show that this pipeline is not as good as one thinks.",
        "Table 1 shows that it is worse than simple prompting.",
        "Table 1 shows that the proposed method is worse than the baseline in terms of all the metrics considered in this work. moreover, the baselines are weak, stronger baselines from prior work should have been used.",
        "The approach outperformed 2 of 3 baselines on some metrics (mostly pertaining to code correctness). However, scored lowest on time and token input/output efficiency and maintainability.",
        "The proposed method is worse than the standard prompting baseline on all metrics\u2014average time, average input tokens, average output tokens, average compilable (%), average MI (%), and average tests passed (%). It produced a higher percentage of compilable solutions and had an average higher percentage of test cases passed than the CoT baseline, but at a significantly higher cost (by time, input tokens, and output tokens). The method does perform better than the third baseline, Fixed Chunk Prompt, but this also underperforms against Standard and CoT prompting and is not used in practice (and there are no cites to the contrary).",
        "The baseline is rather simple and there are mixed results (potentially due to limited result interpretation). More analysis is needed to validate the effectiveness.",
        "I am not sure about this. As I commented before, the experiment design and the metric reported is not very informative. Technically speaking, the method might improve upon the most naive baseline that it compares to, the experimental design might make it unclear in my opinion. It should also compare to more baselines. ",
        "The effectiveness of the methods is hard to tell from the results.",
        "While the proposed method outperform the baseline clearly, I think the baseline considered is somewhat limited (only all-evidence concatenation). Also, there should be an ablation studying on the credibility weighting (e.g. use the same weight for all evidence).",
        "The results are quite mixed, with the best results varying between the main approach and the baselines quite a bit. On SQUAD in particular though the results seem stronger, for gpt-3.4o so there maybe something more clear happening there. ",
        "Mixed results. The paper experimented on 3 datasets and only one worked very well. Moreover, I think there should be a computational complexity analysis to compare this method (like average run-time) versus grid search. ",
        "Though EGPM did very well on SQuAD dataset, the other baselines like direct prompt or ensemble works quite better than EGPM on TruthfulQA and SWAG.",
        "The experiments show very limited gain compared to the baselines. The proposed method is only really better on a few of the settings, and the gains are marginal. On some of the settings, there is even some degradation compared to the direct prompting baseline, which is the simplest approach.",
        "The proposed method seems effective on various datasets and robust towards various backbone language models.",
        "Table 1 presents the conclusion that this method is more effective than any existing approach quite effectively. The percentages on WMDP are lowest when using this method across the board, while not causing large drops in MMLU and MT-bench performance. The additional writing in Section 4.1 also indicates better performance on TOFU and WHP",
        "The reported results is consistently better than the baselines by a small margin.",
        "Again, wished they had tested against a stronger baseline. However, the method seems to be effective and beats the naive baselines by a lot without harming models' utility.",
        "Shows poor results and even traditional NMT beats the proposed method.",
        "The lexical prompting is almost always matching baseline performance or even worse.  This could be a dataset issue or a methods issue, but it's a bit difficult to tell since only the one dataset was evaluated.  There are no settings where the method beats the baselines on any of the languages evaluated.",
        "The proposed approach almost never outperforms anything else, and all the margins between approaches are very similar. I suspect the TED setting may partially be liable for issues here (or potential leakage of translated transcript data for example). ",
        "Most of the numbers shown with the proposed method are on par or lower than the baseline methods (Table 2, 3). The proposed method also didn't how higher performance for the autoMT baselines.",
        "The proposed method generally results in a drop in performance (on accuracy, brier, ECE). It only leads to improvement for the AUC metric.",
        "Table 1 suggests that even simple temperature-based calibration basically performs at par with this method. This by itself would imply a score of 3. However figures 1-3 show that this method completely breaks down at low confidences where it has an accuracy of 0 whereas other methods have an accuracy comparable to reported confidences. Therefore my takeaway is that this method performs worse than the baselines.",
        "Gains are marginal over ensembling and slightly weird (depends on alpha, which was tuned questionably)",
        "The proposed method has worse accuracy, Brier Score, and ECE than all other baselines. AUC is better than the Temperature Scaling baseline but worse than the Ensemble-Based Calibration",
        "As stated earlier, the paper showed the effectiveness of its designed symbol graph generation and iterative paradigm. However, it doesn't compare to the current baselines so we have no idea how much it improves upon those previous methods. ",
        "Just on a quick scan of related work, this paper has much worse scores: e.g. https://arxiv.org/pdf/2406.01304#page=3.59 from June 2024. Also see leaderboard: https://www.swebench.com/index.html",
        "On the two models evaluated, the proposed method does show clear improvements. DeepSeek-Coder-v2 improves from 0.7% to 2.7%, and Claude-3.7-Sonnet from 2% to 10.0%. However, these numbers are still too low to represent a meaningful takeaway, especially when considering that, e.g., SWE-Agent achieves 48.0% with the same Claude-3.7-Sonnet. It is unclear whether these improvements will persevere when added to an agentic framework (such as those leading the current pack of methods).",
        "Table 1 shows that the paper\u2019s approach clearly outperforms Standard and Standard + Example in Prompt for two different models. Additionally, Table 1 also shows that both key parts of the methodology are required, with both \u201cFile Content Determined by Package and Symbol Graph\u201d and the refinement step causing large increases in the number of issues solved.",
        "the experiments shows consistent improvement on different base models and there are 2 sets of ablations. But one issue is that it only has one dataset",
        "From the results in the table, the proposed method could not even outperform CoT or standard prompting. The paper analyzes the potential reasons but does not conduct further experiments to verify them, making the claims superficial and the contribution of the method less significant than expected.",
        "As the paper suggests, EST is statistically worse than CoT, the easy-to-implement baseline. It is not an effective method at all, even on the factuality metric the paper elects to use.",
        "No, the results of the proposed methodology performed worse than all baselines.",
        "It's clear from Table 1 that the results are not very good, all metrics decline by a significant margin across the board. This is true for both models. "
    ],
    "codebase_quality": [
        3,
        3,
        3,
        3,
        3,
        3,
        5,
        2,
        4,
        3,
        3,
        3,
        2,
        5,
        1,
        5,
        3,
        3,
        4,
        4,
        3,
        2,
        3,
        3,
        5,
        5,
        4,
        3,
        4,
        5,
        5,
        4,
        4,
        4,
        4,
        4,
        4,
        3,
        3,
        5,
        5,
        4,
        4,
        3,
        3,
        3,
        4,
        3,
        4,
        5,
        4,
        3,
        2,
        3,
        3,
        3,
        4,
        3,
        4,
        4,
        4,
        5,
        5,
        4,
        4,
        5,
        3,
        3,
        3,
        2,
        3,
        3,
        3,
        4,
        4,
        3,
        2,
        5,
        4,
        5,
        3,
        4,
        2,
        4,
        4,
        4,
        3,
        4,
        3,
        3,
        3,
        2,
        4,
        5,
        4,
        4,
        4,
        4,
        5,
        4,
        5,
        3,
        3,
        4,
        5,
        4,
        4,
        5,
        3,
        4,
        5,
        3,
        4,
        4,
        4,
        4,
        4,
        3,
        3,
        1,
        3,
        5,
        4,
        2,
        4,
        4,
        3,
        4,
        4,
        4,
        3,
        1,
        4,
        3,
        4,
        4,
        4,
        2,
        4,
        4,
        3,
        1,
        3,
        4,
        4,
        3,
        2,
        2,
        2,
        5,
        4,
        4,
        2,
        3,
        3,
        3,
        3,
        3,
        4,
        3,
        4,
        4,
        3,
        4,
        4,
        5,
        4,
        4,
        4,
        4,
        5,
        5,
        4,
        3,
        4,
        5,
        3,
        4,
        3,
        3,
        3
    ],
    "overall_score": [
        1,
        6,
        3,
        2,
        4,
        3,
        6,
        3,
        3,
        2,
        3,
        2,
        3,
        4,
        1,
        4,
        3,
        1,
        3,
        3,
        6,
        3,
        5,
        2,
        6,
        4,
        3,
        2,
        2,
        6,
        5,
        5,
        4,
        3,
        3,
        4,
        5,
        1,
        6,
        3,
        5,
        4,
        7,
        5,
        3,
        4,
        4,
        4,
        4,
        7,
        3,
        1,
        5,
        3,
        2,
        3,
        5,
        4,
        2,
        4,
        4,
        7,
        5,
        6,
        3,
        5,
        4,
        3,
        4,
        4,
        5,
        5,
        3,
        2,
        4,
        3,
        7,
        7,
        6,
        3,
        3,
        4,
        2,
        3,
        5,
        4,
        2,
        2,
        3,
        4,
        3,
        3,
        2,
        6,
        7,
        5,
        6,
        5,
        3,
        7,
        4,
        3,
        3,
        3,
        5,
        2,
        6,
        7,
        6,
        6,
        6,
        4,
        7,
        4,
        3,
        3,
        3,
        1,
        2,
        1,
        4,
        7,
        4,
        2,
        4,
        5,
        3,
        3,
        2,
        3,
        2,
        2,
        4,
        3,
        4,
        5,
        5,
        3,
        4,
        5,
        2,
        4,
        4,
        6,
        5,
        2,
        3,
        4,
        1,
        2,
        3,
        1,
        4,
        3,
        3,
        4,
        3,
        4,
        4,
        4,
        5,
        6,
        4,
        6,
        1,
        5,
        3,
        3,
        3,
        1,
        2,
        3,
        3,
        3,
        2,
        2,
        2,
        3,
        2,
        1,
        2
    ],
    "overall_rationale": [
        "This is a trivial contribution that would more likely serve as a weak baseline for more robust prompt tuning methods. The papers does not provide significant baselines for comparison, so results don\u2019t mean much to the reader.",
        "The motivation is relatively sound (maybe needs more justification), the solution is relatively novel, and the outcomes seem to be effective. The only downside is the lack of additional baselines from prior work and a stronger evaluation, but the contribution is sound.",
        "Considering the mixed results on effectiveness and lack of meaningful analysis, such as relevant safety or utility benchmark evaluation and common defense comparison, the paper will likely be rejected at a major AI conference. ",
        "1. Idea is a randomized version of Safety prompt injection. 2. Missing comparisons with existing methods. 3. Experimental details not clear, sometimes incorrect experimental reporting.",
        "The main reason for the overall score is the mixed results of the proposed prompting technique across all models. Despite the soundness of its evaluation designs, the results are not generalizable and make it hard to convince the readers of the effectiveness of the proposed method. ",
        "The results are not good, and the presentation of the results were messy. It would also be good to elaborate the analysis on different social biases, instead of grouping and taking all of them together.",
        "I think this would likely be accepted at a major AI conference, but additional experiments or discussion might be requested by the reviewers first. Mostly because of the mixed results across different OpenAI models - as a reader, I would have liked to see some of the pivot prompts that had the greatest differences in performance metrics (for example).",
        "The approach is creative, but lacks any clear theoretical grounding or relation to stereotypes and bias. It is also ineffective, which worsens this issue. But, given these shortcomings, it is well-executed in its breadth of evaluation.",
        "The idea is not new, which means the novelty of this paper is hard to see. The evaluation is flawed, while the analyses are significantly incomplete. The presentation is bad, with all the prompts stacking as the main content for methodology section. The motivation of neuro-symbolic structuring is lacking.",
        "Generally this paper is not well-written, with no comparison against prior work, there is a lack of depth in the analysis either, and there are some clarity issues in the paper presentation (e.g. why are all the scores for Task 8 N/A?).  When reviewing for the previous sections, I find myself sometimes needing to differentiate between my judgment of the content of the work versus the presentation of this work. If I were to review this paper on well-written-ness / clarity, this paper would not have had a very high score. Hopefully other reviewers in your study would be able to successfully disentangle this, and this is something you can consider as a confounding factor in your study.",
        "I chose clear rejection because similar methods involving iterative refinement with execution feedback already exist. Although the method automates the constraint-violation-checking process completely with LMs, their evaluation setting and metrics are under-specified and thus it is unclear whether the results can support their claim.",
        "Overall this paper lacks clarity in data creation, the experiments fail to justify the main arguments that this paper aims to state, and many many related baselines are missed from this paper for fair evaluations. It seems that the workflow of this paper is problematic - it does not went through a complete literature review, and citations to existing work are minimal.",
        "Ideas are good and interesting. Execution, analysis, and presentation of the paper would need significant improvement to reach a publishable state.",
        "I think that while the idea and method was strong, the results did not offer enough strength to be above the acceptance threshold. ",
        "Paper extremely poorly written (huge prompts in main text for like 2 pages), unexciting idea, unclear metrics and premise.",
        "There are some shortcomings of using LLMs to generate and perturb data at each step of the methodology, especially with no validation steps after each generation. The work also lacks novelty as many other works have explored this topic. ",
        "The paper explores an interesting topic with good motivations, but the proposed method only contributes marginally to the literature, and does not yield strong enough results compared with baselines. The writing of the paper needs to be significantly improved, particularly adding more clarifications of the proposed method.",
        "The idea is trivial, experimental design is critically flawed and the paper draws all the wrong conclusions from an incomplete study.",
        "As mentioned, the paper experimented with an existing idea in a new setting, without proposing novel methods.",
        "The paper presents empirical results that shows that CoT and RAG helps culturally ground LMs - a fact that lacks novelty and excitement. These methods are already industry practices, and combining the two of them produces no interesting insights or analysis beyond the simple fact that the model improves in performance. Metrics and evaluation are poorly written, and some of its claims with regards to weaknesses of lexical overlaps are not substantiated. ",
        "This paper presents a first try at defining the PBT problem and conducting experiments to see if LLMs can generate PBTs. However, it falls short of proving whether this system can bring end-to-end accuracy improvements.",
        "The paper lacks enough novelty and lacks sufficient experiments. The writing is also not good enough. While it has some value and is finished, it will clearly be rejected by ACL.",
        "PBT with LLMs is a sound idea and expected in an eval since LLMs have been shown to be able to write PBTs. The comparative study is well conducted, but since there's nothing inherently too novel, it's hard to justify accepting it into a major AI conference.",
        "Clear case of plagiarism and incremental work. ",
        "I like the idea and motivation of this paper. But the implementation and analysis of this paper still needs significant improvement to be accepted to the major AI conference. For example, more details should be added for the methodology, and further analyses should be added like I mentioned in the previous questions to justify the author's claim and the effectiveness of the proposed PBT.",
        "The idea itself sounds interesting, but the empirical results presented in the paper are mainly negative. Since there are no convincing evidence that demonstrates the effectiveness of the idea, it would be hard to turn this into a publication. The conclusion section of the paper offers some possible approaches to enhancing model factuality by tweaking the proposed idea. However, these are still speculations without any empirical results.",
        "No conclusive improvements shown. Authors ackowledge that further research is necessary.",
        "The idea itself isn't very novel. It simply used prompting with multimodal input to see if it increases accuracy. The paper lacks a more grounding reasoning to argue how this is different from other paper in the field. The propose methodology fall behind other baseline (simply just using text is better than multimodal) indicating the method is not successful. However, the paper didn't dig deep into the error analysis. Finally, the paper is only tested on only one model. It's hard to say how generic the result is. ",
        "In terms of novelty of both the topic and method, the paper does not stand out from related work. This is a method paper to enhance factuality but the proposed method underperform baselines.",
        "Overall, this paper makes meaningful contributions to the field of long-form text generation, particularly with the ACP method making dynamic updates to context memories \u2014 as opposed to the static contexts used in common approaches. Most results look promising. \u2028  However, the paper's contributions are limited by the method\u2019s effectiveness across broader datasets. ACP\u2019s actual superiority over existing methods is also quite marginal across most metrics, as shown in the Table 2 and 3 .   Furthermore, there are previous works, such as \"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text,\" that already tackle long-context generation using dynamic context tracking \u2014 but in a more interactive and delicate system. ",
        "While the paper is well written and the experiments are well-designed and well-reported, the results do not seem too conclusive to warrant publication. The proposed method ACP probably needs to beat / be on-par of the full text method for me to recommend publication. Otherwise, authors could conduct experiments on models without these long contexts to show that ACP is a necessary component in the generation process.",
        "The idea is very interesting, but the emperical performance is not promising",
        "The idea of context pruning presented in the paper is intuitively sound. However, the execution leaves much to be desired - the prompts are imprecise, and the method is also very computationally expensive. Additionally, the experiments presented are only on a single dataset, and the proposed method obtains little to no gains on it.",
        "Overall, while this paper presents a somewhat novel prompting technique, its main weakness is that it presents very little positive evidence of the proposed method, making it hard to convince the readers that the idea actually works.",
        "The results are not great and the idea is not entirely novel/groundbreaking. Also, there needs to be more evaluations in order for the paper to be more accepted.",
        "The idea is interesting and the experimental design is sound. However, this paper would be rejected due to the lack of effectiveness, lack of model and dataset diversity in the exploration, and little to no insightful discussion that could have provided useful takeaways even in the case of a non-effective solution.",
        "The idea is very exciting and novel and seeks to address a critical issue in a context-agnostic way, but falls short in its ineffectiveness. But, the future work suggests very promising next steps that, if this paper incorporated, holds promise for novel contributions.",
        "Please refer to the other sections for major drawbacks. The paper does not show enough methodological contribution and fails to provide sensible and robust empirical results for the procedure efficacy in bias mitigation. Given that both of these fail to make an appealing case, I would ask for a strong reject for the paper.",
        "This paper is interesting and technically sound thus should be above the acceptance threshold. Detailed case studies are provided and the overall presentation is satisfactory.",
        "The results are not promising, additionally no new insights were provided. It would help to provide examples and more explanation.",
        "The small number of samples, not using the same metrics as other works to compare the efficacy of this method, and the limited cultural context seemingly to stereotypes within a western context weakens the paper. But overall it is a fairly novel idea and could be useful for the field. I think this paper would be on the borderline of acceptance.",
        "The work only provides minor improvement over the existing LLMs. The results are not significant.",
        "The proposed method is novel and effective. It tackles an important problem for long-context translation in low-resource languages, where even trending models (GPT-4o-mini with base prompting) still fall behind in performance. The proposed pipeline, although manually crafted, still works effectively.",
        "See my rationales above! ",
        "The methodology proposed in the paper is extremely computationally heavy, and does not result in substantial improvements over the baseline. Additionally, the method claims to reduce off-target translation errors, however no relevant metrics are presented in the experiments section (only generic metrics such as BLEU and chrF are provided, the extent to which translations to the wrong target language happens is not evaluated). The experimentation in the paper is restricted to a single model, and details are not provided regarding the prompting strategies used, or the baseline implemented.",
        "The idea is a little novel and the overall implementation with experiment is reasonably clear. But the effect is not very good and the idea is also not novel enough.",
        "This is a relatively straightforward idea, executed with weak experimental results. The low sophistication and development of this idea makes it most likely a very weak submission for major AI conferences, hence a score of 4.",
        "Overall, I think the paper has some interesting ideas and solid experiments, especially the ablation studies. But the method feels a bit incremental, and some key design choices and results aren\u2019t clearly explained or justified, which makes it less convincing to me.",
        "The methodology proposed in the paper is sound, and results in performance improvements on the generation tasks evaluated in the paper. However, it has limited utility to MCQ datasets, and high computational cost without commensurate gains in this setting. Most importantly, the methodology proposed is not novel, borrowing heavily from existing work (without citing all of them).",
        "This paper could have been considered for publication at a top ACL venue since its idea is highly similar to a recent publication at EMNLP 2024. The execution in the paper demonstrates the effectiveness of the proposed idea. The positive results with significant improvements would likely convince reviewers at top NLP conferences. However, the current state of the paper may need further improvement for more rigorous evaluations to be as comprehensive as the published EMNLP paper.",
        "Novelty is not enough and insufficient comparison to baseline methods. No qualitative examples to support and the presentation is also problematic (i.e. Figure 1). The work need major revision and more comprehensive experiments.",
        "The paper explores prompting to elicit fairness in language models. However, fairness and toxicity is not clearly defined, and the methodology does not support the results of the paper (only mentions \"African-American son\" as a singular prompt, without mention of language, age, or religion as presented in the results). I suspect this paper is LLM-generated.",
        "I think this paper presents a simple intervention that appears to be effective in reducing biases. With the current experiments, I don't have enough confidence that the method can beat out other mitigation methods, but at least relative to the baseline it provides some improvements. While the method itself is not entirely novel, I do think that applying this technique to the bias + multilingual space makes it more interesting. ",
        "The evaluation here is simply too suspicious to take the results seriously, and there's a critical lack of motivation or reference to prior work. The approach itself isn't particularly novel, and the results themselves are largely negative. ",
        "The proposed method isn't effective and underperforms compared to the baseline. The reader cannot gain major insights from the findings in the paper.",
        "For top AI conference, it has major limitations: (1) the setup is not generic enough so it is applicable to general audience. it is very specific to a single domain. So the work might be suitable for some workshop? (2) the performance is not that strong compared with simple baseline.",
        "The scope of the problem under study could be wider. The improvement of the proposed method over baseline may not be significant enough.",
        "It doesn't give too much of a performance improvements against baseline, and are far from SOTA methods. The method is interesting but slightly incremental, with computational cost overhead.",
        "Due to unclear motivation and (slightly incremental) methodology, I don't think this work is providing enough insight or impact for *ACL community. ",
        "I have some concerns about the novelty of the method, It feels more like an engineering design for a code agent rather than a scientific finding. However, I think the design and evaluation are reasonable.",
        "While the results are mostly positive, I think it would be rejected at an AI conference due to its limited novelty and baselines. There are simply too many related works that also leverages retrieval for coding, and it's not clear from the paper to me how it is much more novel than previous works. As mentioned before, it still misses some relevant baselines and more ablations to verify its effectiveness. ",
        "The paper proposes a well-motivated and technically sound approach to a real-world problem in code generation using large language models (LLMs), specifically for repository-level tasks as benchmarked by SWE-Bench. Limitations are well-analyzed such as the higher computational cost. ",
        "the experiments are well executed and show strong performance gains. The main limitations are the lack of novelty, weak baselines, and some incorrect results in the tables.",
        "The paper covers the basic part for proposing a new methodology. The methodology is shown to be affective compared with some baseline. The paper also conducts ablations for a detailed analysis of the method. The idea is pretty interesting to me in turns of giving model this \"awareness\" to harmful input. I would like to see whether this harm the general performance of the model and what is the trade off in increasing the input length.",
        "The novelty is the major issue, even though the experiment is solid.",
        "Interesting idea if we exclude the Deliberative Alignment paper, solid experiments with reasonable baselines, but some concerns about soundness. Missing some interesting extensions that would make this paper more \"full\" --- currently feels skim like a workshop, rather than conference, paper.",
        "As said, the baseline comparison is far from enough. Importing results from existing work, even without replication can be a good way to address this. The results are mixed and the method should be further motivated and improved.",
        "I think the paper was well executed in terms of the code and the experiments. However, I think the experiment design and idea itself was not that compelling to begin with. It felt like this paper was essentially a fairly niche prompting mechanism that was better than simpler baselines for two particular benchmarks. I think there should have been a stronger description of what this method actually entails (particularly the different difficulty-level prompts), and the experiments should not just be numbers, but also include discussion of why ACGP actually produced the results provided.",
        "The lack of clear description of the prompting strategy and the underwhelming experiments are the main reasons why I would reject this paper.",
        "The idea is somewhat novel that itself is not the reason for rejection, assuming that there is no existing work with similar ideas. However, experiments are poorly designed and there's basically no analysis what causes this method to work or to fail. Furthermore, some claims are flawed or under-substantiated. With more rigorous experiments and more comprehensive analysis it could reach an acceptance threshold.",
        "The idea is interesting and the paper's overall structure is ok, may need better experiment design and more results.",
        "This is a relatively fresh idea on introducing axiomatic structure to LLM codegen. However, the execution is a fair bit superficial in terms of the effectiveness and design of the axioms and the datasets used to demonstrate the results. Hence, it would be difficult to see it getting accepted into major AI conferences.",
        "In general, this paper proposes a method to improve the code generation and task solving ability of LLMs based on a two-stage pipeline. From the experiments results, it could be inferred that the proposed method has increased the overall performance compared to the baseline methods. However, the method is still too simple to make essential difference, and the experiments are not strong enough to support some perspectives in the paper. For example, few-shot performance is worse than zero-shot performance, which is somehow contradictory to the few-shot learning ability of LLMs, and the paper does not give a concrete explanation to this.",
        "The results are not significant and the task and metric is also somehow non-informative. Besides, the method is basically auto summarized good coding principles for code generation, which could be easily achieved and even outperform by more careful curation and fine-tune the model. ",
        "This paper has limited novelty as it mostly follows the Tree-of-Thought algorithm but with replacing \"thoughts\" with quote generation. The results are also mixed and comparison to Tree-of-Thought is missing.",
        "As explained in the previous three response boxes, I think there are problems with this paper along various axes: its proposal is not very novel or likely to be impactful, its method is too expensive compared to the baselines, and the results are not convincing. I therefore think this paper does not present an advance meaningful enough for acceptance.",
        "This is a good idea and well-executed. I think it adds something novel to the landscape and honestly should be published.",
        "This paper provides a reasonable and well-grounded insight and results based on the proposed method. The modular attribution pipeline is sufficiently original and the empirical results are convincing. Although the fundamental elements rely on well-known prompting techniques, the components integration solves an important problem related to LLMs and their grounding capabilities. ",
        "The paper proposes a reasonable novel idea that builds upon prior works with a good motivation. Experiments are well-designed and evaluated carefully. The results are mixed but the successful cases are pretty strong. ",
        "The paper does not present any exciting insights or methods and the evaluations are flawed. However, given it is only 4-page short paper, i thought it would deserve more than a strong rejection",
        "The idea of this paper isn't very exciting to begin with. It proposed a new way to look at negation while didn't really give any clear results. The framework also wasn't creative and innovative enough considering there aren't clear score differences between model and the score are pretty saturated. The paper didn't provide clear insights into what are the roots of the problems and how will this insights help future work.",
        "The data and resource section is weak, as it lacks sufficient human verification and does not effectively highlight the limitations of existing models. The method section is also not effective.",
        "The paper does not have enough coverage on negation types, evaluation metrics, and error analyses. Also, after seeing the results in Sec 5, I do not find the research question well-motivated.",
        "While the idea of reformulating negation into prompts with positive tone is interesting, the core methods (\u201cReplace\u201d and \u201cProposition\u201d) to me are implementation of existing ideas for this specific negation task and have strong limitations (refer to the \u201cExcitement\u201d section).",
        "The idea itself is valuable, and the execution of the idea is good overall. The major limitation of the paper is the lack of thorough evaluation. The paper also points out the lack of a benchmark to make comparisons with. If this is indeed the first paper that aims to address cultural/social relevance or diversity in LLM dialogue generation, it could be more impactful to design several metrics (both automatic and manual) for rigorous evaluation. Without manual evaluation, it is hard to gauge whether the outputs really improve in terms of cultural adaptability or there are other data artifacts that LLM judges rely on when making their preference decisions. ",
        "The methodology is generally correct and the experiments are reasonable however I still think this is a more advanced prompting technique and this could be a baseline that others may use to compare their methods to. ",
        "Overall I think this paper propose a simple and naive method. The evaluation is very limited. And I don't think it's a good paper. ",
        "The work has limited novelty given many other similar work on using LLMs for social simulation. Also, the work is very simple and does not offer much significant contributions or new insights.",
        "Although the idea is novel, this paper does not present significant improvements, is missing some key ablations and experimentations, and does not provide a deep enough discussion of limitations and potential causes of lower performance. It is unclear what the argument of this paper is and it feels incomplete.",
        "Overall, I think this is an interesting idea, and retrieval-aided reasoning are definitely a heated topic. However, this method seems to \"over claim\" the self-improving part a little bit, and the fact that it cannot outperform direct prompting and the additional computational overhead it adds doesn't convince me that it is indeed useful. ",
        "Overall, I think the idea has some promise, but given the poor results, I don't see it being accepted in any major conferences. The evaluation settings are flawed and relevant baselines are still missing. ",
        "The method is unclear, while also lack of motivations and evidence to showing that 1) decomposed thoughts 2) memory are good for prompting MATH problems in LLM. The experiments do not including prior works in baselines. 3) Analysis (Section 5) is not sound.",
        "The topic itself is not strongly-motivated and the novelty seems limited. The experimental results show that the proposed method do not work.",
        "I think that the results in this paper are pretty simple, but the idea seems to work and is effective, which are all good reasons to support acceptance.  Unfortunately the work is missing some related system prompting baselines which would validate whether this method is effective against the state of the art methods, which is the reason why I cannot recommend stronger acceptance.  I also am surprised that the jailbreak attack this method is supposed to prevent doesn't appear to scale (see appendix A), which harms the motivation for the work because this attack may not be a problem as models continue to grow and adding more examples only harms qwen.  I also think this work would benefit from showing the robustness of this prompting other types of attacks besides the multi-few shot prompting attack.  Overall the idea seems to work and is presented clearly, but there are more experiments needed to recommend strong acceptance.",
        "Idea is nice, results are promising",
        "The evaluation seems comprehensive and the proposed method seems to be working. However, there is no statistical test, which makes it very hard to know whether the method is really better. More robustness checks are needed",
        "The motivation is valid, the experiments are reasonably sound. ",
        "The paper is generally well-written, but there is a lack of contrasting against prior work (and there is no sufficient justification to not contrast against prior work). There are some details that can be better clarified, and there is a lack of human evaluation (although I understand it is often challenging to conduct human evaluation studies for potentially toxic content).",
        "Not novel, not a large enough research contribution to justify a main conference paper.",
        "This paper is technically sound and novel. The presentation is clear and most of the essential details are included in the paper. The method is effective, and a new dataset has also made extra contribution to the field.",
        "The paper has critical flaws in the design of the dataset and metrics which make it impossible to verify if the stated findings are true.  Therefore it would not be accepted at a major AI conference.",
        "This paper has clear areas of improvement: first, in terms of execution, there is little details on how the testbed dataset is formed which raises concerns about the validity of the evaluation. Second, as discussed in a previous section, the method does not show marked improvements over a basic empathy prompting approach. To justify this approach, I would expect to see (1) empirical evidence that we see significant performance boosts on a metric that correlates with bias, not a contrived metric made for this task and (2) ablations on the different parts of the network. ",
        "The results are decently interesting despite being negative. However, the justification for the experimental setup is rather weak, and the evaluation is not well justified with respect to literature in the literary world, especially if the evaluation is made specifically to evaluate the quality of metaphors. Strengthening the evaluation would help the paper significantly. Another significant reason for rejection would be because existing methods for prompting are much simpler and do far better than MCT. There needs to be much more justification regarding why one would choose to use MCT over something like CoT prompting.",
        "As mentioned in the previous sections, there are various flaws in terms of methodology and analysis rigor in the paper. The method, while novel in idea formulation, does not contribute significantly to performance, and does not seem practical for large-scale adoption. Overall, there is a clear gap between this work and top conference level paper. ",
        "The integration of metaphor into LLM mathematical reasoning represents a fresh and promising direction with the potential for significant impact. However, the study's lack of rigor and clarity in method and experimental design limits its informativeness even for a short paper. ",
        "the aspect of the idea in this paper is interesting(relying on providing model of the potential way to solve math problem). but the effectiveness is quite bad.",
        "The paper promotes a strong idea to find the best pre-pend for each language given a task. The pre-pend will allow us to investigate internal model activations for interpretability work. In its most generalizable form, I can learn a generic pre-pend for all tasks for a language. However, the key pushback to this idea is that there are many types of adapters that can be learned for a model. The best adapter should be one of natural language - a language specific instruction leads to language proficiency. ",
        "As a short paper, I think this paper present a sound methodology for improving multilingual prompting performance by extending AutoPrompt to a multilingual setting. The experimental design is clear and easy to follow and can be achieved with parameter-efficient fine-tuning. The results looks promising on the models for which they tested. However, the fact that this method requires language ID as an initial step might be limiting.",
        "I would not mind accepting this as a short paper if there are some additional analyses of generalizability to unseen language of the trigger token is provided! Otherwise, a workshop paper seems like very reasonable!",
        "It is qualified for acceptance as a short paper. It has enough novelty, sound technical design and consistent performance improvement.",
        "The paper is carried out thoughtfully and thoroughly. Results clearly demonstrate the effectiveness of the proposed method. The only limitation about the paper is that it feels like a direct application of trigger tokens to multilingual LLMs, which lacks novelty and limits the potential impact towards the research community. ",
        "Overall the proposed method seems to be better than the baseline. But due to the limitations of the evaluiation, it is really hard to tell whether it's a truly novel method. ",
        "The paper proposed a simple yet effective method to jailbreak LLMs (GPT-4o), although the writtings in the first half part reveal the motivation is to improve robustness instead of jailbreaking. I would see this paper can be accepted as workshop or main conference small paper.",
        "Generally the idea here isn't bad, but the lack of good experimental setup makes it really hard to parse how good the results are here. There's also a notable lack of investigation and analysis into why masking may improve things here, which could otherwise enhance this paper quite a bit. ",
        "Despite some interesting result, the paper in its current state is a clear self-contradiction. Unless it reframes all of its results towards looking at NAM as a jailbreaking attack and not a defense (and adds further experiments showing this is true/analyzing NAM), it is not in a ready state for a major AI conference.",
        "The paper proposes an interesting \"hallucination inversion\" idea for prompting. However, without concrete examples, it seems challenging for the model to understand how its hallucinations originate. The results show the hallucination rate does not consistently decrease compared to the baseline, while relevance, factuality, and ROUGE score stay the same or decrease.",
        "No clear contribution, and the method shows low effectiveness.",
        "First, the methodology is very weak. Self-correction prompts is indeed work for multiple studies if they are strong guidance to self-check. This works just prompt LLM in every step and this increasingly open for more hallucinations without any good guidance. ",
        "The main flaws of the paper are in its poor performance and choice of evaluation datasets and baselines. Reviewers would not be convinced on a factuality work if it uses a summarization dataset to evaluation, for example. On the other hand, the only valid evaluation dataset--TruthfulQA--yields poor performance for the proposed method. It also doesn't have any qualitative examples or examples of how the method could work better than existing works.",
        "There are critical flaws in the experimental setup that lead me to doubt whether these results come from any real experiments. Even if they did come from real experiments, it's unclear what the value of these results would be given that they are a negative result that aligns with what my expectations would have been to begin with.",
        "Probably a reject to top AI NLP conference. Due to lack of strong motivation, lack of baselines.",
        "The idea is well-grounded in both linguistic theory and recent work in LLMs. There is a robust comparison to a variety of baselines, as well as over a number of models and languages. Generally, the methods shown lead to strong improvement over existing LLM baselines, and increased competitiveness with SOTA neural parsers. The experimental setup is strong and well-conceived.",
        "The methodology presented in the paper is interesting and could augment LLM-driven dependency parsing approaches. However, the paper does not explicitly mention the deltas over zero-shot approaches for their method, and the proposed methodology substantially outperforms existing, older dependency parsing methods.",
        "The main issues are that: (1) lacks technical contribution: the design of the prompts is too simple and not well-motivated/ablated. (2) lacks comparison with previous work: the method is only compared with the simplest baselines despite on well-acknowledged benchmarks. Even for short paper, there is at least several baseline comparisons needed as well as some analysis required, e.g., qualitative analysis or temperature scaling",
        "The score mainly because 1. lacking of baselines that it compared to and 2. the results is mixed (the performance is not that good).  ",
        "The idea of prompting to improve the overconfidence issue is novel and exciting. Yet the experiment is not principled and needs major revision. I'd be happy to raise my score if more systematic evaluation is done in the experiment section.",
        "The paper does not have enough technical contributions to the existing literature. The empirical results of the proposed method are mixed, and there are potential issues in the evaluations.",
        "In my opinoin, lack of detailed analysis of experimental results is a major rejection reason. While the idea is somewhat novel, the execution and lack of convicing experimental results makes the paper sub-par to top AI conference standards.",
        "This paper lack technical contribution and the method is not sound",
        "The main issues with the paper are a lack of clear improvement compared to baselines, insufficient baselines/models/datasets for comparison, and no error analysis or method improvements as a response to the limited improvement. While the method makes sense and seems promising, the paper doesn't sufficiently explore the method, and the presented results do not justify its publication.",
        "The method is quite trivial and without much novelty. It also involves a key assumption on independence that is not well-justified. The experiment results do not suggest reliable calibration improvement from using the method and is at the risk of decreasing model accuracy.",
        "Despite the research problem being an important one, this paper offers limited insights into solving biases in LLMs. The specific task settings are already studied in previous works, and the few-shot prompting method setting the paper apart is also a well-known simple technique. Result-wise, the paper shows that few-shot prompting does not work for reducing biases, which has limited contributions to the field overall.",
        "I think the problem they are studying is interesting, however it has been studied pretty extensively by other (https://arxiv.org/pdf/2410.19803, https://arxiv.org/html/2402.14875v2 and https://arxiv.org/abs/2406.1223) so the paper provides limited novelty and their few shot prompting approach does not seem to work that well. Maybe they would have seen different results if they evaluate more models. ",
        "Overall, I feel that the paper offers some interesting observations, but the lack of methodological novelty and inconsistent results across settings make its contributions relatively weak. While the analysis touches on important issues like social bias, the design flaws and limited depth reduce the overall impact of the work.",
        "Though the experiments are technically sound, the paper generally lacks novelty in its methodology (merely applying a very simple few-shot prompt), and also lacks any nuance or new contribution in the type of bias it explores (simple and somewhat unrealistic yes/no decision, as opposed to more subtle forms of bias like microaggressions, dialect differences, other correlates with race, etc.). The paper leaves a lot of room for more interesting analysis, like other prompting methods or non-prompting techniques, or more experiments to understand why this method fails to improve over the baseline. Even asking the model to explain its reasoning could provide insights beyond what this paper presents.",
        "Overall, this paper presents a somewhat novel prompting technique for improving the adversarial robustness of language models. The authors show through 6 different evaluation benchmarks that the proposed method brings consistent gains for both GPT3.5 and GPT4. The reviewer is not the most familiar with the literature on improving adversarial robustness of language models through prompting and therefore doubts the proposed method's novelty. ",
        "The paper could use some work on fleshing out its contributions more, and include more motivating examples. Without looking at the code, I probably would think that this paper is of lesser quality than it actually is.",
        "Way more details are needed for the paper, and at least some baseline (CoT, self-play.etc) should be compared. ",
        "The idea is solid, but ultimately overly incremental as a prompting strategy that is only a slight variation from existing techniques - while it's possible that this could be presented in a way that differentiates itself more from self-critique, the choice to limit to reasoning datasets heavily limits this. I'm also still concerned that the baselines are not an effective comparison due to differences in token budget. ",
        "The structures for the experiment, evaluation, and writing are all good. After further literature survey and experiment polish, this paper can potentially be around the acceptance threshold.",
        "The method lacks novetly, its not sound and the results are no good either. This would be an easy reject.",
        "I'm excited about this paper in terms of it takes a fairness aware approach to bias mitigation rather than fairness through blindness approach. I think the papers flaws lie in that it is similar to existing papers and that the prompting strategy has limited novelty. ",
        "The proposed method has some potentials but the experiment show mixed results. The commonly used disclaimer baseline is simpler and more effective. Computation-wise, it seems that the proposed method are less efficient as it involves multiple stages.",
        "The idea seems novel and effective. My only concern is lack of baseline comparison",
        "The idea is somewhat exciting and shows great effect on some models, but it's not novel enough.",
        "Overall this idea is not very novel and the cost is very high, so I think it should not be accepted. ",
        "- The idea is very incremental. - The method contains arbitrary choices without rigorous justification. - The experiment design is flawed and doing \"apple-to-orange\" comparisons. - The results are mixed even with flawed experiment design.  Base on all of the above this should be a clear rejection.",
        "Based on the fact that the idea is not completely novel, the score sits at mostly a 4, but possibly could be a 5. Because there are a lack of baselines, it is unlikely that the proposed method works better than other existing methods. Furthermore, the evaluation seems like it must be tuned for each dataset, which makes the method much less generalizable than other possibly proposed methods.",
        "A very trivial paper and it does not even work better than simple prompts. The codebase looks very confusing.",
        "The proposed method is not novel and does not improve over baselines. but the ideas are well executed. The experiments are complete and implementation details are well documented.",
        "This paper would be rejected on the basis of a lack of transparency in the experimental procedure and metric computation, and claims that are unsupported by the results shown. Reviewers would also take issues with the lack of discussion, namely what were the limitations, failure cases, and big takeaways. ",
        "The idea and experiments are reasonable, but the ideas have been explored in previous work. The specific method here is also shown to be ineffective (worse than a standard prompting baseline).",
        "In general, the idea is good, but the current version is not completed yet. Further effort can make the paper be above the threshold",
        "I think the idea in general makes sense but in general a little incremental, not very novel to the field. Moreover, the experimental design make it not showing whether it is in fact effective or not, and it seems to lack one row to report in the main table. ",
        "The results of the paper is confusing, and thus it's hard to make a qualitative judge on the efficacy of the method.",
        "While the idea and setting is interesting, I don't think the contribution of the paper warrant an acceptance. For me this looks more like preliminary results for a potentially interesting idea, more work needs to be done (e.g. studying more realistic settings, considering more baselines) for it to be a conference paper.",
        "The results are generally mixed, and the lack of empirical validation for most of the design decisions makes the experimental setup a bit questionable as well. Overall there aren't enough clear strengths to motivate publication. ",
        "The idea is somewhat exciting but the paper is just poorly done (missing key details on the method and also lacks complexity analysis) and the results are not exciting enough.",
        "Idea is good, but the methodology are not provided clear enough, if 2.2 and 2.3 has better explanation, this could be 6-7 for sure. For example, why using 0.2/0.4 on those two fitness function and what terms are for, why they are necessarily, why don't just use f_entropy  directly.",
        "The idea is decent and can be exciting. However, given the limited gain in terms of performance, it would likely be a rejection for conferences. The lack of baselines would also hurt its chances. ",
        "Very close to \u201cMarginally above the acceptance threshold of major AI conferences\u201d. Need domain experts to further verify.",
        "It's a simple idea that doesn't really introduce any new methods, I think it's just a very clever use case of prompting. Otherwise, I would say the execution of the method and the empirical results make this strategy quite convincing. The reason I scored marginally above is to represent the balance in what I think was an idea that was limited in its interesting-ness, but delivered well in terms of results.",
        "This paper enhanced the \"unlearning via prompting\" literatures by improving on top of Thakar et al., 2024. The improvement is consistent across multiple benchmarks comparing with the state-of-the-art methods. However, the pipeline is also more computationally expensive and it's unknown if the marginal improvement is due to the extra LLM filtering (no ablation study is done on that)",
        "I think the paper would be borderline between accept and reject, tending a bit more to an accept as it seems to have good results and the methodology doesn't seem flawed. There is a lot of uncertainty here, though, as I feel the baselines they compare against are weak.",
        "Poor idea, poor result. Doesn't give us any insight into the problem either.",
        "The idea is interesting, it unfortunately doesn't work which would not be a reason for rejection on it's own entirely (though it hurts excitement score), but it is also a paper with only one experiment.  I think put together (the method not working, but then having no analysis of why or failure cases) makes it a question of if the specific instantiation of the idea that the authors tested didn't work or if the idea itself is fundamentally flawed.  As such the paper would not be ready for publication.",
        "Overall, the fact that a very similar published paper already exists is a major drawback, (this work doesn't cite that paper, and in generally has an alarmingly low number of citations). The empirical setup is not focused enough for their claims, and their results show no clear effects, or even negative effects. ",
        "The major downfall lies in the novelty. The proposed method is not very novel and makes marginal contribution to the field. Additionally, the proposed is not supported to be more effective than the baselines.",
        "First of all, the idea in this paper is of limited novelty. Its discussion of related work is very limited and the approach is quite simple (not too much fresh insights involved). Also, it does not lead to clear performance improvement.",
        "As mentioned above, the description of the idea as in the paper is both incomplete and (what is described is) incorrect. The results strongly support this point: the method is at par with other methods at high reported confidences, and much worse at low confidences.",
        "Very weird method that seems fundamentally flawed, plus weak results",
        "This paper presents some interesting ideas in using contrastive QA pairs and a graph-based calibration method. However, the proposed method fails to beat any of the baselines, and the method is costly and could be domain limited as a result (since it requires generating contrastive QA pairs that are similar enough to future queries).",
        "The method of this paper is basically using some bew combinations of similar prior ideas, and the baselines are not correctly selected. ",
        "Incremental method, weak performance",
        "I think that idea in this paper is fundamentally flawed, because it tries to solve a problem (presenting codebases in a form digestible by an LM) in the wrong setting: there already exist many agentic frameworks that prompt LMs with tools and scaffolds, and one must either (1) evaluate such an idea within one such framework, or (2) compare against these frameworks to show improvements upon them. Showing improvements over base models is unsurprising, and focusing on that setting would be to pretend that we've made no progress as a field in the last two (or more) years.",
        "The proposed method is effective but it is highly derivative of prior work and lacks novelty. Specifically, both of these main points of the paper were explored thoroughly and shown to be effective in prior works: 1) using static program analysis was used by AutoCodeRover, MarsCodeAgent, and others; and 2) using an LLM to generate tests and provide feedback to refine the generated code was used by CodeR, MarsCodeAgent, and others.",
        "The result is not significant, it does not show how can this method solve harder and more complex cases. The edge of this method is not enough to provide insight in improving current framework that already achieve 50+% resolve rate",
        "Overall, I find this paper's contribution somewhat lacking. While the idea of source validation is interesting and potentially useful for improving RAG systems, the experiments are too straightforward and lack depth. The claims made are not well-supported by experiments or clear clarifications, which weakens the impact of the work.",
        "This is a paper working on an important topic with an interesting but well-established intuition. The poor framework and execution has made the experiment results worse than the baseline, hence justifying a strong rejection for a major AI conference.",
        "Quality of idea, methodology and experiments are wrong. Since I already mentioned that idea and methodology are not reasonable, experiments also perform worst than other (simpler) baselines. Therefore, overall score is 1.",
        "I think the paper is lacking in both novelty and results. I don't think the community would find parametric-only approaches to factuality to be very promising as most recent works are using external search systems. Furthermore, the results are simply too bad to be considered for a conference. "
    ],
    "faithfulness_score": [
        8,
        6,
        5,
        5,
        7,
        8,
        7,
        8,
        3,
        7,
        8,
        6,
        5,
        9,
        1,
        8,
        6,
        3,
        4,
        6,
        6,
        2,
        3,
        5,
        5,
        6,
        6,
        3,
        6,
        6,
        8,
        6,
        5,
        6,
        6,
        7,
        8,
        6,
        8,
        8,
        8,
        5,
        6,
        1,
        8,
        5,
        7,
        5,
        10,
        5,
        6,
        3,
        4,
        7,
        5,
        7,
        6,
        6,
        3,
        8,
        8,
        6,
        9,
        8,
        7,
        8,
        6,
        8,
        6,
        4,
        5,
        4,
        6,
        5,
        8,
        5,
        10,
        8,
        7,
        9,
        7,
        8,
        5,
        5,
        4,
        10,
        5,
        8,
        5,
        5,
        7,
        8,
        9,
        6,
        8,
        8,
        6,
        9,
        8,
        7,
        8,
        8,
        8,
        7,
        5,
        8,
        8,
        4,
        7,
        8,
        9,
        5,
        7,
        6,
        6,
        8,
        8,
        8,
        7,
        7,
        7,
        8,
        6,
        6,
        7,
        6,
        4,
        8,
        7,
        7,
        6,
        10,
        9,
        6,
        8,
        8,
        8,
        6,
        8,
        7,
        5,
        8,
        7,
        5,
        8,
        5,
        8,
        6,
        3,
        8,
        4,
        8,
        6,
        4,
        3,
        6,
        7,
        6,
        8,
        5,
        7,
        8,
        7,
        10,
        10,
        7,
        6,
        10,
        8,
        7,
        3,
        7,
        6,
        6,
        7,
        6,
        3,
        3,
        5,
        6,
        5
    ],
    "faithfulness_rationale": [
        "The paper follows the outline very faithfully. The main aspects missing lie in not having any baselines and having limited ablation experiments. The paper also does not report a control group of benign prompts.",
        "For instance, post-processing is not included from the proposed method in the outline and the keyword is only seen once in the paper. Beyond this difference, among the problem and statement and motivation, the paper is reasonably faithful to the outline.",
        "The sections that are implemented are reasonably faithful, although dataset coverage is not comprehensive. Step 6 - Step 10 are implemented either incompletely or missing. ",
        "1. The general idea of SFI is preserved in the paper. 2. Experimental details in the paper are mostly unfaithful to the experimental plan described in the idea document. - Missing fog density - Missing comparison with existing strategies",
        "The main deviation from the outline I see is that the paper doesn't include the character description dataset (e.g., a curated subset of the OpenAI WebText dataset; optional), even though the outline mentions it.  ",
        "The paper does follow the outline provided. It does not have the optional dataset though.",
        "This paper stayed mostly faithful to the outline, the only differences being that there was one less dataset used in the experiments, three additional models evaluated, and some of the ablations were left out. I think if the outlined ablations WERE executed in the paper, it would have made stronger than just a marginal accept.",
        "This follows all aspects of the outline closely, though does not turn to the fallback plan given the ineffectiveness of the initial approach.",
        "The methodology is simplified. It just follows the overall coarse idea. The implementation of type checking and constraint identification is coarse by just prompting the model using expression in the outline, without optimization. The data collection process is also flawed. \"Ensure the dataset covers a range of task complexities and API usage patterns.\" There is no distinction of task complexity and API usage patterns, or it is not well justified or analyzed. For the evaluation setting, the guideline clearly require an ablation study and the error analysis, while the implementation completely leave these two important things out.",
        "There are sections are the paper that are copied almost verbatim from the outline, which I suppose indicates faithfulness! I see that the final paper did not include the ablation studies from the outline, which could potentially help strengthen the paper.",
        "The paper is decently faithful to the outline, except the paper did not include the ablation studies, which are important. ",
        "I feel the fallback plan is not executed, as clearly the proposed method is not outperforming baselines. No meaningful analysis has been provided.",
        " The example questions in the proposal seem to be more specific and have a comparatively more well-defined answer. However, the example questions shown in the paper seem more like a prompt for an argumentative essay where the answer could be way more open-ended. I do not think the election in the paper faithfully follow the original proposal's intent. ",
        "It was extremely faithful, the only difference being that the paper didnt evaluate on PaLM but the outline did. ",
        "The paper does not really talk about step 4 and 5 in the paper and also does not \"creat[e] social/cultural answer pairs from models trained in different languages and backgrounds.\"",
        "The paper is very similar to the outline.",
        "The paper and code implement most of the outline. Particularly, the codebase seems to be structured exactly based on the outline.",
        "1. Cross-lingual transfer using a high-resource language as a pivot (again, a guided prompt of GPT 4.1 and Claude 3.5 Sonnet which will use inherent zero-shot low-resource translation capabilities) baseline is missing. 2. Cultural reasoning and Context-dependent translation are missing from the datasets. 3. Only 1 model was evaluated and the paper doesn't say which model. 4. No ablation studies.",
        "Overall, the paper implemented the CG-CoT idea, though with a few differences from the plan: - The paper fixed the reasoning steps to 2, and thus did not include the ablation study in varying the reasoning steps. - The paper did not create the cultural knowledge base and thus was retrieving from proverb only.   Other deviation: - Step 1: the paper only includes one dataset (Yoruba proverb interpretation), instead of three different tasks for 5 languages. - Step 2: the paper didn't create the cultural knowledge bases containing 1k entries. - Step 3: the paper didn't include the 3rd baseline (cross-lingual transfer) - Step 5: the paper did not include Llama-3",
        "The core motivation is kept the same, but parts of the experimental plan is different. For example, the final paper does not compile its own datasets, it doesn't implement \"cross-lingual transfer using a high-resource language\" as a baseline, and does not perform some of the ablation studies that would definitely make the paper better (e.g varying the number of reasoning steps, use different cultural knowledge base sizes, etc). ",
        "I am not sure if the distinction between a) PBT in Code and b) PBT in Natural Language is captured well in the final report. Except that, the report seems to be following the outline closely.",
        "The outline intends to develop a reasoning method to enhance code generation capacity via PBT. However, the paper is doing a comparative study of different methods, without proposing a new Property-Based Reasoning that enhances the models' capacity.",
        "This is not faithful: the original idea is to use PBT to enhance LLM codegen. Instead we got a paper that studies LLM codegen with PBT.",
        "The paper captures only the property generation part of the outline. The outline specifically states that the idea is to study \"program repair\" capability when using Unit Tests vs PBT. However, the paper does not study the program repair at all. Rather it includes experiments to compare unit tests, PBT and self-verification as a testing method for LLM generated code.",
        "The differences mainly lie in the implementation part.  For datasets, the outline clearly suggest other datasets than MBPP and HumanEval. Specifically, the outline requires experiments on CodeContest and QuixBugs-Python. For models, the model of StarCode and DeepSeekCoder is different from the outline. The outlines states Llama-3 and Mistral. For this part, I think the paper actually is a better choice. Since Llama-3 and Mistral is not specifically post-trained on code, which may result in the limited capability in coding and may hinder the performance evaluation. Finally for methodology, the guideline provide two choices for PBT, including a code version and a natural language version. The paper only implements the code version using Hypothesis library, neglecting the natural language version.",
        "I think the execution in the paper mostly follows the provided outline. However, I have doubts about the implementation of the metrics in the paper because the paper does not offer sufficient and precise descriptions to understand how the evolution metrics are implemented.",
        "The fallback plan does not seem to be carried out. Except that the rest is faithful to the outline.",
        "It includes most of the main body of the proposal. However, at the end of the proposal, there is a fall back plan, meaning if the methodology didn't work, the paper is ought to do some error analysis to test out the reasons behind. However, this error analysis part isn't shown in the paper at all. ",
        "In general, the paper follows the methods, dataset, and evaluation in the outline. However, without enough technical details in the paper, I cannot evaluate how closely the implementation matches the outline and to what extent the fallback plan is executed. For example, I checked the source code and it seems that all the evaluations are done by LLM evaluators (prompting), which probably don't match \"Step 4: Evaluation Metrics\" in the outline.",
        "Except for the experiment data, where the outline lists much more diverse data sources, everything else in the paper seems well-aligned with the outline.   The method explanation in the outline looks actually clearer than in the paper, in which sense a pseudo-code or figure would definitely help the paper\u2019s readability.",
        "Except the dataset used and slight modification from the proposed pipeline, the paper is very faithful to the original outline.",
        "The dataset used is different from what's in the outline.",
        "The paper follows the main ideas from the outline, but has substantially scaled down experimentation. It also misses a key part of the motivation - emulating how \"humans\" attend to specific parts of context as they write. The methodology in the paper instead uses a vague notion of relevance with no cognitive grounding.",
        "The paper is mostly faithful to the outline, except that it doesn't implement the fallback plan even though the idea kind of fails. It would be more faithful if it also explored additional ways to improve the prompting method e.g. using search/retrieval. ",
        "There is only one dataset used in the paper. In the outline, there were several datasets to be included and this definitely would have helped strengthen the paper, and perhaps the results would have been more convincing.",
        "The motivations and experimental design were faithful to the outline, however, some models, datasets, and baselines were omitted. It appears as though the paper contains the most simplistic implementation of the project plan. The fall-back plan was also not executed, even though the approach was unsuccessful. ",
        "The paper does not fully execute 3(3), which could have addressed some of my concerns above: \u201cThis procedure could be iterative, involving debate over sub-claims raised from each side to back up their arguments on the original claims.\u201d It also only tests one model, in contrast to 4(Step 3), which also would have addressed a concern. Otherwise, the paper follows the outline closely.",
        "The paper adheres well to the problem statement, motivation and the fall back plan. I think there are some mismatch in understanding the notion of decay, however, I could not understand the notion from reading the document as well. Additionally, the implementation and the test cases were not appropriately added.",
        "The implementation is mostly faithful; the detailed description of the prompts used (Step 3) are missing in the methodology section. ",
        "The work is reasonably faithful to the outline based on the provided draft, with the same datasets also being used.",
        "The paper follows the outline well. It uses the same methods and experimental setup. It doesn't include the backup plan (section 6 of the outline) but that doesn't seem to be relevant based on the results.",
        "I have some concerns over the code. I did not find code for the retrieving context (Step 4).",
        "The paper follows closely to the outline. The only problem is that no qualitative examples are provided, as suggested in the outline. This is probably due to the page limit, but such examples could still be placed in the appendix.",
        "Yep. i think the paper follows the google closely in implementing the prompting pipeline.",
        "The paper adheres closely to the provided outline - only the target languages are changed from Spanish to a broader set. The paper also implements some of the fallback strategies mentioned in the outline, when the initial method failed.",
        "The paper follows the main high-level idea, but some experiment designs are not accurate.",
        "The core idea is correctly preserved in the paper compared to the outline. Additionally, experimental design is also followed through, including the choice of models and baseline methods.",
        "The evaluation metrics and fallback plan for analysis are not well-followed. The motivation is clearly outlined but rather weak in the actual paper presentation. Also, the paper does not provide as many insights (e.g., why this phenomenon happens?) as expected, which is a bit disappointing. The outline overall seems more exciting than the content.",
        "The paper adheres closely to the outline provided. It also uses the same datasets and models as mentioned in the outline, and implements the methodology in the outline verbatim in the paper.",
        "The proposal mentions several datasets and metrics that the work could (and perhaps should) utilize for a comprehensive experimental setup. However, the current paper does not include these benchmark datasets and their corresponding evaluation metrics.   My understanding is the original proposal wanted to study whether the response from LLMs better conform to various cultural norms through FairPrompt, but the current paper does not study this aspect. Instead, the paper uses only toxicity as the bias score metric. This is a signifiant deviation from the original proposal.",
        "The paper follows the general flow of the outline. However, it's missing much of the analysis and qualitative examples section as proposed in part 5 of the outline. I believe this might be due to page limits, but the authors should still use the appendix to include test case examples.",
        "The introduction seems on-topic for the problem statement, although it lacks details. (Problem Statement: Multilingual language models (MLLMs) often exhibit biases and unfair treatment towards languages with fewer resources, resulting in poorer performance and misrepresentation for speakers of these languages.) However, the method described in the paper seems to fall short of the proposed idea, which seems to be much more comprehensive.",
        "The main method and implementation follows the outline, but I do notice that some sections are missing from the paper. For example, the outline mentioned including a comparative analysis; however, this paper is missing this comparison. Second, the datasets (FLORES-200, XNLI) mentioned were not used by the author, and in particular, the outline mentioned looking at multilingual datasets which the author does not do. Finally, for the metrics, the author looks at the toxicity of the generated text, which deviates from the suggestions in the plan, which include bias metrics like stereotype score, ICAT, etc. ",
        "Results in this paper don't seem to support being able to make claims, which is the main discontinuity here. The experiments themselves and setup generally do follow the description of the abstract though. ",
        "The paper follows the first half of the outline by implementing the proposed method and providing qualitative examples. However, the major part missing is following the fallback plan, since the results actually underperform compared to the baselines.",
        "Yes, the implementation in the paper follows the google doc fairly well!",
        "The outline mentioned analysis of effectiveness of each step as well as error analysis, but these were not presented in the paper.",
        "Instead of GPT-4o-mini, GPT-4, Gemini Pro 1.5, Claude-3.5 series, and Llama-3-70B/-chat, only GPT-4o and O3-mini are used. ",
        "I noticed that the outline has this as motivation \"Our key motivation is that LLMs can decompose an under-specified intent into several specific sub-tasks/steps\", which I dont see in the paper. The paper didn't highlight that subtask is \"mainly\" to consistent with user intent.",
        "The full paper does not include a case study, but the rest of the content is consistent.",
        "The final paper followed the initial plan pretty well. There aren't any significant deviation from what I can tell. The model implemented and followed the initial plan of CoC in each step. The only difference that I can tell between the two is that (1) the plan says one of the baselines would be direct prompting, which was not in the final paper and (2) the plan also includes additional models to evaluate like Gemini and Claude. I don't think these deviations alter the key ideas though. ",
        "The paper only used GPT4-o and o3-mini for experiments, but the outline mentioned GPT-4o-mini, GPT-4, Gemini Pro 1.5, and Claude-3.5 series. The outline also mentioned using  open-source large models, including LLaMA-3-70B-Chat and LLaMA-3-70B-Instruct.   The \"Test Set Examples\" section and \"Fallback Plan\" section in the outline were not mentioned in the paper. ",
        "sections 1-3 are almost exactly followed. A few proposed datasets (e.g., AdvGLUE, and RealToxicityPrompts) are not explored in the paper. A few steps in section 4 are also missing.  For instance the authors did not evaluate ASE's performance on out-of-distribution adversarial inputs not seen during the initial testing.",
        "The paper nicely cover the proposal and the experiments.",
        "The paper does not include many Test Case Examples shown in the outline. ",
        "Faithful on steps 1-6, skipped steps 7-10",
        "The dataset selected is different.  The original outline is a bit unclear about how \u201carea\u201d means. If it is a sub-question semantic unit, the excitement can improve. The design of the low/medium/high confidence prompt is a bit different \u2013 the draft version makes more sense to me.",
        "I could match pretty much every bullet point to some paragraph or sentence in the paper. However, I don\u2019t think the paper went that much deeper than what was offered in the outline. As a researcher, I\u2019d imagine I could take a high level point and extrapolate further insights / experiments to run, but I feel like I didn\u2019t see strong evidence of how outline points were expanded upon effectively in the paper.",
        "The models used are different (Claude was omitted from the final paper) and more benchmarks were suggested in the outline. The description of the prompting strategy was faithful to the outline. The concrete examples given in the outline would have been nice to see in the paper.",
        "Dataset in outline includes TruthfulQA, HotpotQA and SciQ whereas the actual manuscript includes TruthfulQA and SimpleQA. Model in outline includes GPT-4o, Claude 3.5 Sonnet and one open-weight model, but the manuscript only includes results from GPT-4o-mini. The results are mixed -- based on the fallback plan there should be a fine-grained analysis over ACGP in comparison to baseline methods, but the manuscript only analyzed ACGP at a coarse level in terms of ECE after each iterative refinement.  The rest of the outline seems to be followed pretty accurately.",
        "The overall idea is consistent. ",
        "The paper attempts to implement the ideas in the outline but falls a fair bit short: the axioms in the outline are distilled from large corpus of high quality text, while the ones actually used in the paper are cursory and elementary.",
        "Basically the paper follows the core idea listed in the document, while there are indeed some aspects that the paper does not obey the original idea. To be concrete, the paper formulates the research problem reasonably faithful to the document, and the method proposed in the paper is basically the same as the one stated in the document. The experiments are conducted according to the step-by-step experiment plan as well. However, there are several differences between the final paper and the original idea. For example, the plan instructs to use GPT-4 and GPT-3.5-Turbo as the base model, while in the paper the models used are GPT-4o and GPT-4o-mini. In addition, the instruction mentions to conduct axioms refinement, which may be crucial if the extracted axioms are initially unclear and not useful enough.",
        "It follow the general idea but with multiple places unfaithfully followed: like 1. the CoT is mentioned in the outline to be one of the baseline but not in the paper 2. Ablation study and case analysis is mentioned but not executed in the paper. ",
        "The proposed methodology is followed, with the exception of \"If the model decides to quote, generate N candidates for this reasoning step.\" Instead, the method generates quotes until one with a score above the required threshold is found. All the datasets used in the paper (MuSiQue, 2WikiMultiHopQA, StrategyQA, and MedQA) are also mentioned in this experiment plan, as are the main evaluation metrics (EM, F1, AlignScore), although the paper reports more evaluation metrics than specified in the outline. Out of the suggested baselines in the outline, the paper only compares against CoT, but adds a CoQ baseline.",
        "The final paper adheres to the spirit of this proposal, but several datasets are left out, such as HotpotQA, FEVER, TruthfulQA, and SciFact. Several metrics are also left out such as GREEN or MEDCON. The models mentioned in the outline are also left out in favor of GPT-4o and an old model - DeepSeek-Chat. Some baselines like a RAG system are also missed.",
        "Exactly follows the proposed methodology, but changes the name of the method",
        "The paper closely follows the outline, implementing the core idea of having quoting into each reasoning step using a structured prompting pipeline. It retains all key components like quote scoring and retries. Major datasets and evaluation metrics align well with the plan.",
        "The codebase pretty much implements the main ideas of the outline. The paper structure follows the outline almost exactly.",
        "The proposed problem and methods are almost exactly the same to what's in the outline. There are minor differences such as missing the\"self-verification step where the model checks its own output against the original negation constraints\" in the outline, missing the comparison between closed-form vs open-form replacement.",
        "The paper represents the overall structure of the proposal and managed the implement the models proposed. It also includes some of the examples in the proposal on the paper.",
        "The paper follows the outline. The Test Case Examples section is not included in the paper, but it is minor and does not affect the overall understanding.",
        "The outline mentions more fine-grained controls of negation, which are not presented in the paper. Similarly, the paper misses the confidence scores and human evaluation described in the outline.",
        "- The paper did not mention many other types of negation and the classification step mentioned in the outline, which can be quite helpful for the models to have even better performance.  - Differences in the method: There are no self-verification step and confidence scoring in the paper but are mentioned in the outline.  - Differences in the experiment setting: There are no \u201cclosed form\u201d and \u201copen form\u201d replacement baselines mentioned in the paper  - Human evaluation and specific test examples are also not mentioned in the paper but are in the outline.",
        "I don't think the execution reflects the study of low-resource languages or dialects. To my understanding after reading the code, experiments are conducted solely in English, and this is a significant deviation from the original proposal.  There are existing datasets mentioned in the proposal (e.g., OpenSubtitles corpus for dialogue generation), but the paper does not mention this. There may be legitimate reasons for not using or comparing with the dataset, but I wonder why.",
        "The paper is a very direct reflection of this outline with each section augmenting the outline (e.g., Section 2 Proposed Method formalizes the content in the outline into a more mathematical and formal problem statement )",
        "The key novelty in the original idea is about multilingual and low-resource, which are clearly mising from the current paper.",
        "The paper generally follows the outline except for some minor parts. In terms of model choice, it just use one model (gpt4o) instead of the three models (gpt4, gpt3.5, llama3) in the outline.",
        "The proposed test cases for the problem in the outline are much more conducive to subtask decomposition than the examples that were highlighted in the paper (which were mostly questions posed in free-form text, aimed at grade school children). The core idea was consistent, but prompting techniques were not discussed thoroughly, nor were they modeled in the paper for readers. There were no ablations over this either. Most importantly, the direction to generate subtasks if there were not enough matches retrieved from memory was not executed.",
        "Step 4 (4) are not discussed in the paper, especially in section 2 (2.3).",
        "The paper follows basically everything that was written in the plan. The datasets and models are mostly the same. However, there are a couple key differences like the baselines (plan mentions CoT PoT and others that are missing in the paper). The plan also mentions GPT-3.5 that was not used. ",
        "Proposed method and experimental plan are consistent between the paper and the outline. The outline mentioned multiple baselines \"select direct prompting, CoT, PoT, Self-consistency\", while the paper just have direct prompting.",
        "The paper clearly follow the experiment designs, including methods, baselines, and datasets. It seems that the methods do not work and thus the authors do the granularity of memory units experiment listed in the Fallback Plan.",
        "All of the methods and ablations from the outline are tested in the paper. The reason for the medium score here is primarily due to the fact that the outline was very vague about how to measure things like helpfulness and attack success.  Furthermore the outline didn't specify details for dataset construction.  The author made reasonable and even good decisions to fill in these blanks, but the plan was under specified.",
        "It follows the outline while adding an additional dataset for experiments",
        "This paper follows the outline well in general! Althought some parts are missing. ",
        "There were some minor deviations, but the paper generally follows the outline listed. (e.g. not all novel prompting defense in the outline are presented)",
        "The paper clearly follows the key ideas and approaches outlined in the outline. The author used LLM-as-a-judge for implementation for helpfulness, which is not mentioned by the outline. ",
        "Different datasets were used than those proposed in the experiment plan.",
        "Most of the steps in the entry is included and well articulated in the paper.",
        "The dataset plan seems to have changed from the original proposal, but by the human comment this seems to be due to irrelevancy in the proposed datasets.  I think some of the datasets still may have been relevant, such as AskReddit, but I can agree with the human that the demographic details there would be a virtual impossibility to validate at scale.    Otherwise, all experiments, methods, and metrics match very faithfully.",
        "The paper is clearly faithful to the outline provided in the paper. It follows the instructions that were listed out about the types of prompts to use and which datasets; however, I feel like it followed the prompts almost *too* well like literally copying the words into the prompt without much refinement which limited the quality of the work. ",
        "The paper is fairly faithful to the problem statement in that it follows the problem statement (in the paper, notes that models struggle with abstract mathematical concepts), motivation (human understand and apply abstract concepts by drawing analogies to more familiar domains), and the proposed method (MCT). For the most part, the introduction of the paper is a carbon copy of the proposal, and the end results do end up addressing the pivot points (that MCT does not do well, so more analysis on how metaphors help is done instead).",
        "The implementation is mostly faithful to the plan with some modifications, such as missing few-shot baseline and missing statistical analysis.",
        "- The dataset preparation section (4, Step 1) in the outline is unfaithful: MATH and GSM8k were not mentioned in the paper. Instead, the paper used a subset of OlympiadBench.  - Test case examples (5) in the outline were not mentioned in the paper. The paper does not have a concrete case study.  - The paper didn't mention a Fallback Plan (6) (although the fallback plan in the outline looks quite reasonable to me.  ",
        "we can see main components and experiment design in the outline appear on this paper. And the core idea/method.",
        "The paper followed most of the plan. I do feel that having more datasets as outlined in the plan (XNLI, etc) would be helpful to show generalizability of learning trigger tokens to multiple task setups. Overall, the plan is fairly short and the paper followed most of it to the T.",
        "The high level idea from the outline is retained in the paper, but most of the details in the paper are not present in the outline, such as the use of the MMLU benchmark or the specific evaluation methods.",
        "It sticks with the google doc outline really well!",
        "The paper generally follows the guideline except for some minor difference. Instead of having multiple benchmarks and using different task-specific metrics in the outline, it tested MMLU which is all MCQ and just used accuracy.",
        "The final paper closely follows the proposed outline. The only notable difference is the last section, where the proposal mentions a fallback plan that the final paper does not do (probably because the proposed method succeeded). However, some error analysis would still be nice to have. ",
        "The overall idea is well followed, but some key metrics are missing. For example, the proposal mentioned BLEU, which is not used at all in the paper. ",
        "The paper follows the experimental plan strictly, although I believe at the proposal stage it wasn't expected to actually jailbreak the models. The part that diverges from the outline is that the proposed method actually jailbreaks the latest models instead of defending against attacks, which differs from the given outline. I believe the final paper made a good strategic shift by focusing on the experimental results to reveal something new to the reader.",
        "The outline is generally followed, but there's a notable lack of the rule-based baseline across the board, and no BLEU score anywhere, which likely would likely strengthen things. There's also missing qualitative analysis in the paper. ",
        "The paper clearly tries to follow the suggested outline in this document. In Step 6, rule-based filtering was not included as a baseline. Steps 7 and 8 were not sufficiently presented. Apart from these differences, the fact that NAM did noticably worse than simple prompting led the final paper to half-pivot to calling it an attack instead of a defense. I think that angle makes sense, but the pivot should have been full.",
        "The experimental setup and method clearly follow the key ideas from the outline. However, given that the initial method did not work, the authors did not follow up on the fallback plan, such as combining CMI-HM with external knowledge sources.",
        "The method and experiment setup section in the paper is the same as the outline.",
        "I don't see any major difference between outline and the paper. I think the outline has the method implementation with more details than in the paper itself. ",
        "I cannot find any major differences between the paper and the proposed idea. A minor difference is the exclusion of analysis and ablation studies. These could have made a difference to the paper in terms of providing additional insights.",
        "The paper didn't use LLMs for any idiomatic expressions which were mentioned. The paper is pretty vague on what it means by symbolic rules so I think that part seems possibly underspecified, but the paper executes close to the vision. The outline mentions vernaculars and uses a Pidgin (Nigerian) as an example, but the paper didn't evaluate on contact languages, pidgins, or creoles. Nor did the paper test on American Indigenous language or Indic languages. ",
        "I think it is faithful in general! The method being proposed in the google doc is very similar to the one ended in the paper. ",
        "The core idea and experimental protocol given is similar to that of the outline. There are some minor differences - the set of languages chosen from UD is somewhat different than that in the paper (e.g. Yue, which is not in the list of languages or suggested families, is included in the paper). Overall though, the baselines, prompts, and task used are highly similar.",
        "The paper generally follows the given outline. However, it diverges somewhat from the outline in the experimentation, using an additional \"Word Contexts\" approach along with the \"Rule Writing\" approach in the outline for its experiments. The paper also expands on the set of languages for experimentation compared to the outline.",
        "This paper mainly follows the outline, but the analysis (Step 6) is not well studied.",
        "The method and the basic experiment design faithfully followed the outline. Only a few details like how many/which model to use: outline talked about GPT-3.5-turbo and GPT-4 while the paper they use GPT-4o-mini. ",
        "- The paper does not cover a detailed analysis on how CSPP compares to the baseline and how CSPP reveals areas of uncertainty not captured by direct confidence elicitation. - Missing ChatGPT-3.5 implementation (minor issue)",
        "The paper does not implement an important part of the outline: the fallback plans. If this has been implemented, the technical contribution of the paper could have been significantly strengthened. ",
        "The core ideas are executed. The fallback plan:  Fallback Plan: If the proposed method does not yield improvements compared to the baseline, we will analyze each step of the uncertainty estimation to identify obvious erroneous estimations and potential common error patterns, which may lead to interesting insights. Another option is to incorporate fine-tuning techniques for improving calibration (such as those proposed by Xu et al., 2024) into each step and evaluate whether this approach brings more significant improvements than applying these techniques solely to the final uncertainty estimations. Ideally, the improvements will accumulate throughout the process.  did not seem to be executed. ",
        "It follows the outline except that it does not include the StrategyQA dataset",
        "The core components in the outline resemble those in the final output. However, there are some differences: (1) The original outline proposes multiple potential datasets to evaluate the method on. However, the paper only considers one dataset, GSM8K. While a small change, this weakens the proposed impact of the method by not considering multiple datasets. (2) The original outline proposes detailed error analysis that should be done in the event that the method does not show clear improvements. However, the paper does not conduct this analysis, even though the method only shows improvements for some models. (3) The original outline suggests Llama-3-8B, but the paper uses Llama-3-70B.",
        "The high-level design of the method and implementation align with the outline. However, there are obvious missing parts such as lack of datasets and models.",
        "Basically, all the sections in the outline are well-reflected in the paper \u2014 the paper seems to correspond exactly to the outline. There\u2019s little divergence. Even the experiment plan is strictly followed. ",
        "The paper was very faithful to the outline with the exception of investigate few shot example ordering in the fall back plan. ",
        "The paper mostly follows this outline, especially in the statement of experimental settings, designs, etc. But the motivation part is kind of lacking, as I do not see discussions of strong motivations or related works. Also, the results actually do not seem as expected, but the paper still fails to provide in-depth analysis and repeatedly focuses on one or two major arguments, writing about them several times. This makes the depth and breadth of the research not sufficient as planned.",
        "It is not clear that this paper really addresses the method to \"unlearn the biased associations\"--it seems like there are techniques that could try to address this more directly, by accounting for the baseline level of bias. Also, it seems like the fallback plan in the outline would have addressed some of the gaps I discuss, but were not followed in the paper. Otherwise, it follows the outline very closely.",
        "The paper follows the outline closely, including the steps in the prompting technique, the proposed evaluation datasets, as well as the suggested qualitative analysis. The review doesn't see any major deviation in the paper from the outline. ",
        "The paper does follow the outline quite closely. The human evaluation portion would be interesting to consider adding for the future.",
        "Metrics:  Robustness score (ratio of adversarial to original accuracy) and average response length are not reported, instead, the paper report robustness accuracy and token usage (which include both input and output? need clarification)",
        "The paper is very faithful to the core ideas of the outline, including the methodology, the models used, most of the datasets, and the suggested analysis. One dataset that the paper does not include is the adversarial test set. Having this adversarial test would've better demonstrated the need to think adversarially more than any of the reasoning datasets. However, it also makes sense to not include it due to implementation concerns.",
        "The outline is well-written. The main difference is the model selection",
        "Uses the method, but doesn't follow all the evaluation metrics mentioned in the paper. The steps have been concatenated as a single prompt, instead of doing them step by step.",
        "There were something missing like \"Stereotype Adherence Score: Use GPT-4 to rate responses on a scale of 1-5 for how much they adhere to common stereotypes\", but included almost everything else!",
        "The paper follows the topic, evaluation metrics, and analyses in the outline. For the human evaluation part, the outline says 5-10 human evaluators but the paper only has three evaluators.",
        "The paper does not include the baseline listed in outline. And the model choice is different but this is understandable, the paper uses more recent models.",
        "The outline is mostly followed, but some models tested are different.",
        "The overall idea is followed but I think the framing is not. The paper should highlight it's value to low resource languages as instructed in the original proposal. ",
        "I think this paper matches the outline clearly with no major modifications, additions or omissions. The flaws stem from execution not outline as there's an underspecification how the experiments should be designed in detail.",
        "Most of the information presented in the outline are included. However, there is some difference in the motivation (for instance, the goal of marginalizing knowledge across languages, is not noted). Beyond this, the proposed method is faithful to the outline.",
        "There lacks many steps of proposed methods in the outline (e.g. Consistency Checking). Eval metric also miss out consistency score. Also should have done statistical analysis.",
        "The paper uses the same methods and baselines as in the outline. There are a few minor modifications. For instance the paper used  Use GPT-4o-mini only for all experiments, but the outline mentioned testing the method on  GPT-3.5-turbo and Claude-3.5.",
        "The paper completely commits any discussion of the task decomposition strategy that is mentioned in the outline. Only one of the two benchmarks listed are used in the experiments, and some of the evaluation metrics are omitted as well (cyclomatic complexity, consistency score). Most importantly, the backup plan was not executed. There was no experimentation or discussion of why the approach did not outperform one of the baselines.",
        "The overall method is faithful to the outline, although the exact prompts used for decomposition and iterative prompting to generate chunks of code are different. The paper's experiments are performed on LiveCodeBench using GPT-4o-mini as specified in the outline, although the outline's additional experiments on Repoformer and experiments using other models are both missing. The paper includes most of the metrics used in the paper.",
        "There is difference in the model weight selection and data selection",
        "The idea is generally followed in the outline but there are not faithfulness in almost every section:  1. the method details is not faithfully followed, like the weights for all evidence in the outline are obtained in one forward pass in a prompt but the real implementation is one prompt for each evidence.  2. The datasets are planned to do two: ConflictQA and MMLU but it only did one.  3. Models are planned to use three llama-3-8B, 13B, 70B but in the end using 2 (one of them only report the baseline method). ",
        "Major dataset mentioned in the outline is missing in the paper; evidence ratio is not defined in the paper; Missing qualitative evaluation.",
        "Some discrepancy from the plan: - Method: the outline proposed to use one LLM call to estimate weight for all evidence and assign weights. Instead, the paper proposed to assign weight independently and then conduct normalization. - 4 (1) MMLU is not experimented with, only ConflictingQA. - 7 Missing result analysis on cases where the proposed method outperform baseline.",
        "Most of the stuff in the outline is followed. There's a bit of an exclusion of claude as a model , though not necessarily fatal. Limited to no analysis is really done either which is a bigger issue with the work. ",
        "Only experimented two models and the outline asked for 4. ",
        "I dont see these two: dropout-based techniques, and temperature scaling in the experiments where they should be include.",
        "The high level ideas are implemented but there are a few key details missing. For instance, the plan specifies developing crossover and mutation of prompts, but this was missing from the paper. The implementation of these ideas would have made the paper much more interesting. Furthermore, from the code I saw that the initial population of prompts are fixed but not randomly generated. Finally, it's also missing other LLMs that it was going to evaluate like Claude.",
        "The outline is well-written, and the paper follows the design. The authors further added analysis of DSPy and multiple backbone LLMs.",
        "The paper does a good job of following through on the initial outline. I couldn't really identify any parts of the paper that deviate significantly from what was provided.",
        "Besides section 5 (Test case examples), most of the content in the outline has been faithfully followed.",
        "Most things from the outline are in the paper, even the fallback plan of optimizing the prompt in case the performance was not as good in the first iteration, for example. Other key aspects that are both in the final paper and the outline are: (a) to test on different strong reasoner models such as GPT4 and Claude; (b) the benchmarks on unlearning to evaluate on; (c) the benchmarks on utility to evaluate on; (d) the motivation behind the idea.",
        "It follows the exact methodology and uses the same models for doing so.",
        "The datasets, evaluations, metrics, etc. all matched.  The one deviation seems to be that in my opinion the suggested fallback plan should have been triggered.  The results were negative so it makes sense to investigate why, which actually was considered by the fallback plan.  This was not done in the actual paper so the replication was not perfectly faithful.",
        "Generally the paper follows the outline in terms of experiment structure quite straightforwardly. There's very little extension beyond outline, in fact. The overall claim about traditional systems being better is somewhat suspect and not fully supported by results and setup, however. ",
        "N/A. The paper appears to be faithful to the outline.",
        " The paper closely follows the core idea in the given outline. There are some minor modifications like it proposes a new metric AUC which is not in the outline. Also, the paper does not achieve significant improvement compared to baseline but it does not implement the fallback plan stated in the outline.",
        "Some of the later stages in the proposed plan are skipped (mainly steps 8 and 9). The fallback plan described at the end (analysis of various methods if the proposal doesn't pan out) has also not featured in the paper. Otherwise, the method describes in the paper matches this outline.",
        "The hybrid semantic similarity thing is completely different than this confidence-only space. The original idea made a lot more sense.",
        "The paper sticks to the proposed outline, including the exact datasets to use, the method proposed, and the model to test. It also uses the proposed evaluation metrics, ECE and Brier Score, although the paper adds AUC as a metric. The paper does not do the ablation studies in Step 8 of the outline or the analysis of the confidence maps proposed in Step 9 of the outline.",
        "The general idea are almost included appropriately in the method. The only problem is that the outline mentioned that the unit tests are generated from a bag of reasoners with their confidence calibration, while the paper only use one (the local LLM). ",
        "The QA step is underdeveloped in the final paper, e.g. no calibration / correctness checks, not a bag of reasoners. Though I'm not sure I would use the bag of reasoners anyway (seems overkill for little benefit).",
        "The final paper is mostly consistent with this document. The main difference I found is that this outline suggests using a \"bag of reasoners\" with many models, whereas the paper uses a single local model. The analysis of the symbol graph was also left out.",
        "The paper generally follows the outline, although the specific datasets, tools and models are slightly different (but within the same family). For Step 1, Instead of SWEBench, the paper uses SWEBench-Lite. For Step 2, instead of using Sourcegraph for static code analysis, the paper uses StaticPA. For Step 3, istead of using diverse system prompts, the paper only uses a single system prompt. For Step 4, the paper does not use any of the specific suggested models, but uses the latest version of one of the models (Claude) and DeepSeek-Coder-v2. Step 5 is followed. Step 6 is skipped, and the paper only uses one iteration of improvement for Step 7. The suggested baselines are followed. The paper tests the core ideas proposed in the outline, but the methodology is notably streamlined by using a single refinement iteration.",
        "the paper retains the element of having other LLM doing the code snippet analysis. But the core idea of the outline is about having a bag of LLM reasoner to propose test and leverage the calibration of each LLM to eventually provide feedback. This paper mainly analyze single-LLM effectiveness on this topic.",
        "The result of the proposed method is obviously not good. However, even with this result, the model tries to propose this method instead of switching to a different perspective to analyze the results. Also, the fallback plan becomes a section in the paper, but it should be something that LLMs should do or consider once results fail to reach the claim.",
        "The paper attempts to implement the outline but superficially - it reduces the most important part of the idea, \"identify potential sources of this information\", into a superficial prompt template that is not even experimentally effective. Without it, the method proposed by the outline cannot be realistically developed, and hence the paper is somewhat unfaithful.",
        "The paper mostly aligned with the outline. However, it lacks the research-level precision such as motivation, problem framing, and clear example.",
        "Most of the main ideas are followed and implemented the paper. However, I noticed a couple key deviations that would have been differences in the paper. First, the plan notes using metrics like source diversity, attribution rate, and confidence calibration to judge the models, but all of these are missing from the results, whereas the inclusion of these could have made more contribution. The second problem is that it lacks qualitative analysis on the source attributions but focuses on trivial things like being rejected by the API. "
    ],
    "confidence_score": [
        3,
        4,
        5,
        4,
        4,
        3,
        5,
        3,
        4,
        4,
        4,
        4,
        4,
        5,
        3,
        4,
        5,
        4,
        3,
        3,
        3,
        4,
        3,
        4,
        4,
        4,
        3,
        4,
        4,
        4,
        2,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        2,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        3,
        4,
        4,
        4,
        4,
        5,
        4,
        4,
        4,
        4,
        3,
        4,
        3,
        3,
        4,
        4,
        4,
        4,
        4,
        5,
        4,
        3,
        3,
        3,
        4,
        3,
        4,
        3,
        3,
        4,
        4,
        3,
        4,
        5,
        4,
        4,
        4,
        4,
        3,
        3,
        5,
        4,
        4,
        4,
        4,
        3,
        3,
        3,
        2,
        3,
        3,
        4,
        4,
        4,
        4,
        4,
        3,
        4,
        5,
        4,
        3,
        3,
        4,
        4,
        4,
        4,
        5,
        3,
        3,
        4,
        5,
        3,
        3,
        5,
        4,
        2,
        3,
        4,
        3,
        3,
        4,
        4,
        4,
        3,
        5,
        4,
        4,
        4,
        4,
        4,
        3,
        3,
        4,
        4,
        4,
        4,
        3,
        4,
        3,
        4,
        3,
        3,
        4,
        5,
        4,
        4,
        4,
        3,
        4,
        4,
        3,
        3,
        4,
        3,
        3,
        3,
        4,
        4,
        5,
        3,
        4,
        4,
        4,
        4,
        4,
        3,
        4,
        3,
        4,
        4,
        4,
        5,
        3,
        3,
        4
    ],
    "minutes": [
        75,
        60,
        60,
        50,
        27,
        60,
        60,
        40,
        60,
        42,
        150,
        60,
        40,
        30,
        25,
        40,
        30,
        35,
        45,
        70,
        90,
        100,
        45,
        60,
        55,
        30,
        60,
        30,
        60,
        40,
        120,
        60,
        30,
        40,
        60,
        60,
        45,
        70,
        60,
        40,
        35,
        25,
        45,
        20,
        45,
        35,
        40,
        50,
        45,
        30,
        45,
        35,
        30,
        45,
        40,
        35,
        28,
        52,
        40,
        30,
        90,
        50,
        45,
        30,
        30,
        43,
        70,
        100,
        30,
        60,
        35,
        45,
        50,
        60,
        70,
        120,
        30,
        45,
        30,
        80,
        30,
        30,
        50,
        90,
        35,
        40,
        20,
        45,
        60,
        50,
        60,
        45,
        50,
        45,
        30,
        30,
        40,
        40,
        60,
        50,
        60,
        30,
        60,
        60,
        60,
        25,
        60,
        35,
        20,
        50,
        65,
        25,
        40,
        60,
        90,
        40,
        60,
        40,
        60,
        95,
        30,
        50,
        60,
        70,
        65,
        50,
        60,
        120,
        60,
        40,
        60,
        45,
        25,
        45,
        45,
        30,
        60,
        60,
        45,
        60,
        35,
        46,
        45,
        45,
        41,
        30,
        60,
        60,
        20,
        60,
        60,
        80,
        80,
        75,
        50,
        45,
        40,
        50,
        60,
        70,
        70,
        110,
        70,
        65,
        25,
        35,
        50,
        30,
        75,
        120,
        40,
        120,
        90,
        41,
        90,
        120,
        40,
        35,
        45,
        30,
        55
    ],
    "topic": [
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Math",
        "Math",
        "Math",
        "Math",
        "Math",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Math",
        "Math",
        "Math",
        "Math",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Bias",
        "Bias",
        "Bias",
        "Bias",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Safety",
        "Safety",
        "Safety",
        "Safety",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Multilingual",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Uncertainty",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Coding",
        "Factuality",
        "Factuality",
        "Factuality",
        "Factuality"
    ],
    "condition": [
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI",
        "Human",
        "Human",
        "Human",
        "Human",
        "Human",
        "AI",
        "AI",
        "AI",
        "AI"
    ],
    "idea_id": [
        "Safety_2_AI",
        "Safety_2_AI",
        "Safety_2_AI",
        "Safety_2_AI",
        "Bias_3_AI",
        "Bias_3_AI",
        "Bias_3_AI",
        "Bias_3_AI",
        "Coding_3_AI",
        "Coding_3_AI",
        "Coding_3_AI",
        "Coding_3_AI",
        "Bias_2_Human",
        "Bias_2_Human",
        "Bias_2_Human",
        "Bias_2_Human",
        "Multilingual_8_AI",
        "Multilingual_8_AI",
        "Multilingual_8_AI",
        "Multilingual_8_AI",
        "Coding_9_Human",
        "Coding_9_Human",
        "Coding_9_Human",
        "Coding_9_Human",
        "Coding_9_Human",
        "Factuality_6_AI",
        "Factuality_6_AI",
        "Factuality_6_AI",
        "Factuality_6_AI",
        "Factuality_8_AI",
        "Factuality_8_AI",
        "Factuality_8_AI",
        "Factuality_8_AI",
        "Factuality_4_Human",
        "Factuality_4_Human",
        "Factuality_4_Human",
        "Factuality_4_Human",
        "Bias_1_AI",
        "Bias_1_AI",
        "Bias_1_AI",
        "Bias_1_AI",
        "Multilingual_6_Human",
        "Multilingual_6_Human",
        "Multilingual_6_Human",
        "Multilingual_6_Human",
        "Factuality_3_Human",
        "Factuality_3_Human",
        "Factuality_3_Human",
        "Factuality_3_Human",
        "Bias_3_Human",
        "Bias_3_Human",
        "Bias_3_Human",
        "Bias_3_Human",
        "Multilingual_2_AI",
        "Multilingual_2_AI",
        "Multilingual_2_AI",
        "Multilingual_2_AI",
        "Coding_5_Human",
        "Coding_5_Human",
        "Coding_5_Human",
        "Coding_5_Human",
        "Coding_5_Human",
        "Safety_4_AI",
        "Safety_4_AI",
        "Safety_4_AI",
        "Safety_4_AI",
        "Factuality_10_AI",
        "Factuality_10_AI",
        "Factuality_10_AI",
        "Factuality_10_AI",
        "Coding_7_AI",
        "Coding_7_AI",
        "Coding_7_AI",
        "Coding_7_AI",
        "Factuality_5_Human",
        "Factuality_5_Human",
        "Factuality_5_Human",
        "Factuality_5_Human",
        "Factuality_5_Human",
        "Factuality_8_Human",
        "Factuality_8_Human",
        "Factuality_8_Human",
        "Factuality_8_Human",
        "Factuality_8_Human",
        "Multilingual_5_AI",
        "Multilingual_5_AI",
        "Multilingual_5_AI",
        "Multilingual_5_AI",
        "Math_1_Human",
        "Math_1_Human",
        "Math_1_Human",
        "Math_1_Human",
        "Math_1_Human",
        "Safety_5_Human",
        "Safety_5_Human",
        "Safety_5_Human",
        "Safety_5_Human",
        "Safety_5_Human",
        "Bias_2_AI",
        "Bias_2_AI",
        "Bias_2_AI",
        "Bias_2_AI",
        "Math_3_AI",
        "Math_3_AI",
        "Math_3_AI",
        "Math_3_AI",
        "Multilingual_2_Human",
        "Multilingual_2_Human",
        "Multilingual_2_Human",
        "Multilingual_2_Human",
        "Multilingual_2_Human",
        "Safety_1_AI",
        "Safety_1_AI",
        "Safety_1_AI",
        "Safety_1_AI",
        "Factuality_9_AI",
        "Factuality_9_AI",
        "Factuality_9_AI",
        "Factuality_9_AI",
        "Multilingual_7_AI",
        "Multilingual_7_AI",
        "Multilingual_7_AI",
        "Multilingual_7_AI",
        "Uncertainty_2_AI",
        "Uncertainty_2_AI",
        "Uncertainty_2_AI",
        "Uncertainty_2_AI",
        "Uncertainty_2_Human",
        "Uncertainty_2_Human",
        "Uncertainty_2_Human",
        "Uncertainty_2_Human",
        "Bias_1_Human",
        "Bias_1_Human",
        "Bias_1_Human",
        "Bias_1_Human",
        "Safety_5_AI",
        "Safety_5_AI",
        "Safety_5_AI",
        "Safety_5_AI",
        "Bias_4_AI",
        "Bias_4_AI",
        "Bias_4_AI",
        "Bias_4_AI",
        "Factuality_1_Human",
        "Factuality_1_Human",
        "Factuality_1_Human",
        "Factuality_1_Human",
        "Factuality_1_Human",
        "Coding_8_AI",
        "Coding_8_AI",
        "Coding_8_AI",
        "Coding_8_AI",
        "Uncertainty_5_Human",
        "Uncertainty_5_Human",
        "Uncertainty_5_Human",
        "Uncertainty_5_Human",
        "Uncertainty_6_AI",
        "Uncertainty_6_AI",
        "Uncertainty_6_AI",
        "Uncertainty_6_AI",
        "Safety_4_Human",
        "Safety_4_Human",
        "Safety_4_Human",
        "Safety_4_Human",
        "Multilingual_8_Human",
        "Multilingual_8_Human",
        "Multilingual_8_Human",
        "Multilingual_8_Human",
        "Uncertainty_4_AI",
        "Uncertainty_4_AI",
        "Uncertainty_4_AI",
        "Uncertainty_4_AI",
        "Coding_1_Human",
        "Coding_1_Human",
        "Coding_1_Human",
        "Coding_1_Human",
        "Coding_1_Human",
        "Factuality_11_AI",
        "Factuality_11_AI",
        "Factuality_11_AI",
        "Factuality_11_AI"
    ]
}